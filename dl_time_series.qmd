---
title: "DL for Time Series"
format: 
  html: 
    code-fold: true
jupyter: python3
engine: knitr
---

In this comprehensive analysis, I aim to navigate the intricate realm of forecasting time series data using cutting-edge deep learning techniques. My focus will be on delving into the functionalities and applications of **Recurrent Neural Networks (RNNs)**, **Gated Recurrent Units (GRUs)**, and **Long Short-Term Memory networks (LSTMs)**. These sophisticated models will undergo rigorous scrutiny, comparing their predictive capabilities and practicality against established statistical methodologies like **ARMA**, **ARIMA**, and **SARIMA** models. Through evaluation, I seek to shed light on the inherent strengths and potential trade-offs between the innovative deep learning approaches and traditional statistical methods in the realm of time series analysis.

To augment my analysis, I'll integrate pertinent real-world data. Firstly, tracking the **Pfizer** and **TEVA** stock prices will provide invaluable insights into the pharmaceutical industry's performance. Additionally, considering **average life expectancy** trends can enrich our understanding of broader demographic shifts and potential economic impacts. Furthermore, observing **smoking prevalence** rates can serve as a proxy for public health dynamics, potentially influencing consumer behavior and market trends. By intertwining these diverse datasets, I aim to craft a holistic analysis that not only elucidates the efficacy of our forecasting methodologies but also captures the intricate interplay between economic, demographic, and health factors in shaping healthcare industry.

```{r}
library(reticulate)
library(readxl)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{python, message = FALSE}
# !pip3 install scikit-learn
# !pip3 install tensorflow 
# !pip3 install yfinance
# !pip3 install plotly
# !pip3 install statsmodels
# !pip3 install IPython
# !pip3 install matplotlib
# !pip3 install seaborn
# !pip3 install jupyter
# !pip3 install keras
```

```{python}
import tensorflow
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import yfinance as yf
import plotly.express as px
import statsmodels.api as sm
from IPython.display import IFrame
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from tensorflow import keras
from keras import layers
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
from keras.layers import Dense, SimpleRNN, LSTM, GRU
```

```{python}
# Pfizer
pfe = yf.download("PFE", '2019-01-01','2024-04-01')

# Teva
teva = yf.download("TEVA", '2019-01-01','2024-04-01')

# Clean PFE
pfe = pfe.reset_index()
pfe = pfe.rename(columns={'Date':'t', 'Adj Close':'y'})
pfe = pfe[['t', 'y']]
t_pfe = np.array([*range(0,pfe.shape[0])])
x_pfe = np.array(pfe['y']).reshape(t_pfe.shape[0],1)
feature_columns_pfe = [0]
target_columns_pfe = [0]

# Clean TEVA
teva = teva.reset_index()
teva = teva.rename(columns={'Date':'t', 'Adj Close':'y'})
teva = teva[['t', 'y']]
t_teva = np.array([*range(0,teva.shape[0])])
x_teva = np.array(teva['y']).reshape(t_teva.shape[0],1)
feature_columns_teva = [0]
target_columns_teva = [0]

# Average Life Expectancy

ale = pd.read_excel('data/life_expectancy_7countries_2000_2022.xlsx')
ale = ale[['Year', 'United States']]
ale = ale.rename(columns={'Year':'t', 'United States':'y'})
ale = ale[['t', 'y']]
t_ale = np.array([*range(0,ale.shape[0])])
x_ale = np.array(ale['y']).reshape(t_ale.shape[0],1)
feature_columns_ale = [0]
target_columns_ale = [0]

# Number of Smokers

smoke = pd.read_excel('data/number_of_adult_smokers_us_1965_2021.xlsx')
smoke = smoke.rename(columns={'Year':'t', 'Number':'y'})
smoke = smoke[['t', 'y']]
t_smoke = np.array([*range(0,smoke.shape[0])])
x_smoke = np.array(smoke['y']).reshape(t_smoke.shape[0],1)
feature_columns_smoke = [0]
target_columns_smoke = [0]

```

# 1. Original Data

Based on the four datasets I  chose, let's do the data visualization for the original data of Pfizer Stock, TEVA Stock, Average Life Expectancy, and Smoking Prevalence.

:::panel-tabset

## Pfizer Stock

```{python}
print(type(t_pfe), type(x_pfe))
print(t_pfe.shape, x_pfe.shape)

fig, ax = plt.subplots()
for i in range(0,x_pfe.shape[1]):
    ax.plot(t_pfe, x_pfe[:,i],'o',alpha = 0.5)
    ax.plot(t_pfe, x_pfe[:,i],"-")
ax.plot(t_pfe, 0*x_pfe[:,0],"-")
plt.show()
```

## TEVA Stock

```{python}
print(type(t_teva), type(x_teva))
print(t_teva.shape, x_teva.shape)

fig, ax = plt.subplots()
for i in range(0,x_teva.shape[1]):
    ax.plot(t_teva, x_teva[:,i],'o', alpha = 0.5)
    ax.plot(t_teva, x_teva[:,i],"-")
ax.plot(t_teva, 0*x_teva[:,0],"-") # add baseline for reference 
plt.show()
```

## Average Life Expectancy

```{python}
print(type(t_ale), type(x_ale))
print(t_ale.shape, x_ale.shape)

fig, ax = plt.subplots()
for i in range(0,x_ale.shape[1]):
    ax.plot(t_ale, x_ale[:,i],'o', alpha = 0.5)
    ax.plot(t_ale, x_ale[:,i],"-")
ax.plot(t_ale, 0*x_ale[:,0],"-") # add baseline for reference 
ax.set_ylim(70,80)
plt.show()
```

## Smoking Prevalence

```{python}
print(type(t_smoke), type(x_smoke))
print(t_smoke.shape, x_smoke.shape)

fig, ax = plt.subplots()
for i in range(0,x_smoke.shape[1]):
    ax.plot(t_smoke, x_smoke[:,i],'o', alpha = 0.5)
    ax.plot(t_smoke, x_smoke[:,i],"-")
ax.plot(t_smoke, 0*x_smoke[:,0],"-") # add baseline for reference 
plt.show()
```

:::

# 2. Fitting Neural Networks and Forecast

## 2.1 Data Preprossing

In this following section, given the diverse scales of variables within the original data, I'll first apply **normalization** to standardize the regression values, aiming to optimize model performance by ensuring data uniformity, thus boosting the accuracy and efficacy of predictive models. Then, I'll separate the datasets into **training** and **testing** subset. In order to further bolster the training process, lastly I'll integrate **mini-batching**, a strategy involving more frequent gradient updates within each epoch.

### Data Normalization & Splitting

:::panel-tabset

### Pfizer Stock

```{python}
# Pfizer
print(np.mean(x_pfe,axis=0).shape,np.std(x_pfe,axis=0).shape)
x_pfe=(x_pfe-np.mean(x_pfe,axis=0))/np.std(x_pfe,axis=0)
print(x_pfe.shape)
fig, ax = plt.subplots()
for i in range(0,x_pfe.shape[1]):
    ax.plot(t_pfe, x_pfe[:,i],'o')
    ax.plot(t_pfe, x_pfe[:,i],"-")
ax.plot(t_pfe, 0*x_pfe[:,0],"-")
plt.show()
```

### TEVA Stock

```{python}
# TEVA
print(np.mean(x_teva,axis=0).shape,np.std(x_teva,axis=0).shape)
x_teva=(x_teva-np.mean(x_teva,axis=0))/np.std(x_teva,axis=0)
print(x_teva.shape)
fig, ax = plt.subplots()
for i in range(0,x_teva.shape[1]):
    ax.plot(t_teva, x_teva[:,i],'o')
    ax.plot(t_teva, x_teva[:,i],"-")
ax.plot(t_teva, 0*x_teva[:,0],"-")
plt.show()
```

### Average Life Expectancy

```{python}
print(np.mean(x_ale,axis=0).shape,np.std(x_ale,axis=0).shape)
x_ale=(x_ale-np.mean(x_ale,axis=0))/np.std(x_ale,axis=0)
print(x_ale.shape)
fig, ax = plt.subplots()
for i in range(0,x_ale.shape[1]):
    ax.plot(t_ale, x_ale[:,i],'o')
    ax.plot(t_ale, x_ale[:,i],"-")
ax.plot(t_ale, 0*x_ale[:,0],"-")
plt.show()
```

### Smoking Prevalence

```{python}
print(np.mean(x_smoke,axis=0).shape,np.std(x_smoke,axis=0).shape)
x_smoke=(x_smoke-np.mean(x_smoke,axis=0))/np.std(x_smoke,axis=0)
print(x_smoke.shape)
fig, ax = plt.subplots()
for i in range(0,x_smoke.shape[1]):
    ax.plot(t_smoke, x_smoke[:,i],'o')
    ax.plot(t_smoke, x_smoke[:,i],"-")
ax.plot(t_smoke, 0*x_smoke[:,0],"-")
plt.show()
```

:::

### Data Split

:::panel-tabset

### Pfizer Stock

```{python}


## PFE
split_fr = 0.8
cut=int(split_fr*x_pfe.shape[0])
tt_pfe=t_pfe[0:cut]; xt_pfe=x_pfe[0:cut]
tv_pfe=t_pfe[cut:]; xv_pfe=x_pfe[cut:]

fig, ax = plt.subplots()
for i in range(0,x_pfe.shape[1]):
    ax.plot(tt_pfe, xt_pfe[:,i],'ro',alpha=0.25)
    ax.plot(tt_pfe, xt_pfe[:,i],"g-")
for i in range(0,x_pfe.shape[1]):
    ax.plot(tv_pfe, xv_pfe[:,i],'bo',alpha=0.25)
    ax.plot(tv_pfe, xv_pfe[:,i],"g-")
plt.show()
```

### TEVA Stock

```{python}
## TEVA
split_fr = 0.8
cut=int(split_fr*x_teva.shape[0])
tt_teva=t_teva[0:cut]; xt_teva=x_teva[0:cut]
tv_teva=t_teva[cut:]; xv_teva=x_teva[cut:]

fig, ax = plt.subplots()
for i in range(0,x_teva.shape[1]):
    ax.plot(tt_teva, xt_teva[:,i],'ro',alpha=0.25)
    ax.plot(tt_teva, xt_teva[:,i],"g-")
for i in range(0,x_teva.shape[1]):
    ax.plot(tv_teva, xv_teva[:,i],'bo',alpha=0.25)
    ax.plot(tv_teva, xv_teva[:,i],"g-")
plt.show()
```

### Average Life Expectancy

```{python}
## Average Life Expectancy
split_fr = 0.8
cut=int(split_fr*x_ale.shape[0])
tt_ale=t_ale[0:cut]; xt_ale=x_ale[0:cut]
tv_ale=t_ale[cut:]; xv_ale=x_ale[cut:]

fig, ax = plt.subplots()
for i in range(0,x_ale.shape[1]):
    ax.plot(tt_ale, xt_ale[:,i],'ro',alpha=0.25)
    ax.plot(tt_ale, xt_ale[:,i],"g-")
for i in range(0,x_ale.shape[1]):
    ax.plot(tv_ale, xv_ale[:,i],'bo',alpha=0.25)
    ax.plot(tv_ale, xv_ale[:,i],"g-")
plt.show()
```

### Smoking Prevalence

```{python}
## Smoking Prevalence
split_fr = 0.8
cut=int(split_fr*x_smoke.shape[0])
tt_smoke=t_smoke[0:cut]; xt_smoke=x_smoke[0:cut]
tv_smoke=t_smoke[cut:]; xv_smoke=x_smoke[cut:]

fig, ax = plt.subplots()
for i in range(0,x_smoke.shape[1]):
    ax.plot(tt_smoke, xt_smoke[:,i],'ro',alpha=0.25)
    ax.plot(tt_smoke, xt_smoke[:,i],"g-")
for i in range(0,x_smoke.shape[1]):
    ax.plot(tv_smoke, xv_smoke[:,i],'bo',alpha=0.25)
    ax.plot(tv_smoke, xv_smoke[:,i],"g-")
plt.show()
```

:::

### Mini-Batching

```{python}

def from_arrays(x, lookback=3, delay=1, step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):
  # initialize 
  i_start = 0
  count = 0
  x_out = []
  y_out = []

  # sequentially build mini-batch samples
  while i_start + lookback + delay < x.shape[0]:
    i_stop = i_start + lookback
    i_pred = i_stop + delay

    # report if desired 
    if verbose and count < 2: 
      print("indice range:", i_start, i_stop, "-->", i_pred)

    # define arrays: 
    indices_to_keep = []
    j = i_stop
    while j >= i_start:
      indices_to_keep.append(j)
      j -= step

    # create mini-batch sample
    xtmp = x[indices_to_keep, :]    # isolate relevant indices
    xtmp = xtmp[:, feature_columns] # isolate desired features
    ytmp = x[i_pred, target_columns]
    x_out.append(xtmp)
    y_out.append(ytmp)

    # report if desired 
    #if verbose and count < 2: 
      #print(xtmp, "-->", ytmp)
      #print("shape:", xtmp.shape, "-->", ytmp.shape)

    # Plot for debugging    
    if verbose and count < 2:
      fig, ax = plt.subplots()
      ax.plot(x, 'b-')
      ax.plot(x, 'bx')
      ax.plot(indices_to_keep, xtmp, 'go')
      ax.plot(i_pred * np.ones(len(target_columns)), ytmp, 'ro')
      plt.show()

     # Update start point 
    if unique:
      i_start += lookback
    else:
      i_start += 1
    count += 1
        
  return np.array(x_out), np.array(y_out)
```

:::panel-tabset

### Pfizer Stock

```{python}

## PFE
L=25; S=1; D=1

Xt_pfe, Yt_pfe = from_arrays(xt_pfe, lookback=L, delay=D, step=S, feature_columns=feature_columns_pfe, target_columns=target_columns_pfe, unique=False,verbose=True)
Xv_pfe, Yv_pfe = from_arrays(xv_pfe, lookback=L, delay=D, step=S, feature_columns=feature_columns_pfe, target_columns=target_columns_pfe, unique=False,verbose=True)

print('Training:', Xt_pfe.shape, Yt_pfe.shape)
print('Validation:', Xv_pfe.shape, Yv_pfe.shape)
```

### TEVA Stock

```{python}
## TEVA
L=25; S=1; D=1

Xt_teva, Yt_teva = from_arrays(xt_teva, lookback=L, delay=D, step=S, feature_columns=feature_columns_teva, target_columns=target_columns_teva, unique=False,verbose=True)
Xv_teva, Yv_teva = from_arrays(xv_teva, lookback=L, delay=D, step=S, feature_columns=feature_columns_teva, target_columns=target_columns_teva, unique=False,verbose=True)

print('Training:', Xt_teva.shape, Yt_teva.shape)
print('Validation:', Xv_teva.shape, Yv_teva.shape)
```

### Average Life Expectancy

```{python}
## ALE
L=2; S=1; D=1

Xt_ale, Yt_ale = from_arrays(xt_ale, lookback=L, delay=D, step=S, feature_columns=feature_columns_ale, target_columns=target_columns_ale, unique=False,verbose=True)
Xv_ale, Yv_ale = from_arrays(xv_ale, lookback=L, delay=D, step=S, feature_columns=feature_columns_ale, target_columns=target_columns_ale, unique=False,verbose=True)

print('Training:', Xt_ale.shape, Yt_ale.shape)
print('Validation:', Xv_ale.shape, Yv_ale.shape)
```

### Smoking Prevalence

```{python}
## Smoking
L=4; S=1; D=1

Xt_smoke, Yt_smoke = from_arrays(xt_smoke, lookback=L, delay=D, step=S, feature_columns=feature_columns_smoke, target_columns=target_columns_smoke, unique=False,verbose=True)
Xv_smoke, Yv_smoke = from_arrays(xv_smoke, lookback=L, delay=D, step=S, feature_columns=feature_columns_smoke, target_columns=target_columns_smoke, unique=False,verbose=True)

print('Training:', Xt_smoke.shape, Yt_smoke.shape)
print('Validation:', Xv_smoke.shape, Yv_smoke.shape)
```
:::

## 2.2 RNN

In this section, I'll apply **RNNs** to the four datasets I chose. Recurrent Neural Networks (RNNs) stand as a foundational pillar in the landscape of deep learning, offering a powerful framework for processing sequential data. At their core, RNNs are designed to capture temporal dependencies within sequences, making them exceptionally well-suited for tasks such as time series forecasting, and natural language processing. Unlike traditional feedforward neural networks, RNNs possess an inherent memory mechanism that enables them to **retain information about past inputs while processing current ones**. This unique capability allows RNNs to exhibit dynamic behavior, effectively transforming input sequences into meaningful output representations. However, traditional RNNs are susceptible to the vanishing gradient problem, hindering their ability to effectively capture long-range dependencies. Despite this limitation, simple RNNs serve as a fundamental building block upon which more advanced architectures, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, have been developed to address these challenges and further extend the capabilities of sequential data processing.

The following code will be used to generate the models, which include **regularization**.

```{python}
## Regression Report Function

def regression_report(yt,ytp,yv,yvp):
    print("---------- Regression report ----------")
    
    print("TRAINING:")
    print(" MSE:",mean_squared_error(yt,ytp))
    print(" MAE:",mean_absolute_error(yt,ytp))
    # print(" MAPE:",mean_absolute_percentage_error(Yt,Ytp))
    
    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt,ytp,'ro')
    ax.plot(yt,yt,'b-')
    ax.set(xlabel='y_data', ylabel='y_predicted',
        title='Training data parity plot (line y=x represents a perfect fit)')
    plt.show()
    
    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot=1.0
    upper=int(frac_plot*yt.shape[0]); 
    # print(int(0.5*yt.shape[0]))
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper],'b-')
    ax.plot(ytp[0:upper],'r-',alpha=0.5)
    ax.plot(ytp[0:upper],'ro',alpha=0.25)
    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Training: Time-series prediction')
    plt.show()

      
    print("VALIDATION:")
    print(" MSE:",mean_squared_error(yv,yvp))
    print(" MAE:",mean_absolute_error(yv,yvp))
    # print(" MAPE:",mean_absolute_percentage_error(Yt,Ytp))
    
    # PARITY PLOT 
    fig, ax = plt.subplots()
    ax.plot(yv,yvp,'ro')
    ax.plot(yv,yv,'b-')
    ax.set(xlabel='y_data', ylabel='y_predicted',
        title='Validation data parity plot (line y=x represents a perfect fit)')
    plt.show()
    
    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper=int(frac_plot*yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper],'b-')
    ax.plot(yvp[0:upper],'r-',alpha=0.5)
    ax.plot(yvp[0:upper],'ro',alpha=0.25)
    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')
    plt.show()
```

```{python}
## History Plot
def history_plot(history):
  FS=18 #fontsize

  history_dict = history.history
  loss_values = history_dict['loss']
  val_loss_values = history_dict['val_loss']
  epochs = range(1, len(loss_values) + 1)
  plt.plot(epochs, loss_values, 'bo', label='Training Loss')
  plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')
  plt.title('Training and Validation Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()
```

:::panel-tabset

### Pfizer Stock

```{python}

## PFE
Xt1_pfe = Xt_pfe.reshape(Xt_pfe.shape[0],Xt_pfe.shape[1]*Xt_pfe.shape[2])
# New Sizes 
print(Xt_pfe.shape,"-->",Yt_pfe.shape)
print(Xv_pfe.shape,"-->",Yv_pfe.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-4
input_shape=(Xt_pfe.shape[1],Xt_pfe.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_pfe)              # batch training

# BUILD MODEL
recurrent_hidden_units=128

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(l2=L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_pfe = model.fit(Xt_pfe,
                    Yt_pfe,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_pfe, Yv_pfe))
# History plot
history_plot(history_pfe)

# Predictions 
Ytp_pfe=model.predict(Xt_pfe)
Yvp_pfe=model.predict(Xv_pfe) 
# REPORT
regression_report(Yt_pfe,Ytp_pfe,Yv_pfe,Yvp_pfe)

```
### TEVA Stock

```{python}

## TEVA
Xt1_teva = Xt_teva.reshape(Xt_teva.shape[0],Xt_teva.shape[1]*Xt_teva.shape[2])
# New Sizes 
print(Xt_teva.shape,"-->",Yt_teva.shape)
print(Xv_teva.shape,"-->",Yv_teva.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-4
input_shape=(Xt_teva.shape[1],Xt_teva.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_teva)              # batch training

# BUILD MODEL
recurrent_hidden_units=128

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_teva = model.fit(Xt_teva,
                    Yt_teva,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_teva, Yv_teva))
# History plot
history_plot(history_teva)

# Predictions 
Ytp_teva=model.predict(Xt_teva)
Yvp_teva=model.predict(Xv_teva) 
# REPORT
regression_report(Yt_teva,Ytp_teva,Yv_teva,Yvp_teva)

```

### Average Life Expectancy

```{python}
## ALE
Xt1_ale = Xt_ale.reshape(Xt_ale.shape[0],Xt_ale.shape[1]*Xt_ale.shape[2])
# New Sizes 
print(Xt_ale.shape,"-->",Yt_ale.shape)
print(Xv_ale.shape,"-->",Yv_ale.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=20 #100
L1=0
L2=1e-3
input_shape=(Xt_ale.shape[1],Xt_ale.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_ale)              # batch training

# BUILD MODEL
recurrent_hidden_units=32

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_ale = model.fit(Xt_ale,
                    Yt_ale,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_ale, Yv_ale))
# History plot
history_plot(history_ale)

# Predictions 
Ytp_ale=model.predict(Xt_ale)
Yvp_ale=model.predict(Xv_ale) 
# REPORT
regression_report(Yt_ale,Ytp_ale,Yv_ale,Yvp_ale)
```

### Smoking Prevalence

```{python}
## Smoking
Xt1_smoke = Xt_smoke.reshape(Xt_smoke.shape[0],Xt_smoke.shape[1]*Xt_smoke.shape[2])
# New Sizes 
print(Xt_smoke.shape,"-->",Yt_smoke.shape)
print(Xv_smoke.shape,"-->",Yv_smoke.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1= 0 # 1e-3
L2=1e-4
#L2=0 #1e-4

input_shape=(Xt_smoke.shape[1],Xt_smoke.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_smoke)              # batch training

# BUILD MODEL
recurrent_hidden_units=64

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L1L2(l1=L1,l2=L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_smoke = model.fit(Xt_smoke,
                    Yt_smoke,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_smoke, Yv_smoke))
# History plot
history_plot(history_smoke)

# Predictions 
Ytp_smoke=model.predict(Xt_smoke)
Yvp_smoke=model.predict(Xv_smoke) 
# REPORT
regression_report(Yt_smoke,Ytp_smoke,Yv_smoke,Yvp_smoke)
```
:::

## 2.3 GRU

In this section, I'll apply **GRU** to the four datasets I chose. Gated Recurrent Units (GRUs) represent a pivotal advancement in the realm of recurrent neural networks (RNNs), offering a streamlined yet powerful architecture for sequential data processing. Built upon the foundation of traditional RNNs, GRUs introduce a sophisticated gating mechanism that enables them to effectively capture long-range dependencies while mitigating the vanishing gradient problem. Unlike standard RNNs, GRUs feature gated units that regulate the flow of information throughout the network, allowing for more efficient information retention and utilization. This innovative design grants GRUs the ability to adaptively learn and update their internal states, making them particularly well-suited for tasks requiring memory and context preservation. Furthermore, GRUs exhibit computational efficiency compared to more complex architectures like Long Short-Term Memory (LSTM) networks, making them a popular choice for various applications where resource constraints are a consideration. With their blend of simplicity and effectiveness, GRUs stand as a versatile tool in the arsenal of deep learning practitioners, offering a robust solution for modeling sequential data with ease and efficiency.

:::panel-tabset

### Pfizer Stock

```{python}
# GRU
## PFE
Xt1_pfe = Xt_pfe.reshape(Xt_pfe.shape[0],Xt_pfe.shape[1]*Xt_pfe.shape[2])
# NEW SIZES 
print(Xt_pfe.shape,"-->",Yt_pfe.shape)
print(Xv_pfe.shape,"-->",Yv_pfe.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-4
input_shape=(Xt_pfe.shape[1],Xt_pfe.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_pfe)              # batch training

# BUILD MODEL
recurrent_hidden_units=128

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(l2=L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_pfe = model.fit(Xt_pfe,
                    Yt_pfe,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_pfe, Yv_pfe))
# History plot
history_plot(history_pfe)

# Predictions 
Ytp_pfe=model.predict(Xt_pfe)
Yvp_pfe=model.predict(Xv_pfe) 
# REPORT
regression_report(Yt_pfe,Ytp_pfe,Yv_pfe,Yvp_pfe)
```

### TEVA Stock

```{python}
## TEVA
Xt1_teva = Xt_teva.reshape(Xt_teva.shape[0],Xt_teva.shape[1]*Xt_teva.shape[2])
# NEW SIZES 
print(Xt_teva.shape,"-->",Yt_teva.shape)
print(Xv_teva.shape,"-->",Yv_teva.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-4
input_shape=(Xt_teva.shape[1],Xt_teva.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_teva)              # batch training

# BUILD MODEL
recurrent_hidden_units=128

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_teva = model.fit(Xt_teva,
                    Yt_teva,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_teva, Yv_teva))
# History plot
history_plot(history_teva)

# Predictions 
Ytp_teva=model.predict(Xt_teva)
Yvp_teva=model.predict(Xv_teva) 
# REPORT
regression_report(Yt_teva,Ytp_teva,Yv_teva,Yvp_teva)
```

### Average Life Expectancy

```{python}
# GRU
## ALE
Xt1_ale = Xt_ale.reshape(Xt_ale.shape[0],Xt_ale.shape[1]*Xt_ale.shape[2])
# NEW SIZES 
print(Xt_ale.shape,"-->",Yt_ale.shape)
print(Xv_ale.shape,"-->",Yv_ale.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-3
input_shape=(Xt_ale.shape[1],Xt_ale.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_ale)              # batch training

# BUILD MODEL
recurrent_hidden_units=64

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_ale = model.fit(Xt_ale,
                    Yt_ale,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_ale, Yv_ale))
# History plot
history_plot(history_ale)

# Predictions 
Ytp_ale=model.predict(Xt_ale)
Yvp_ale=model.predict(Xv_ale) 
# REPORT
regression_report(Yt_ale,Ytp_ale,Yv_ale,Yvp_ale)
```

### Smoking Prevalence

```{python}
# GRU
## Smoking
Xt1_smoke = Xt_smoke.reshape(Xt_smoke.shape[0],Xt_smoke.shape[1]*Xt_smoke.shape[2])
# NEW SIZES 
print(Xt_smoke.shape,"-->",Yt_smoke.shape)
print(Xv_smoke.shape,"-->",Yv_smoke.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100

L1= 0 # 1e-3
L2=1e-4

#L2=0 #1e-4
input_shape=(Xt_smoke.shape[1],Xt_smoke.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_smoke)              # batch training

# BUILD MODEL
recurrent_hidden_units=64

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L1L2(l1=L1,l2=L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)

# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_smoke = model.fit(Xt_smoke,
                    Yt_smoke,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_smoke, Yv_smoke))
# History plot
history_plot(history_smoke)

# Predictions 
Ytp_smoke=model.predict(Xt_smoke)
Yvp_smoke=model.predict(Xv_smoke) 
# REPORT
regression_report(Yt_smoke,Ytp_smoke,Yv_smoke,Yvp_smoke)
```

:::

## 2.4 LSTM

In this section, I'll apply **LSTM** to the four datasets I chose. Long Short-Term Memory (LSTM) networks represent a significant breakthrough in the realm of recurrent neural networks (RNNs), addressing the inherent challenges of capturing long-range dependencies and mitigating the vanishing gradient problem. Characterized by a sophisticated memory cell and gating mechanisms, LSTMs excel at learning and retaining information over extended sequences. Unlike traditional RNNs, LSTMs feature a more intricate architecture that allows them to selectively update and forget information based on the context of the input sequence, enabling them to effectively model both short-term patterns and long-term dependencies. The key components of LSTMs, including input, forget, and output gates, work in tandem to regulate the flow of information throughout the network, facilitating robust learning and memory retention. Despite their complexity, LSTMs have become a cornerstone in the field of deep learning, demonstrating unparalleled performance in a wide range of sequential data analysis tasks and serving as a foundation for further advancements in recurrent neural network architectures.

::: panel-tabset

### Pfizer Stock

```{python}

## PFE

Xt1_pfe = Xt_pfe.reshape(Xt_pfe.shape[0],Xt_pfe.shape[1]*Xt_pfe.shape[2])
# NEW SIZES 
print(Xt_pfe.shape,"-->",Yt_pfe.shape)
print(Xv_pfe.shape,"-->",Yv_pfe.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-4
input_shape=(Xt_pfe.shape[1],Xt_pfe.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_pfe)              # batch training

# BUILD MODEL
recurrent_hidden_units=128

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
model.add(LSTM(
#model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(l2=L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_pfe = model.fit(Xt_pfe,
                    Yt_pfe,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_pfe, Yv_pfe))
# History plot
history_plot(history_pfe)
# Predictions 
Ytp_pfe=model.predict(Xt_pfe)
Yvp_pfe=model.predict(Xv_pfe) 
# REPORT
regression_report(Yt_pfe,Ytp_pfe,Yv_pfe,Yvp_pfe)


```

### TEVA Stock

```{python}
## TEVA

Xt1_teva = Xt_teva.reshape(Xt_teva.shape[0],Xt_teva.shape[1]*Xt_teva.shape[2])
# NEW SIZES 
print(Xt_teva.shape,"-->",Yt_teva.shape)
print(Xv_teva.shape,"-->",Yv_teva.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1=0
L2=1e-4
input_shape=(Xt_teva.shape[1],Xt_teva.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_teva)              # batch training

# BUILD MODEL
recurrent_hidden_units=128

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
model.add(LSTM(
#model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_teva = model.fit(Xt_teva,
                    Yt_teva,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_teva, Yv_teva))
# History plot
history_plot(history_teva)
# Predictions 
Ytp_teva=model.predict(Xt_teva)
Yvp_teva=model.predict(Xv_teva) 
# REPORT
regression_report(Yt_teva,Ytp_teva,Yv_teva,Yvp_teva)
```

### Average Life Expectancy

```{python}

## ALE

Xt1_ale = Xt_ale.reshape(Xt_ale.shape[0],Xt_ale.shape[1]*Xt_ale.shape[2])
# NEW SIZES 
print(Xt_ale.shape,"-->",Yt_ale.shape)
print(Xv_ale.shape,"-->",Yv_ale.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=20 #100
L1=0
L2=1e-3
input_shape=(Xt_ale.shape[1],Xt_ale.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_ale)              # batch training

# BUILD MODEL
recurrent_hidden_units=32

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
model.add(LSTM(
#model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L2(L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_ale = model.fit(Xt_ale,
                    Yt_ale,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_ale, Yv_ale))
# History plot
history_plot(history_ale)
# Predictions 
Ytp_ale=model.predict(Xt_ale)
Yvp_ale=model.predict(Xv_ale) 
# REPORT
regression_report(Yt_ale,Ytp_ale,Yv_ale,Yvp_ale)


```

### Smoking Prevalence

```{python}

## Smoking

Xt1_smoke = Xt_smoke.reshape(Xt_smoke.shape[0],Xt_smoke.shape[1]*Xt_smoke.shape[2])
# NEW SIZES 
print(Xt_smoke.shape,"-->",Yt_smoke.shape)
print(Xv_smoke.shape,"-->",Yv_smoke.shape)
# # HYPERPARAMETERS 
optimizer="rmsprop"
loss_function="MeanSquaredError" 
learning_rate=0.001
numbers_epochs=200 #100
L1= 0 # 1e-3
L2=1e-4

#L2=0 #1e-4
input_shape=(Xt_smoke.shape[1],Xt_smoke.shape[2])

# # batch_size=1                       # stocastic training
# # batch_size=int(len(x_train)/2.)    # mini-batch training
batch_size=len(Xt1_smoke)              # batch training

# BUILD MODEL
recurrent_hidden_units=64

# CREATE MODEL
model = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
model.add(LSTM(
#model.add(GRU(
#model.add(SimpleRNN(
units=recurrent_hidden_units,
return_sequences=False,
input_shape=input_shape, 
# recurrent_dropout=0.8,
recurrent_regularizer=regularizers.L1L2(l1=L1,l2=L2),
activation='relu')
          ) 
     
# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR 
model.add(Dense(units=1, activation='linear'))

# MODEL SUMMARY
print(model.summary()); #print(x_train.shape,y_train.shape)
# # print("initial parameters:", model.get_weights())

# # COMPILING THE MODEL 
opt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)
model.compile(optimizer=opt, loss=loss_function)

# TRAINING YOUR MODEL
history_smoke = model.fit(Xt_smoke,
                    Yt_smoke,
                    epochs=numbers_epochs,
                    batch_size=batch_size, verbose=False,
                    validation_data=(Xv_smoke, Yv_smoke))
# History plot
history_plot(history_smoke)
# Predictions 
Ytp_smoke=model.predict(Xt_smoke)
Yvp_smoke=model.predict(Xv_smoke) 
# REPORT
regression_report(Yt_smoke,Ytp_smoke,Yv_smoke,Yvp_smoke)

```

:::

# 3 Discussion

## 3.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?

:::panel-tabset

### Pfizer Stock 

**SIMPLE RNN**

---------- Regression report ----------

TRAINING:

MSE: 0.09003306405774096

MAE: 0.22935533520666676

VALIDATION:

MSE: 0.07152575556546274

MAE: 0.20140568269113573

**GRU**

---------- Regression report ----------

TRAINING:

MSE: 0.07582283564521113

MAE: 0.21662457798905047

VALIDATION:

MSE: 0.047982706225703546

MAE: 0.16800236956128273

**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.06083309764499571

MAE: 0.19734143610035615

VALIDATION:

MSE: 0.051430222021359814

MAE: 0.1647251482979423

**Conclusion:**

For the Pfizer Stock data, we can see from the evaluation metrics of three models we fit that **GRU** is the best performing model.

### TEVA Stock

**SIMPLE RNN**

---------- Regression report ----------

TRAINING:

MSE: 0.14224490639450332

MAE: 0.2766673451356279

VALIDATION:

MSE: 0.18162044250956272

MAE: 0.31885585749557244

**GRU**

---------- Regression report ----------

TRAINING:

MSE: 0.13898719031178638

MAE: 0.29045156732048505

VALIDATION:

MSE: 0.09160100968046733

MAE: 0.2522322086363801

**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.0570398157523587

MAE: 0.17491911305353966

VALIDATION:

MSE: 0.0437083955363541

MAE: 0.1610215293517061

**Conclusion:**

For the TEVA Stock data, we can see from the evaluation metrics of three models we fit that **LSTM** is the best performing model.

### Average Life Expectancy

**SIMPLE RNN**

---------- Regression report ----------

TRAINING:

MSE: 0.07496999048951764

MAE: 0.23799187433914348

VALIDATION:

MSE: 1.6271619522321523

MAE: 1.202708950450143
 
**GRU**

---------- Regression report ----------

TRAINING:

MSE: 0.009263268811444294

MAE: 0.07671659784789651

VALIDATION:

MSE: 1.974347099495756

MAE: 1.099753070450283
 
**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.38868042745021164

MAE: 0.5600229344557486

VALIDATION:

MSE: 1.1643005406464868

MAE: 0.8072635045413741
 
**Conclusion:**

For the Average Life Expectancy data, we can see from the evaluation metrics of three models we fit that **LSTM** is the best performing model.
 
### Smoking Prevalence

**SIMPLE RNN**

---------- Regression report ----------

TRAINING:

MSE: 0.0017923777777443042

MAE: 0.03193420489724911

VALIDATION:

MSE: 3.896097217779866

MAE: 1.9687103122764207
 
**GRU**

---------- Regression report ----------

TRAINING:

MSE: 0.033673515118184075

MAE: 0.13948307019576878

VALIDATION:

MSE: 1.462594036994753

MAE: 1.203614750533638
 
**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.03252545345599816

MAE: 0.13955811391405032

VALIDATION:

MSE: 0.20626923253046567

MAE: 0.453361609607277

**Conclusion:**

For the Smoking Prevalence data, we can see from the evaluation metrics of three models we fit that **LSTM** is the best performing model.

:::

To conclude, among all the models we considered, the **LSTM model** emerged as the standout performer, demonstrating superiority across a spectrum of evaluation metrics. Its effectiveness, efficiency in navigating complexity, and adept handling of performance trade-offs underscored its dominance. Notably, the LSTM model exhibited exceptional accuracy and predictive prowess, particularly on unseen data. This remarkable performance can be attributed to the **intricacies of its architecture**, which facilitates the nuanced capture of intricate relationships and patterns within the dataset. The LSTM's more sophisticated design empowers it to discern subtle dependencies and temporal dynamics, thereby unlocking deeper insights and driving enhanced predictive performance.

## 3.2 What effect does including regularization have on the results?

Including regularization into machine learning models is very significant, because it can **improve generalization** and **combat overfitting** by imposing a penalty on the model's complexity. By introducing regularization techniques, such as **adjusting the loss function** to penalize complex models or **implementing dropout layers**, our aim is to promote generalization and prevent models from becoming overly specialized to the training data. Regularization helps prevent the reliance on specific features or patterns in the training data, fostering greater adaptability and resilience in model predictions. After I applied regularization to the four models I fit, I found that it improved the models' accuracy a lot, avoiding overfitting.

## 3.3 How far into the future can the deep learning model accurately predict the future?

The accuracy of deep learning models such as RNNs, LSTMs, and GRUs in forecasting future events is contingent upon various factors, with predictive accuracy typically diminishing as the **forecast horizon** extends. While these models excel in short-term predictions, accurately forecasting the next few steps, their performance tends to degrade as the forecasting timeframe lengthens. Although architectures like LSTMs and GRUs may offer improved performance over longer horizons compared to basic RNNs due to their ability to address issues like the vanishing gradient problem, the precise range of accurate forecasting remains highly dependent on the specific application and model conditions. Considering this, I think we should focus on shorter and more reliable forecast horizons, and regularly update models with new data, which will help us yield superior results. 

Meanwhile, the predictive capability of deep learning models also relies heavily on the **data complexity** during training. As supervised learners, these models generate predictions based on the patterns within their training datasets. Consequently, their predictive power extends only as far as recognizing patterns similar to those in their training data. When predictions extend further into the future, their accuracy becomes increasingly dependent on the quality and representativeness of historical data, limiting accuracy when facing novel scenarios or trends not encountered during training.

Last but not least, the accuracy of the models in predicting future depends on the **model architectures** as well.

## 3.4 How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models?

In this section, I'll compare the ARIMA models we got before and the best performing deep learning models.

:::panel-tabset

### Pfizer Stock 

**ARMA(2,8)**

Training set error measures:
                        ME      RMSE       MAE MPE MAPE      MASE        ACF1
                        
Training set -1.244409e-05 0.5955813 0.4091127 NaN  Inf 0.6382266 -0.00247564

**GRU**

---------- Regression report ----------

TRAINING:

MSE: 0.07582283564521113

MAE: 0.21662457798905047

VALIDATION:

MSE: 0.047982706225703546

MAE: 0.16800236956128273


**Conclusion:**

For the Pfizer Stock data, we can compare the **MAE** and the **RMSE** values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the deep learning model has lower RMSE and MAE values than ARIMA, suggesting that deep learning model may better handle the *Pfizer Stock* data and produce more accurate predictions.

### TEVA Stock

**ARMA(1,5)**

Training set error measures:
                       ME      RMSE       MAE MPE MAPE      MASE         ACF1
                       
Training set -0.000172408 0.3340522 0.2310923 NaN  Inf 0.7093577 -0.001059798

**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.0570398157523587

MAE: 0.17491911305353966

VALIDATION:

MSE: 0.0437083955363541

MAE: 0.1610215293517061

**Conclusion:**

For the TEVA Stock data, we can compare the **MAE** and the **RMSE** values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the deep learning model has lower RMSE and MAE values than ARIMA, suggesting that deep learning model may better handle the *TEVA Stock* data and produce more accurate predictions.

### Average Life Expectancy

**ARMA(0,0,2)**

Training set error measures:
                      ME      RMSE       MAE  MPE MAPE      MASE       ACF1
                      
Training set -0.01784985 0.2933763 0.1901972 -Inf  Inf 0.5501572 -0.1987566
 
**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.38868042745021164

MAE: 0.5600229344557486

VALIDATION:

MSE: 1.1643005406464868

MAE: 0.8072635045413741
 
**Conclusion:**

For the Average Life Expectancy data, we can compare the **MAE** and the **RMSE** values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the **ARIMA** model surprisingly has lower RMSE and MAE values than ARIMA, suggesting that ARIMA model may better handle the *Average Life Expectancy* data and produce more accurate predictions.
 
### Smoking Prevalence

**ARMA(1,0,1)**

Training set error measures:
                     ME     RMSE      MAE MPE MAPE      MASE        ACF1
                     
Training set -0.0724224 3.206556 1.824766 NaN  Inf 0.6521255 -0.03006084

**LSTM**

---------- Regression report ----------

TRAINING:

MSE: 0.03252545345599816

MAE: 0.13955811391405032

VALIDATION:

MSE: 0.20626923253046567

MAE: 0.453361609607277

**Conclusion:**

For the Smoking Prevalence data, we can compare the **MAE** and the **RMSE** values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the deep learning model has lower RMSE and MAE values than ARIMA, suggesting that the deep learning model may better handle the *Smoking Prevalence* data and produce more accurate predictions.

:::

# 4 Comparision
## DEEP LEARNING PREDICTION Vs. Traditional TS Models

In summary, the analysis I did in this page highlights the effectiveness of deep learning models in time series forecasting tasks. While direct comparisons between deep learning and traditional models(eg. ARIMA, SARIMA, ARIMAX, SARIMAX, VAR) is challenging due to differences in training/validation splits, the **RMSE values** consistently favor deep learning models just except the Average Life Expectancy data. 

Actually, for most of the traditional time series models we fit, they have higher RMSE, especially evident in the Smoking Prevalence data(RMSE for ARMA: 3.207), indicating their limitations in capturing data variance. Conversely, deep learning models consistently demonstrate lower RMSE values, most of them are below 1.


Additionally, when we compare the training/validation plots of our deep learning models with the forecasting plots of the previous traditional time series models, we can clearly see that the deep learning models **more closely** align with the real data trend. This disparity shows that traditional time series models such as ARIMA struggle in capturing data variance, while deep learning models adeptly memorize it. From Part 3.1, we can see that the hierarchy of deep learning models' performance were impressive, and LSTM model outperforms than the other two models(GRU, simple RNN) in most of our data.

Traditional time series models often require data to be stationary, often necessitating differencing to achieve forecasting accuracy. However, even with differencing, satisfactory results may not be attainable. Deep learning models, conversely, act as universal curve-fitters, capable of accommodating various data variances. This adaptability renders deep learning models more suitable for forecasting datasets characterized by fluctuations and seasonal patterns.

Conventional time series models frequently demand **stationary** data, prompting the need for **differencing** to enhance forecasting accuracy. Despite such preprocessing efforts, satisfactory results remain elusive in certain cases. In contrast, deep learning models serve as versatile curve-fitters, possessing the inherent capability to adapt to diverse data variances. This inherent adaptability positions deep learning models as ideal candidates for forecasting datasets characterized by pronounced fluctuations and intricate seasonal patterns. By virtue of their flexibility, deep learning models offer a compelling alternative to traditional approaches, seamlessly accommodating the inherent complexities of real-world data and yielding more robust and accurate forecasts.
