[
  {
    "objectID": "explore_analysis.html",
    "href": "explore_analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "1. Original Data\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small and Medium-sized BiotechPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes)Public Health Outcomes: Infant Mortality RateHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Room VisitsHealthcare Costs: Costs of Prescription DrugsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behavior and Risk Factors: Smoking PrevalenceHealth Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"PFE\", \"VRTX\", \"TEVA\", \"SAGE\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2019-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(PFE$PFE.Adjusted,\n                    VRTX$VRTX.Adjusted,\n                    TEVA$TEVA.Adjusted,\n                    SAGE$SAGE.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock &lt;- stock %&gt;%\n  rownames_to_column(var = \"date\")\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\npharma_ts &lt;- ts(stock$PFE, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 12)\n\npharma_plot&lt;-autoplot(pharma_ts, xlab = \"Time\", ylab = \"Price\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Pfizer Stock')+theme_bw()\nggplotly(pharma_plot)\n\n\n\n\n\n\n\n\n\n\nCode\nbiotech_ts &lt;- ts(stock$VRTX, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 12)\n\nbiotech_plot&lt;-autoplot(biotech_ts, xlab = \"Time\", ylab = \"Price\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Vertex Stock')+theme_bw()\nggplotly(biotech_plot)\n\n\n\n\n\n\n\n\n\n\nCode\ndrug_ts &lt;- ts(stock$TEVA, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 12)\n\ndrug_plot&lt;-autoplot(drug_ts, xlab = \"Time\", ylab = \"Price\", colour = \"#27aeef\")+ggtitle('Time Series Plot For TEVA Stock')+theme_bw()\nggplotly(drug_plot)\n\n\n\n\n\n\n\n\n\n\nCode\nsmall_biotech_ts &lt;- ts(stock$SAGE, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 12)\n\nsmall_biotech_plot&lt;-autoplot(small_biotech_ts, xlab = \"Time\", ylab = \"Price\", colour = \"#27aeef\")+ggtitle('Time Series Plot For SAGE Stock')+theme_bw()\nggplotly(small_biotech_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\naverage_life_expectancy &lt;- read_xlsx(\"data/life_expectancy_7countries_2000_2022.xlsx\")\n\n# Change it to long format\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Country\", \n               values_to = \"Life_Expectancy\")\n\naverage_life_expectancy$Year &lt;- as.Date(paste(average_life_expectancy$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\naverage_life_expectancy$Life_Expectancy &lt;- as.double(average_life_expectancy$Life_Expectancy)\n\n# Filter to only include the US\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  filter(Country == \"United States\")\n\nale_ts &lt;- ts(average_life_expectancy$Life_Expectancy, start = c(year(min(average_life_expectancy$Year)), month(min(average_life_expectancy$Year))), end = c(year(max(average_life_expectancy$Year)), month(max(average_life_expectancy$Year))), frequency = 12)\n\nale_ts_plot&lt;-autoplot(ale_ts, \n                      xlab = \"Time\", \n                      ylab = \"Average Life Expectancy(years)\", \n                      colour = \"#27aeef\")+\n  ggtitle('Time Series Plot For Average Life Expectancy in the US')+\n  theme_bw()\n\nggplotly(ale_ts_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\ndeath_cancer &lt;- read_xlsx(\"data/deaths_cancer_us_1950_2019.xlsx\")\ndeath_heart &lt;- read_xlsx(\"data/deaths_heart_diseases_us_1950_2019.xlsx\")\ndeath_diabetes &lt;- read_xlsx(\"data/deaths_diabetes_us_1950_2019.xlsx\")\n\n# Merge these three datasets\ndeath_major_diseases &lt;- merge(death_cancer, death_heart, by = \"Year\")\ndeath_major_diseases &lt;- merge(death_major_diseases, death_diabetes, by = \"Year\")\n\n# Change Date format\ndeath_major_diseases$Year &lt;- as.Date(paste(death_major_diseases$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ndeath_major_diseases$Death_Cancer &lt;- as.double(death_major_diseases$Death_Cancer)\ndeath_major_diseases$Death_Heart_Disease &lt;- as.double(death_major_diseases$Death_Heart_Disease)\ndeath_major_diseases$Death_Diabetes &lt;- as.double(death_major_diseases$Death_Diabetes)\n\n# Create time series\ncancer_ts &lt;- ts(death_major_diseases$Death_Cancer, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 12)\nheart_ts &lt;- ts(death_major_diseases$Death_Heart_Disease, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 12)\ndiabetes_ts &lt;- ts(death_major_diseases$Death_Diabetes, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 12)\n\n# Show the plots\ncancer_plot&lt;-autoplot(cancer_ts, xlab = \"Time\", ylab = \"Deaths\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Deaths Caused by Cancer')+theme_bw()\nheart_plot&lt;-autoplot(heart_ts, xlab = \"Time\", ylab = \"Deaths\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Deaths Caused by Heart Disease')+theme_bw()\ndiabetes_plot&lt;-autoplot(diabetes_ts, xlab = \"Time\", ylab = \"Deaths\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Deaths Caused by Diabetes')+theme_bw()\n\nggplotly(cancer_plot)\n\n\n\n\n\n\nCode\nggplotly(heart_plot)\n\n\n\n\n\n\nCode\nggplotly(diabetes_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\ninfant_mortality &lt;- read_xlsx(\"data/infant_mortality_rate_us_1990_2021.xlsx\")\n\n# Change Date format\ninfant_mortality$Year &lt;- as.Date(paste(infant_mortality$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ninfant_mortality$Infant_Mortality_Rates &lt;- as.double(infant_mortality$Infant_Mortality_Rates)\n\n# Create time series\ninfant_mortality_ts &lt;- ts(infant_mortality$Infant_Mortality_Rates, start = c(year(min(infant_mortality$Year)), month(min(infant_mortality$Year))), end = c(year(max(infant_mortality$Year)), month(max(infant_mortality$Year))), frequency = 12)\n\n# Show the plot\ninfant_mortality_plot&lt;-autoplot(infant_mortality_ts, xlab = \"Time\", ylab = \"Infant Mortality Rate\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Infant Mortality Rate')+theme_bw()\n\nggplotly(infant_mortality_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nhealth_insurance &lt;- read_xlsx(\"data/share_of_people_us_without_health_insurance_age_1997_2022.xlsx\")\n\n# Change Date format\nhealth_insurance$Year &lt;- as.Date(paste(health_insurance$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Change it to long format\nhealth_insurance &lt;- health_insurance %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Percentage\")\n\n# Create time series\nhealth_insurance_ts &lt;- ts(health_insurance$Percentage, start = c(year(min(health_insurance$Year)), month(min(health_insurance$Year))), end = c(year(max(health_insurance$Year)), month(max(health_insurance$Year))), frequency = 12)\n\n# Show the plot\nhealth_insurance_plot&lt;-autoplot(health_insurance_ts, xlab = \"Time\", ylab = \"Percentage\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Share of People Without Health Insurance')+theme_bw()\n\nggplotly(health_insurance_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nemergency_room &lt;- read_xlsx(\"data/us_emergency_room_visits_1997_2019_by_age.xlsx\")\n\n# Change it to long format\nemergency_room &lt;- emergency_room %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Visits_Percent\")\n\n# Change Date format\nemergency_room$Year &lt;- as.Date(paste(emergency_room$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nemergency_room_ts &lt;- ts(emergency_room$Visits_Percent, start = c(year(min(emergency_room$Year)), month(min(emergency_room$Year))), end = c(year(max(emergency_room$Year)), month(max(emergency_room$Year))), frequency = 12)\n\n# Show the plot\nemergency_room_plot&lt;-autoplot(emergency_room_ts, xlab = \"Time\", ylab = \"Percentage\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Emergency Room Visits')+theme_bw()\n\nggplotly(emergency_room_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nprescription_drug &lt;- read_xlsx(\"data/prescription_drug_expenditure_us_1960_2022.xlsx\")\n\n# Change Date format\nprescription_drug$Year &lt;- as.Date(paste(prescription_drug$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nprescription_drug_ts &lt;- ts(prescription_drug$Expenditure, start = c(year(min(prescription_drug$Year)), month(min(prescription_drug$Year))), end = c(year(max(prescription_drug$Year)), month(max(prescription_drug$Year))), frequency = 12)\n\n# Show the plot\nprescription_drug_plot&lt;-autoplot(prescription_drug_ts, xlab = \"Time\", ylab = \"Expenditure\", colour = \"#27aeef\")+ggtitle('Time Series Plot For Prescription Drug Expenditure')+theme_bw()\n\nggplotly(prescription_drug_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\ngdp_industry &lt;- read_csv(\"data/GDP_by_Industry.csv\", skip = 4) %&gt;%\n  select(-Line)\n\ncolnames(gdp_industry) &lt;- c(\"Industry\", \"2018-Q1\", \"2018-Q2\", \"2018-Q3\", \"2018-Q4\", \"2019-Q1\", \"2019-Q2\", \"2019-Q3\", \"2019-Q4\", \"2020-Q1\", \"2020-Q2\", \"2020-Q3\", \"2020-Q4\", \"2021-Q1\", \"2021-Q2\", \"2021-Q3\", \"2021-Q4\", \"2022-Q1\", \"2022-Q2\", \"2022-Q3\", \"2022-Q4\", \"2023-Q1\", \"2023-Q2\", \"2023-Q3\")\n\ngdp_healthcare &lt;- gdp_industry %&gt;%\n  filter(\n           (Industry == \"Health care and social assistance\")\n  )\n\n## Change it to long format\ngdp_healthcare &lt;- gdp_healthcare %&gt;%\n  pivot_longer(cols = -Industry, \n               names_to = \"Date\", \n               values_to = \"GDP\")\n\n# Change date to Date format\ngdp_healthcare$Date &lt;- as.yearqtr(gdp_healthcare$Date, format = \"%Y-Q%q\")\n\ngdp_healthcare$Date &lt;- as.Date(gdp_healthcare$Date)\ngdp_healthcare$GDP &lt;- as.double(gdp_healthcare$GDP)\n\ngdp_healthcare &lt;- gdp_healthcare %&gt;%\n  mutate(\n    year = year(Date)\n  )\n\n# Create time series\ngdp_healthcare_ts &lt;- ts(gdp_healthcare$GDP, start = c(year(min(gdp_healthcare$Date))), end = c(year(max(gdp_healthcare$Date))), frequency = 12)\n\n# Show the plot\ngdp_healthcare_plot&lt;-autoplot(gdp_healthcare_ts, xlab = \"Time\", ylab = \"GDP\", colour = \"#27aeef\")+ggtitle('Time Series Plot For GDP Contribution of Healthcare Industry')+theme_bw()\n\nggplotly(gdp_healthcare_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nadult_smokers &lt;- read_xlsx(\"data/number_of_adult_smokers_us_1965_2021.xlsx\")\n\n# Change Date format\nadult_smokers$Year &lt;- as.Date(paste(adult_smokers$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nadult_smokers_ts &lt;- ts(adult_smokers$Number, start = c(year(min(adult_smokers$Year)), month(min(adult_smokers$Year))), end = c(year(max(adult_smokers$Year)), month(max(adult_smokers$Year))), frequency = 12)\n\n# Show the plot\nadult_smokers_plot&lt;-autoplot(adult_smokers_ts, \n                             xlab = \"Time\", \n                             ylab = \"Number of Smokers\", \n                             colour = \"#27aeef\")+\n  ggtitle('Time Series Plot For Number of Adult Smokers')+\n  theme_bw()\n\nggplotly(adult_smokers_plot)\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nalcohol_consumption &lt;- read_xlsx(\"data/per_capita_alcohol_consumption_of_all_beverages_us_1850_2021.xlsx\")\n\n# Change Date format\nalcohol_consumption$Year &lt;- as.Date(paste(alcohol_consumption$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nalcohol_consumption_ts &lt;- ts(alcohol_consumption$Consumption, start = c(year(min(alcohol_consumption$Year)), month(min(alcohol_consumption$Year))), end = c(year(max(alcohol_consumption$Year)), month(max(alcohol_consumption$Year))), frequency = 12)\n\n# Show the plot\n\nalcohol_consumption_plot&lt;-autoplot(alcohol_consumption_ts, \n                                   xlab = \"Time\", \n                                   ylab = \"Gallons\", \n                                   colour = \"#27aeef\")+\n  ggtitle('Time Series Plot For Per Capita Alcohol Consumption')+\n  theme_bw()\n\nggplotly(alcohol_consumption_plot)\n\n\n\n\n\n\n\n\n\nThe equation for additive time series: \\[Series=Trend+Seasonal+Random\\]\nThe equation for multiplicative time series: \\[Series=Trend∗Seasonal∗Random\\]\nFinancial/Economy: Large-scale Pharmaceutical: We use Pfizer stock to represent Large-scale Pharmaceutical stocks. The trend in this series is characterized by fluctuations. There was a significant drop at the beginning of 2020, attributable to the impact of COVID-19. However, it rebounded with the introduction of vaccines. Regarding seasonality, there is no evident pattern, leading to the conclusion that the series does not display any seasonal variations. Additionally, the data behaves in a multiplicative manner.\nFinancial/Economy: Biotechnology: We use Vertex stock to represent Biotechnology stocks. The trend in this series shows an overall increase, but with noticeable fluctuations between 2020 and 2024. As for seasonality, the absence of a clear pattern leads us to conclude that the series does not exhibit seasonal variations. Furthermore, the series exhibits a multiplicative behavior.\nFinancial/Economy: Generic and Specialty Drug: We use TEVA stock to represent Generic and Specialty Drug stocks. The trend in this series initially showed an increasing pattern with fluctuations until the end of 2020, and it remained unaffected by Covid-19. Subsequently, there was a decrease marked by fluctuations. There is no apparent seasonality in the series, leading to the conclusion that it lacks seasonal patterns. Additionally, the behavior of the series is multiplicative.\nFinancial/Economy: Small and Medium-sized Biotech: We use SAGE stock to represent Small and Medium-sized Biotech stocks. The overall trend in the stock price generally shows an increase, albeit with some fluctuations. Throughout 2020, the prices mostly rose, unaffected by Covid-19. Regarding seasonality, there is no evident seasonality influencing the stock prices. The series also displays a multiplicative nature.\nPublic Health Outcomes: Average Life Expectancy: This series demonstrates a recurring pattern of one year of increases with minor fluctuations, followed by nine months of declines, and then a resurgence for about a year. The distinct up and down movement indicates clear seasonality within the data. Furthermore, the series is characterized by an additive nature.\nPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes) Each of these diseases shows repeated cycles of peaks and troughs, suggesting seasonal influences on mortality rates, possibly driven by environmental factors, health system performance variability, or seasonal health patterns. While these plots focus on seasonal variations and do not highlight a long-term trend, it does not imply the absence of such a trend. The series is additive.\nPublic Health Outcomes: Infant Mortality Rate There is a clear seasonal pattern, with sharp declines followed by immediate increases, suggesting a possible reporting or data recording cycle rather than actual fluctuations in infant mortality rates. The sharpness and regularity of the fluctuations suggest that the data might be reported or recorded at specific times of the year, leading to the sharp increases and drops. The series is additive.\nHealthcare Access and Utilization: Health Insurance Coverage The data shows significant fluctuations indicative of a seasonal pattern. The series is characterized by a multiplicative nature.\nHealthcare Access and Utilization: Emergency Room Visits A clear seasonal pattern is observed with sharp increases followed by immediate decreases. The series is multiplicative.\nHealthcare Costs: Costs of Prescription Drugs The plot shows regular fluctuations that suggest an annual cycle in expenditure, which could be influenced by various factors such as policy changes, market dynamics, or seasonal patterns in drug purchases and usage. The pattern of expenditure appears to be cyclical with sharp increases followed by decreases, indicative of a seasonal trend. This regularity suggests that there are underlying factors that consistently affect prescription drug spending at certain times of the year. A long-term trend is not immediately clear from this visual alone. The series is additive.\nHealthcate Costs: GDP Contribution of Healthcare Industry The series follows a consistent cycle every one year and nine months, marked by an increase-flat-decrease-sharp peak-decrease pattern, repeating with the same rhythm. The lack of a 12-month cycle suggests an absence of typical seasonality. The series is multiplicative.\nHealth Behavior and Risk Factors: Smoking Prevelance The trend in this series is marked by a sharp increase with minor fluctuations, followed by a sharp decrease and a subsequent resurgence. The variability in the duration of these periods precludes a definitive conclusion about seasonality. The series is additive.\nHealth Behavior and Risk Factors: Alcohol Consumpition The data shows a repeating pattern of an increase in alcohol consumption, followed by a sharp drop, then an immediate increase, and another rapid decrease. This pattern supports the presence of seasonality in the series. The series is additive.\n\n\n2. Lag Plots\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small and Medium-sized BiotechPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes)Public Health Outcomes: Infant Mortality RateHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Room VisitsHealthcare Costs: Costs of Prescription DrugsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behavior and Risk Factors: Smoking PrevalenceHealth Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\ngglagplot(pharma_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Price ($) \")+\n  ggtitle(\"Lag Plot for Pzifer Stock\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(biotech_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Price ($) \")+\n  ggtitle(\"Lag Plot for Vertex Stock\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(drug_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Price ($) \")+\n  ggtitle(\"Lag Plot for TEVA Stock\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(small_biotech_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Price ($) \")+\n  ggtitle(\"Lag Plot for SAGE Stock\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(ale_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Years\")+\n  ggtitle(\"Lag Plot for Life Expectancy\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Lage Plot of Cancer\ngglagplot(cancer_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Deaths\")+\n  ggtitle(\"Lag Plot for Deaths of Cancer\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\nCode\n# Lage Plot of Heart Disease\ngglagplot(heart_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Deaths\")+\n  ggtitle(\"Lag Plot for Deaths of Heart Disease\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\nCode\n# Lage Plot of Diabetes\ngglagplot(diabetes_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Deaths\")+\n  ggtitle(\"Lag Plot for Deaths of Diabetes\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(infant_mortality_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Deaths\")+\n  ggtitle(\"Lag Plot for Infant Mortality Rate\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(health_insurance_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Lag Plot for Health Insurance Coverage\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(emergency_room_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Visits\")+\n  ggtitle(\"Lag Plot for Emergency Room Visits\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(prescription_drug_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Costs\")+\n  ggtitle(\"Lag Plot for Costs of Prescription Drugs\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(gdp_healthcare_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Lag Plot for GDP Contribution of Healthcare Industry\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(adult_smokers_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Lag Plot for Smoking Prevalence\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ngglagplot(alcohol_consumption_ts, do.lines=FALSE) +\n  xlab(\"Lags\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Lag Plot for Alcohol Consumption\")+\n  theme(axis.text.x=element_text(angle=45, hjust=1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nFinancial/Economy: Large-scale Pharmaceutical: The data shows a strong positive autocorrelation in the initial two lags, which gradually weakens until reaching a very weak positive autocorrelation between lags 13 and 16.\nFinancial/Economy: Biotechnology: A similar pattern emerges with strong positive autocorrelation in the first two lags, which then diminishes progressively, ending in a very weak positive autocorrelation from lags 13 to 16.\nFinancial/Economy: Generic and Specialty Drug: Initially, there is strong positive autocorrelation in the first two lags, which progressively weakens, culminating in a very weak positive autocorrelation in lags 13-16.\nFinancial/Economy: Small and Medium-sized Biotech: It exhibits strong positive autocorrelation in the first two lags, which gradually decreases to a very weak positive autocorrelation by lags 13 to 16.\nPublic Health Outcomes: Average Life Expectancy: The series shows a strong positive autocorrelation in the first lag.\nPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes) There is strong positive autocorrelation in the first four lags, which progressively weakens, reaching very weak positive autocorrelation in lags 13 to 16.\nPublic Health Outcomes: Infant Mortality Rate This data indicates strong positive autocorrelation in the first three lags, which then weakens progressively to a very weak positive autocorrelation by lags 13 to 16.\nHealthcare Access and Utilization: Health Insurance Coverage There is notable strong positive autocorrelation in the first lag, which then diminishes to negligible or very weak positive autocorrelation.\nHealthcare Access and Utilization: Emergency Room Visits Actually we can find no more autocorrelation or very weak positive autocorrelation.\nHealthcare Costs: Costs of Prescription Drugs A strong positive autocorrelation is observed in the first lag, which weakens progressively, resulting in a very weak positive autocorrelation between lags 13 and 16.\nHealthcate Costs: GDP Contribution of Healthcare Industry There is strong positive autocorrelation in the first three lags, which then lessens progressively to a very weak positive autocorrelation in lags 13 to 16.\nHealth Behavior and Risk Factors: Smoking Prevelance The series begins with strong positive autocorrelation in the first two lags, which progressively decreases to a very weak positive autocorrelation by lags 13 to 16.\nHealth Behavior and Risk Factors: Alcohol Consumpition There is strong positive autocorrelation in the first lag, after which there is no more autocorrelation or only very weak positive autocorrelation.\n\n\n3. Decomposition\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small and Medium-sized BiotechPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant Mortality RateHealth Access and Utilization: Health Insurance CoverageHealth Access and Utilization: Emergency Room VisitsHealthcare Costs: Costs of Prescription DrugsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behavior and Risk Factors: Smoking PrevalenceHealth Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\ndecomposed_pharma &lt;- decompose(pharma_ts, \"multiplicative\")\nautoplot(decomposed_pharma, colour = \"#27aeef\", main = \"Decomposition Plot For Pfizer Stock\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_biotech &lt;- decompose(biotech_ts, \"multiplicative\")\nautoplot(decomposed_biotech, colour = \"#27aeef\", main = \"Decomposition Plot For Vertex Stock\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_drug &lt;- decompose(drug_ts, \"multiplicative\")\nautoplot(decomposed_drug, colour = \"#27aeef\", main = \"Decomposition Plot For TEVA Stock\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_small_biotech &lt;- decompose(small_biotech_ts, \"multiplicative\")\nautoplot(decomposed_small_biotech, colour = \"#27aeef\", main = \"Decomposition Plot For SAGE Stock\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_life_expectancy &lt;- decompose(ale_ts, \"additive\")\nautoplot(decomposed_life_expectancy, colour = \"#27aeef\", main = \"Decomposition Plot For Life Expectancy\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\n# Decomposition of Cancer\ndecomposed_cancer &lt;- decompose(cancer_ts, \"additive\")\nautoplot(decomposed_cancer, colour = \"#27aeef\", main = \"Decomposition Plot For Deaths of Cancer\")+theme_bw()\n\n\n\n\n\nCode\n# Decomposition of Heart Disease\ndecomposed_heart &lt;- decompose(heart_ts, \"additive\")\nautoplot(decomposed_heart, colour = \"#27aeef\", main = \"Decomposition Plot For Deaths of Heart Disease\")+theme_bw()\n\n\n\n\n\nCode\n# Decomposition of Diabetes\ndecomposed_diabetes &lt;- decompose(diabetes_ts, \"additive\")\nautoplot(decomposed_diabetes, colour = \"#27aeef\", main = \"Decomposition Plot For Deaths of Diabetes\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_infant_mortality &lt;- decompose(infant_mortality_ts, \"additive\")\nautoplot(decomposed_infant_mortality, colour = \"#27aeef\", main = \"Decomposition Plot For Infant Mortality Rate\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_health_insurance &lt;- decompose(health_insurance_ts, \"multiplicative\")\nautoplot(decomposed_health_insurance, colour = \"#27aeef\", main = \"Decomposition Plot For Health Insurance Coverage\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_er_visits &lt;- decompose(emergency_room_ts, \"multiplicative\")\nautoplot(decomposed_er_visits, colour = \"#27aeef\", main = \"Decomposition Plot For Emergency Room Visits\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_prescription_drug_costs &lt;- decompose(prescription_drug_ts, \"additive\")\nautoplot(decomposed_prescription_drug_costs, colour = \"#27aeef\", main = \"Decomposition Plot For Prescription Drug Costs\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_gdp_healthcare &lt;- decompose(gdp_healthcare_ts, \"multiplicative\")\nautoplot(decomposed_gdp_healthcare, colour = \"#27aeef\", main = \"Decomposition Plot For GDP Contributions of Healthcare Industry\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_smoking_prevalence &lt;- decompose(adult_smokers_ts, \"additive\")\nautoplot(decomposed_smoking_prevalence, colour = \"#27aeef\", main = \"Decomposition Plot For Smoking Prevalence\")+theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndecomposed_alcohol_consumption &lt;- decompose(alcohol_consumption_ts, \"additive\")\nautoplot(decomposed_alcohol_consumption, colour = \"#27aeef\", main = \"Decomposition Plot For Alcohol Consumption\")+theme_bw()\n\n\n\n\n\n\n\n\nIn our Part 1 - Original Data, we have already tried to examine the trend, seasonality, and noise in the time series data. In this section, we’ve decomposed our time series data into trend, seasonality, and noise components. The trend component represents the long-term progression of the time series data, the seasonal component represents the seasonal variation in the time series data, and the noise component represents the random variation in the time series data. To conclude, for all of the series we have, the decomposition plots can support our conclusions on trend, seasonality, and noise.\n\n\n4. ACF and PACF Plots\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small and Medium-sized BiotechPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant Mortality RateHealth Access and Utilization: Health Insurance CoverageHealth Access and Utilization: Emergency Room VisitsHealthcare Costs: Prescription Drug CostsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behavoir and Risk Factors: Smoking PrevalenceHealth Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\npharma_acf &lt;- ggAcf(pharma_ts)+ggtitle(\"ACF Plot for Pzifer Stock \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \n\npharma_pacf &lt;- ggPacf(pharma_ts)+ggtitle(\"PACF Plot for Pzifer Stock \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \ngrid.arrange(pharma_acf, pharma_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nbiotech_acf &lt;- ggAcf(biotech_ts)+ggtitle(\"ACF Plot for Vertex Stock \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nbiotech_pacf &lt;- ggPacf(biotech_ts)+ggtitle(\"PACF Plot for Vertex Stock \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\ngrid.arrange(biotech_acf, biotech_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ndrug_acf &lt;- ggAcf(drug_ts)+ggtitle(\"ACF Plot for TEVA Stock \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ndrug_pacf &lt;- ggPacf(drug_ts)+ggtitle(\"PACF Plot for TEVA Stock \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\ngrid.arrange(drug_acf, drug_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nsmall_biotech_acf &lt;- ggAcf(small_biotech_ts)+ggtitle(\"ACF Plot for SAGE Stock \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nsmall_biotech_pacf &lt;- ggPacf(small_biotech_ts)+ggtitle(\"PACF Plot for SAGE Stock \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\ngrid.arrange(small_biotech_acf, small_biotech_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nlife_expectancy_acf &lt;- ggAcf(ale_ts)+ggtitle(\"ACF Plot for Average Life Expectancy \") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nlife_expectancy_pacf &lt;- ggPacf(ale_ts)+ggtitle(\"PACF Plot for Average Life Expectancy \") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(life_expectancy_acf, life_expectancy_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ndeaths_cancer_acf &lt;- ggAcf(cancer_ts)+ggtitle(\"ACF Plot for Deaths of Cancer\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ndeaths_cancer_pacf &lt;- ggPacf(cancer_ts)+ggtitle(\"PACF Plot for Deaths of Cancer\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ndeaths_diabetes_acf &lt;- ggAcf(diabetes_ts)+ggtitle(\"ACF Plot for Deaths of Diabetes\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ndeaths_diabetes_pacf &lt;- ggPacf(diabetes_ts)+ggtitle(\"PACF Plot for Deaths of Diabetes\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ndeaths_heart_acf &lt;- ggAcf(heart_ts)+ggtitle(\"ACF Plot for Deaths of Heart Disease\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ndeaths_heart_pacf &lt;- ggPacf(heart_ts)+ggtitle(\"PACF Plot for Deaths of Heart Disease\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(deaths_cancer_acf, deaths_cancer_pacf, deaths_diabetes_acf, deaths_diabetes_pacf, deaths_heart_acf, deaths_heart_pacf, nrow=3)\n\n\n\n\n\n\n\n\n\nCode\ninfant_mortality_acf &lt;- ggAcf(infant_mortality_ts)+ggtitle(\"ACF Plot for Infant Mortality Rate\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ninfant_mortality_pacf &lt;- ggPacf(infant_mortality_ts)+ggtitle(\"PACF Plot for Infant Mortality Rate\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(infant_mortality_acf, infant_mortality_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nhealth_insurance_acf &lt;- ggAcf(health_insurance_ts)+ggtitle(\"ACF Plot for Health Insurance Coverage\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nhealth_insurance_pacf &lt;- ggPacf(health_insurance_ts)+ggtitle(\"PACF Plot for Health Insurance Coverage\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(health_insurance_acf, health_insurance_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nemergency_room_acf &lt;- ggAcf(emergency_room_ts)+ggtitle(\"ACF Plot for Emergency Room Visits\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nemergency_room_pacf &lt;- ggPacf(emergency_room_ts)+ggtitle(\"PACF Plot for Emergency Room Visits\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(emergency_room_acf, emergency_room_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nprescription_drug_acf &lt;- ggAcf(prescription_drug_ts)+ggtitle(\"ACF Plot for Prescription Drug Costs\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nprescription_drug_pacf &lt;- ggPacf(prescription_drug_ts)+ggtitle(\"PACF Plot for Prescription Drug Costs\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(prescription_drug_acf, prescription_drug_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\ngdp_healthcare_acf &lt;- ggAcf(gdp_healthcare_ts)+ggtitle(\"ACF Plot for GDP Contribution of Healthcare Industry\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngdp_healthcare_pacf &lt;- ggPacf(gdp_healthcare_ts)+ggtitle(\"PACF Plot for GDP Contribution of Healthcare Industry\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(gdp_healthcare_acf, gdp_healthcare_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nsmoking_acf &lt;- ggAcf(adult_smokers_ts)+ggtitle(\"ACF Plot for Smoking Prevalence\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nsmoking_pacf &lt;- ggPacf(adult_smokers_ts)+ggtitle(\"PACF Plot for Smoking Prevalence\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(smoking_acf, smoking_pacf, nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nalcohol_acf &lt;- ggAcf(alcohol_consumption_ts)+ggtitle(\"ACF Plot for Alcohol Consumption\") + theme_bw() +\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nalcohol_pacf &lt;- ggPacf(alcohol_consumption_ts)+ggtitle(\"PACF Plot for Alcohol Consumption\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(alcohol_acf, alcohol_pacf, nrow=2)\n\n\n\n\n\n\n\n\nFinancial/Economy: Large-scale Pharmaceutical: The ACF plot shows a significant spike at lag 1, which quickly drops below the significance level and moves around zero for subsequent lags. The significant autocorrelation at the first lag suggests that there is some correlation between a value and its immediate predecessor. The PACF plot has a significant spike at lag 1 too, after which the partial autocorrelations fall within the significance bounds. The ACF plot displaying a significant spike at lag 1 followed by a quick decline to non-significant levels could suggest that the series is not stationary, as non-stationary data often have one or more autocorrelation coefficients that are significantly non-zero. However, the drop off after the first lag does indicate that the series may be close to stationarity or made stationary with differencing. The PACF plot also supports this possibility, as the significant partial autocorrelation at lag 1 can be indicative of a need to difference the series once to achieve stationarity.\nFinancial/Economy: Biotechnology: There is a significant autocorrelation at the first lag, which quickly diminishes and remains within the confidence bounds for subsequent lags. This pattern suggests that the series could be integrated of order 1, meaning differencing the series once may achieve stationarity. The PACF shows a significant spike at the first lag and no other significant spikes beyond the confidence bounds for higher-order lags. This cut-off after the first lag in the PACF suggests that an AR(1) model might be appropriate for the data after it has been differenced once. The Vertex stock time series seems to exhibit non-stationarity due to the significant autocorrelation at lag 1 in the ACF plot. The lack of other significant lags in both ACF and PACF plots also suggests that the series does not have a complex seasonal or autoregressive structure.\nFinancial/Economy: Generic and Specialty Drug: There is a gradual decline in the ACF values as the lags increase, which suggests that the series could be an AR process because the autocorrelations are slowly decaying. The ACF values are positive and significant for a number of lags, which implies that there is a positive correlation between the current value and its past values. The PACF cuts off after the first lag, which is typically indicative of an AR(1) process. This means that after accounting for the first lag, the partial autocorrelations of the following lags are not significant. The slow decay of the ACF plot suggests that the series may be non-stationary, as a stationary time series would typically show a quick drop-off in the ACF values. However, the definitive conclusion about stationarity cannot be drawn from the ACF and PACF plots alone.\nFinancial/Economy: Small and Medium-sized Biotech: In the ACF plot for SAGE stock, we observe that the first lag is significant as the bar extends beyond the confidence interval bounds, and there are a few other lags that also appear significant, suggesting some level of autocorrelation in the data. The ACF bars decrease as the lags increase, but since they are significant at various points, it suggests that the data might not be stationary as one of the indicators of stationarity is that autocorrelations should quickly drop off to zero as the lag increases. In the PACF plot, the first lag is also significant, and the subsequent lags are mostly within the bounds, suggesting that there is no significant partial autocorrelation at those lags. This could mean that after accounting for the immediate past value (lag 1), the autocorrelation with further past values is not significant.\nPublic Health Outcomes: Average Life Expectancy: The ACF plot for average life expectancy shows significant autocorrelation at multiple lag intervals, which does not quickly decay as the lags increase. This suggests the data may exhibit a non-stationary behavior because in a stationary time series, we would expect the ACF values to decrease rapidly. The PACF plot indicates a significant spike at the first lag and a few others sporadically, but it is not as systematic as in the ACF plot, suggesting that the immediate past value has a strong relationship with the current value, but this relationship does not persist strongly at higher lags. It’s likely that the time series is non-stationary and may require differencing to achieve stationarity.\nPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes) For Deaths of Cancer, the ACF plot shows significant autocorrelations at multiple lags, which do not taper off, indicating a non-stationary series. The PACF plot shows a sharp cut-off after the first lag, suggesting an AR(1) process might be appropriate after differencing. For Deaths of Diabetes, the ACF plot again shows a gradual decline in autocorrelation but with several significant lags, hinting at a non-stationary series. The PACF plot has a significant spike at the last lag, which could suggest a seasonal component or the need for a higher-order AR model. For Deaths of Heart Disease, the ACF plot shows a slow decay in autocorrelations with many significant lags, suggesting non-stationarity. The PACF plot is similar to that of cancer deaths, with a significant spike at lag 1 and then a cut-off, indicating a possible AR(1) process after differencing.\nPublic Health Outcomes: Infant Mortality Rate The ACF plot for infant mortality rate shows a slow decay in autocorrelations with many significant lags, suggesting non-stationary. The PACF plot shows a significant spike at lag 1 and then a cut-off.\nHealthcare Access and Utilization: Health Insurance Coverage The ACF plot for health insurance coverage shows a few significant autocorrelations at non-consecutive lags, indicating that there are some individual past values that have a correlation with the current value. However, there isn’t a clear decay pattern, which can sometimes be indicative of a non-stationary series. The PACF plot shows a significant partial autocorrelation at several lags and all other lags are within the confidence interval.\nHealthcare Access and Utilization: Emergency Room Visits The ACF plot for emergency room visits shows significant autocorrelations at several lags which are somewhat spread out and not following a clear decay pattern. This could suggest a complex seasonality or that the data is non-stationary. The PACF plot reveals a significant partial autocorrelation at the first several lags, but the subsequent lags are within the confidence bounds.\nHealthcare Costs: Costs of Prescription Drugs The ACF plot for prescription drug costs shows a significant autocorrelation at the first 3 lags, then occasional significant autocorrelations at various other lags without a consistent pattern. This suggests that there are correlations at specific points in time, but not in a regular, decaying manner as one would expect in a stationary series. The PACF plot has a significant spike at the first lag and other significant negative spikes around lag 18.\nHealthcate Costs: GDP Contribution of Healthcare Industry The ACF plot for GDP contribution of the healthcare industry shows that autocorrelations are significant at a few lags, but there is no clear pattern of decay, which often suggests non-stationarity. The PACF plot shows a random pattern, with most of the lags falling within the confidence bounds. This indicates that there are no significant partial autocorrelations after accounting for previous lags. Given these characteristics, the time series may be stationary, but the presence of significant autocorrelations at a few lags in the ACF suggests that the series could have some autoregressive elements.\nHealth Behavior and Risk Factors: Smoking Prevelance The first lag is significantly positive, which suggests a strong correlation with the immediate past value. Several other lags are also significant, though not in a consistent or decaying pattern. This could indicate a complex process with multiple periods influencing the series. In the PACF plot, there’s a significant spike at the first lag, which again indicates that the previous value has a significant effect on the current value. The presence of multiple significant lags in the ACF without a clear decay pattern could suggest that the series is not stationary and might be influenced by seasonal or cyclical components.\nHealth Behavior and Risk Factors: Alcohol Consumpition There are several lags where the autocorrelation is outside of the confidence bounds, indicating significant autocorrelation at these specific lags. For the PACF plot, the partial autocorrelations are mostly within the confidence bounds. The ACF plot suggests that there is some underlying pattern or seasonality to the alcohol consumption data, as indicated by the significant lags. This pattern is not captured by the PACF, which suggests that the relationship is not purely autoregressive. Given the significant lags in the ACF, a more complex model than a simple AR(1) might be needed, potentially an ARIMA model with seasonal components, to account for the periodic spikes in the ACF. The data may not be stationary, or it may exhibit seasonality that should be accounted for in the time series model.\n\n\n5. Dickey-Fuller Test\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small and Medium-sized BiotechPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant Mortality RateHealth Access and Utilization: Health Insurance CoverageHealth Access and Utilization: Emergency Room VisitsHealthcare Costs: Costs of Prescription DrugsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behavoir and Risk Factors: Smoking PrevalenceHealth Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\ntseries::adf.test(pharma_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  pharma_ts\nDickey-Fuller = -3.0732, Lag order = 3, p-value = 0.1405\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(biotech_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  biotech_ts\nDickey-Fuller = -4.1194, Lag order = 3, p-value = 0.01028\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(drug_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  drug_ts\nDickey-Fuller = -2.7568, Lag order = 3, p-value = 0.2686\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(small_biotech_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  small_biotech_ts\nDickey-Fuller = -1.686, Lag order = 3, p-value = 0.7019\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(ale_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ale_ts\nDickey-Fuller = -10.086, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(cancer_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  cancer_ts\nDickey-Fuller = -12.263, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(diabetes_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diabetes_ts\nDickey-Fuller = -11.718, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\ntseries::adf.test(heart_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  heart_ts\nDickey-Fuller = -10.818, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(infant_mortality_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  infant_mortality_ts\nDickey-Fuller = -6.7832, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(health_insurance_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  health_insurance_ts\nDickey-Fuller = -3.2908, Lag order = 6, p-value = 0.07302\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(emergency_room_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  emergency_room_ts\nDickey-Fuller = -6.0747, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(prescription_drug_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  prescription_drug_ts\nDickey-Fuller = -11.269, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(gdp_healthcare_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  gdp_healthcare_ts\nDickey-Fuller = -2.8506, Lag order = 3, p-value = 0.2308\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(adult_smokers_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  adult_smokers_ts\nDickey-Fuller = -10.3, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(alcohol_consumption_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  alcohol_consumption_ts\nDickey-Fuller = -19.821, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nFinancial/Economy: Large-scale Pharmaceutical: With a p-value greater than 0.05, there is insufficient evidence to reject the null hypothesis at the 5% significance level, indicating that the series is likely non-stationary. Additional adjustments may be necessary to achieve stationarity.\nFinancial/Economy: Biotechnology: The p-value is below 0.05, allowing us to reject the null hypothesis at the 5% significance level. This suggests that the series is stationary based on the Dickey-Fuller test, though further verification may be required.\nFinancial/Economy: Generic and Specialty Drug: Since the p-value exceeds 0.05, we lack sufficient evidence to reject the null hypothesis at the 5% significance level, suggesting non-stationarity. Adjustments are needed to render the series stationary.\nFinancial/Economy: Small and Medium-sized Biotech: The p-value being greater than 0.05 indicates insufficient evidence to reject the null hypothesis, suggesting that the series is non-stationary. Further modifications are necessary to stabilize the series.\nPublic Health Outcomes: Average Life Expectancy: With a p-value less than 0.05, we can reject the null hypothesis at the 5% significance level, suggesting that the series is stationary as per the Dickey-Fuller test. Additional analysis may be needed for confirmation.\nPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes) All p-values are below 0.05, enabling us to reject the null hypothesis, indicating stationarity in the series according to the Dickey-Fuller test. Further analysis might be required for confirmation.\nPublic Health Outcomes: Infant Mortality Rate The p-value being below 0.05 means the null hypothesis can be rejected at the 5% significance level, suggesting stationarity in the series per the Dickey-Fuller test. This conclusion may require further validation.\nHealthcare Access and Utilization: Health Insurance Coverage A p-value above 0.05 means we cannot reject the null hypothesis at the 5% level, indicating non-stationarity. Additional measures are required to make the series stationary.\nHealthcare Access and Utilization: Emergency Room Visits The p-value being below 0.05 allows for the rejection of the null hypothesis at the 5% significance level, suggesting stationarity according to the Dickey-Fuller test. Further confirmation may be necessary.\nHealthcare Costs: Costs of Prescription Drugs With a p-value less than 0.05, the null hypothesis can be rejected at the 5% significance level, suggesting that the series is stationary as per the Dickey-Fuller test. Additional analysis might be required.\nHealthcate Costs: GDP Contribution of Healthcare Industry With the p-value exceeding 0.05, there is insufficient evidence to reject the null hypothesis, indicating that the series is non-stationary. Further adjustments are needed to achieve stationarity.\nHealth Behavior and Risk Factors: Smoking Prevelance The p-value being below 0.05 enables the rejection of the null hypothesis at the 5% significance level, indicating stationarity in the series according to the Dickey-Fuller test. Further analysis might be warranted.\nHealth Behavior and Risk Factors: Alcohol Consumpition A p-value below 0.05 allows us to reject the null hypothesis at the 5% significance level, suggesting that the series is stationary per the Dickey-Fuller test. However, further analysis may be necessary to confirm this result.\n\n\n6. Detrend vs. Difference\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small and Medium-sized BiotechPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes)Public Health Outcomes: Infant Mortality RateHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Room VisitsHealthcare Costs: Prescription Drug CostsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behaviors: Smoking PrevalenceHealth Behaviors: Alcohol Consumption\n\n\n\n\nCode\nrequire(gridExtra)\nfit_pharma = lm(pharma_ts~time(pharma_ts), na.action=NULL) \nplot1&lt;-autoplot(resid(fit_pharma), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(pharma_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_biotech = lm(biotech_ts~time(biotech_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_biotech), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(biotech_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_drug = lm(drug_ts~time(drug_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_drug), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(drug_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_small_biotech = lm(small_biotech_ts~time(small_biotech_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_small_biotech), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(small_biotech_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_life_expectancy = lm(ale_ts~time(ale_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_life_expectancy), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(ale_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_cancer = lm(cancer_ts~time(cancer_ts), na.action=NULL)\nplot1_cancer&lt;-autoplot(resid(fit_cancer), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2_cancer&lt;-autoplot(diff(cancer_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\nfit_heart = lm(heart_ts~time(heart_ts), na.action=NULL)\nplot1_heart&lt;-autoplot(resid(fit_heart), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2_heart&lt;-autoplot(diff(heart_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\nfit_diabetes = lm(diabetes_ts~time(diabetes_ts), na.action=NULL)\nplot1_diabetes&lt;-autoplot(resid(fit_diabetes), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2_diabetes&lt;-autoplot(diff(diabetes_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1_cancer, plot2_cancer, plot1_heart, plot2_heart, plot1_diabetes, plot2_diabetes, nrow=3)\n\n\n\n\n\n\n\n\n\nCode\nfit_infant_mortality = lm(infant_mortality_ts~time(infant_mortality_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_infant_mortality), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(infant_mortality_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_health_insurance = lm(health_insurance_ts~time(health_insurance_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_health_insurance), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(health_insurance_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_er_visits = lm(emergency_room_ts~time(emergency_room_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_er_visits), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(emergency_room_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_prescription_costs = lm(prescription_drug_ts~time(prescription_drug_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_prescription_costs), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(prescription_drug_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_gdp_healthcare = lm(gdp_healthcare_ts~time(gdp_healthcare_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_gdp_healthcare), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(gdp_healthcare_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_smoking = lm(adult_smokers_ts~time(adult_smokers_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_smoking), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(adult_smokers_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nfit_alcohol = lm(alcohol_consumption_ts~time(alcohol_consumption_ts), na.action=NULL)\nplot1&lt;-autoplot(resid(fit_alcohol), main=\"Detrended\", colour = \"#27aeef\") + theme_bw()\nplot2&lt;-autoplot(diff(alcohol_consumption_ts), main=\"First Difference\", colour = \"#27aeef\") + theme_bw()\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\nDetrending and first differencing are two techniques used to transform time series data towards stationarity, but they differ in their methods. Detrending involves removing the underlying trend from the data by estimating the trend component and subtracting it from the original series. This method primarily focuses on eliminating the mean trend but may leave other non-stationary elements such as seasonality or variance changes.\nOn the other hand, first differencing works by calculating the difference between consecutive observations in the series, using the formula:\n\\[\n\\Delta y_t = y_t - y_{t-1}\n\\]\nThis method not only addresses the linear trends by focusing on the changes between observations but also helps in stabilizing the mean of the series. However, it might not effectively manage non-linear trends or pronounced seasonal patterns. Differencing is particularly useful when the series exhibits consistent upward or downward trends.\nChoosing between detrending and first differencing depends on the characteristics of the specific dataset involved. Each method has its strengths and is suited to different types of trend and non-stationarity within the time series.\n\n\n7. Original vs. First Difference\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small BiotechnologyPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant MortalityHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Room VisitsHealthcare Costs: Costs of Prescription DrugsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behavior and Risk Factors: Smoking PrevalenceHealth Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\nplot1 &lt;- ggAcf(pharma_ts, 48, main=\"Original Data: Pzifer Stock\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \nplot2 &lt;- ggAcf(resid(fit_pharma), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \nplot3 &lt;- ggAcf(diff(pharma_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(biotech_ts, 48, main=\"Original Data: Biotech Stock\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_biotech), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(biotech_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(drug_ts, 48, main=\"Original Data: Drug Stock\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_drug), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(drug_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(small_biotech_ts, 48, main=\"Original Data: Small Biotech Stock\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_small_biotech), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(small_biotech_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(ale_ts, 48, main=\"Original Data: Life Expectancy\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_life_expectancy), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(ale_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\n# Cancer\nplot1_cancer &lt;- ggAcf(cancer_ts, 48, main=\"Original Data: Deaths of Cancer\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2_cancer &lt;- ggAcf(resid(fit_cancer), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3_cancer &lt;- ggAcf(diff(cancer_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1_cancer, plot2_cancer, plot3_cancer,ncol=3)\n\n\n\n\n\nCode\n# Heart Disease\nplot1_heart &lt;- ggAcf(heart_ts, 48, main=\"Original Data: Deaths of Heart Disease\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2_heart &lt;- ggAcf(resid(fit_heart), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3_heart &lt;- ggAcf(diff(heart_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1_heart, plot2_heart, plot3_heart,ncol=3)\n\n\n\n\n\nCode\n# Diabetes\nplot1_diabetes &lt;- ggAcf(diabetes_ts, 48, main=\"Original Data: Deaths of Diabetes\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2_diabetes &lt;- ggAcf(resid(fit_diabetes), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3_diabetes &lt;- ggAcf(diff(diabetes_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1_diabetes, plot2_diabetes, plot3_diabetes,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(infant_mortality_ts, 48, main=\"Original Data: Infant Mortality\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_infant_mortality), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(infant_mortality_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(health_insurance_ts, 48, main=\"Original Data: Health Insurance Coverage\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_health_insurance), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(health_insurance_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(emergency_room_ts, 48, main=\"Original Data: Emergency Room Visits\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_er_visits), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(emergency_room_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(prescription_drug_ts, 48, main=\"Original Data: Costs of Prescription Drugs\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_prescription_costs), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(prescription_drug_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(gdp_healthcare_ts, 48, main=\"Original Data: GDP Contribution of Healthcare Industry\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_gdp_healthcare), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(gdp_healthcare_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(adult_smokers_ts, 48, main=\"Original Data: Smoking Prevalence\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_smoking), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(adult_smokers_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\n\nCode\nplot1 &lt;- ggAcf(alcohol_consumption_ts, 48, main=\"Original Data: Alcohol Consumption\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot2 &lt;- ggAcf(resid(fit_alcohol), 48, main=\"Detrended Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\nplot3 &lt;- ggAcf(diff(alcohol_consumption_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\ngrid.arrange(plot1, plot2, plot3,ncol=3)\n\n\n\n\n\n\n\n\nFirst differencing is a widely used technique to transform time series data into a stationary state. This involves computing the difference between consecutive values in the series, as represented by the formula:\n\\[\n\\Delta y_t = y_t - y_{t-1}\n\\]\nIn the formula, we have the following variables:\n\n\\(\\Delta y_t\\): the difference at time t\n\\(y_t\\): the value of the series at time t\n\\(y_{t−1}\\): the value of the series at time t−1\n\nThis method effectively removes linear trends, potentially leading to stationarity. It is particularly useful when the series displays consistent upward or downward trends. However, first differencing does not guarantee stationarity in all cases; some series may require additional transformations, such as second differencing or other methods, to achieve full stationarity.\nThe plots provided in this section illustrate the transformation from original to first differenced data. Most of the series appear to become stationary following this transformation, although a few retain minor autocorrelations (values slightly over 0.2, but not significant), except for the Health Insurance Coverage Series and Emergency Room Visits Series. For these, second differencing might be necessary to achieve stationarity.\n\n\n8. First vs. Second Difference\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small BiotechnologyPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant Mortality RateHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Room VisitsHealthcare Costs: Prescription Drug CostsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behaviors: Smoking PrevalenceHealth Behaviors: Alcohol Consumption\n\n\n\n\nCode\nggAcf(diff(pharma_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \n\n\n\n\n\nCode\nggAcf(diff(diff(pharma_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(biotech_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\") \n\n\n\n\n\nCode\nggAcf(diff(diff(biotech_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(drug_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(drug_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(small_biotech_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(small_biotech_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(ale_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(ale_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\n# Cancer\n\nggAcf(diff(cancer_ts), 48, main=\"First Differenced Data(Cancer)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(cancer_ts)), 48, main=\"Second Differenced Data(Cancer)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\n# Heart Disease\n\nggAcf(diff(heart_ts), 48, main=\"First Differenced Data(Heart Disease)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(heart_ts)), 48, main=\"Second Differenced Data(Heart Disease)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\n# Diabetes\n\nggAcf(diff(diabetes_ts), 48, main=\"First Differenced Data(Diabetes)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(diabetes_ts)), 48, main=\"Second Differenced Data(Diabetes)\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(infant_mortality_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(infant_mortality_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(health_insurance_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(health_insurance_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(emergency_room_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(emergency_room_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(prescription_drug_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(prescription_drug_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(gdp_healthcare_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(gdp_healthcare_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(adult_smokers_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(adult_smokers_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\n\nCode\nggAcf(diff(alcohol_consumption_ts), 48, main=\"First Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\nCode\nggAcf(diff(diff(alcohol_consumption_ts)), 48, main=\"Second Differenced Data\") + theme_bw()+\n  geom_segment(lineend = \"butt\", color = \"#27aeef\") +\n    geom_hline(yintercept = 0, color = \"#27aeef\")\n\n\n\n\n\n\n\n\nFirst differencing and second differencing are techniques aimed at transforming a time series into a stationary state by addressing different types of non-stationarity. The primary difference between these techniques lies in the number of times the differencing operation is applied, as well as their effectiveness in handling various trends within the data.\nFirst differencing is typically employed to eliminate linear trends in a time series. It involves subtracting the current value of the series from its preceding value, as follows: \\[\n\\Delta y_t = y_t - y_{t-1}\n\\]\nIf the series remains non-stationary after the application of first differencing, second differencing may be used. This technique is more suited to removing quadratic trends or further stabilizing a series. Second differencing involves applying the differencing operation again to the series that has already undergone first differencing. The formula for second differencing is:\n\\[\n\\Delta^2y_t = (\\Delta y_t) - (\\Delta y_{t-1}) = y_t - 2y_{t-1} + y_{t-2}\n\\] This method further refines the series by addressing deeper or more complex trends that first differencing may not fully resolve.\n\n\n9. Adjusted Dickey-Fuller Test\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small BiotechnologyPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant Mortality RateHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Room VisitsHealthcare Costs: Prescription Drug CostsHealthcare Costs: GDP Contribution of Healthcare IndustryHealth Behaviors and Risk Factors: Smoking PrevalenceHealth Behaviors and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\ntseries::adf.test(diff(pharma_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(pharma_ts)\nDickey-Fuller = -5.1894, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(biotech_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(biotech_ts)\nDickey-Fuller = -4.3169, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(drug_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(drug_ts)\nDickey-Fuller = -3.0797, Lag order = 3, p-value = 0.1382\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(small_biotech_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(small_biotech_ts)\nDickey-Fuller = -5.8674, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(ale_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(ale_ts)\nDickey-Fuller = -6.0981, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Cancer\ntseries::adf.test(diff(cancer_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(cancer_ts)\nDickey-Fuller = -10.418, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\n# Heart Disease\ntseries::adf.test(diff(heart_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(heart_ts)\nDickey-Fuller = -12.4, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\n# Diabetes\ntseries::adf.test(diff(diabetes_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(diabetes_ts)\nDickey-Fuller = -11.59, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(infant_mortality_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(infant_mortality_ts)\nDickey-Fuller = -8.4623, Lag order = 7, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(health_insurance_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(health_insurance_ts)\nDickey-Fuller = -8.6172, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(emergency_room_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(emergency_room_ts)\nDickey-Fuller = -9.7162, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(prescription_drug_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(prescription_drug_ts)\nDickey-Fuller = -13.204, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(gdp_healthcare_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(gdp_healthcare_ts)\nDickey-Fuller = -4.3807, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(adult_smokers_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(adult_smokers_ts)\nDickey-Fuller = -11.855, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\ntseries::adf.test(diff(alcohol_consumption_ts))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(alcohol_consumption_ts)\nDickey-Fuller = -22.833, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nBased on the results presented above, we have successfully transformed all the data series into stationary forms, except for the time series of Generic and Specialty Drug stock. The p-values for all other tests are below 0.05, providing sufficient evidence to reject the null hypothesis at the 5% significance level. Consequently, we can conclude that all series, with the exception of the Generic and Specialty Drug Stock series, are stationary. Further investigation into this particular stock may be warranted.\n\n\n10. Moving Average Smoothing\n\nFinancial/Economy: Large-scale PharmaceuticalFinancial/Economy: BiotechnologyFinancial/Economy: Generic and Specialty DrugFinancial/Economy: Small BiotechnologyPublic Health Outcomes: Average Life ExpectancyPublic Health Outcomes: Deaths of Major DiseasesPublic Health Outcomes: Infant Mortality RateHealthcare Access and Utilization: Health Insurance CoverageHealthcare Access and Utilization: Emergency Department VisitsHealthcare Costs: Prescription Drug CostsHealthcare Costs: GDP Contribution of Healthcare IndustryHealthcare Behavior and Risk Factors: Smoking PrevalenceHealthcare Behavior and Risk Factors: Alcohol Consumption\n\n\n\n\nCode\npharma_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(pharma_ts)),\n  Value = as.numeric(pharma_ts)\n)\n\n# Plot using ggplot\nggplot(pharma_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Pzifer Stock with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\nbiotech_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(biotech_ts)),\n  Value = as.numeric(biotech_ts)\n)\n\n# Plot using ggplot\nggplot(biotech_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Vertex Stock with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\ndrug_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(drug_ts)),\n  Value = as.numeric(drug_ts)\n)\n\n# Plot using ggplot\nggplot(drug_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For TEVA Stock with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\nsmallbiotech_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(small_biotech_ts)),\n  Value = as.numeric(small_biotech_ts)\n)\n\n# Plot using ggplot\nggplot(smallbiotech_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For SAGE Stock with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\nlife_expectancy_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(ale_ts)),\n  Value = as.numeric(ale_ts)\n)\n\n# Plot using ggplot\nggplot(life_expectancy_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Life Expectancy with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Cancer\ncancer_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(cancer_ts)),\n  Value = as.numeric(cancer_ts)\n)\n\n# Plot using ggplot\nggplot(cancer_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Cancer Deaths with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\nCode\n# Heart Disease\nheart_disease_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(heart_ts)),\n  Value = as.numeric(heart_ts)\n)\n\n# Plot using ggplot\nggplot(heart_disease_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Heart Disease Deaths with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\nCode\n# Diabetes\ndiabetes_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(diabetes_ts)),\n  Value = as.numeric(diabetes_ts)\n)\n\n# Plot using ggplot\nggplot(diabetes_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Diabetes Deaths with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Infant Mortality Rate\ninfant_mortality_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(infant_mortality_ts)),\n  Value = as.numeric(infant_mortality_ts)\n)\n\n# Plot using ggplot\nggplot(infant_mortality_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Infant Mortality Rate with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Health Insurance Coverage\nhealth_insurance_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(health_insurance_ts)),\n  Value = as.numeric(health_insurance_ts)\n)\n\n# Plot using ggplot\nggplot(health_insurance_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Health Insurance Coverage with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Emergency Department Visits\nemergency_room_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(emergency_room_ts)),\n  Value = as.numeric(emergency_room_ts)\n)\n\n# Plot using ggplot\nggplot(emergency_room_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Emergency Department Visits with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Prescription Drug Costs\nprescription_drug_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(prescription_drug_ts)),\n  Value = as.numeric(prescription_drug_ts)\n)\n\n# Plot using ggplot\nggplot(prescription_drug_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Prescription Drug Costs with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# GDP Contribution of Healthcare Industry\ngdp_healthcare_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(gdp_healthcare_ts)),\n  Value = as.numeric(gdp_healthcare_ts)\n)\n\n# Plot using ggplot\nggplot(gdp_healthcare_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For GDP Contribution of Healthcare Industry with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Smoking Prevalence\nsmoking_prevalence_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(adult_smokers_ts)),\n  Value = as.numeric(adult_smokers_ts)\n)\n\n# Plot using ggplot\nggplot(smoking_prevalence_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Smoking Prevalence with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\n\nCode\n# Alcohol Consumption\nalcohol_consumption_df &lt;- data.frame(\n  Date = seq(as.Date(\"2019-01-01\"), by = \"months\", length.out = length(alcohol_consumption_ts)),\n  Value = as.numeric(alcohol_consumption_ts)\n)\n\n# Plot using ggplot\nggplot(alcohol_consumption_df, aes(x = Date, y = Value)) + \n  geom_line(aes(color = \"Data\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 3, mean, fill = NA), color = \"3-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 7, mean, fill = NA), color = \"7-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 15, mean, fill = NA), color = \"15-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 27, mean, fill = NA), color = \"27-MA\")) +\n  geom_line(aes(y = zoo::rollapply(Value, 39, mean, fill = NA), color = \"39-MA\")) +\n  xlab(\"Time\") + ylab(\"Price\") +\n  ggtitle('Time Series Plot For Alcohol Consumption with Moving Averages') +\n  theme_bw()+\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\n                               \"3-MA\"=\"violet\",\n                               \"7-MA\"=\"pink\",\n                               \"15-MA\"=\"blue\",\n                               \"27-MA\"=\"#5a3196\",\n                               \"39-MA\"=\"lightblue\"),\n                      breaks=c(\"Data\", \"3-MA\", \"7-MA\", \"15-MA\", \"27-MA\", \"39-MA\"))\n\n\n\n\n\n\n\n\nIn my analysis, I’ve incorporated five moving average windows to study the data—three short-term moving averages of 3 and 7, two medium-term windows of 15 and 27, and one long-term moving average of 39.\nFrom this approach, the 7-day moving average (7-MA) appears to be the most effective choice across all series for capturing relevant trends while smoothing out noise.\nFinancial/Economy: Large-scale Pharmaceutical: The best choice of the moving average for Large-scale Pharmaceutical stock series seems to be 7-MA.\nFinancial/Economy: Biotechnology: The best choice of the moving average for Biotechnology stock series seems to be 7-MA.\nFinancial/Economy: Generic and Specialty Drug: The best choice of the moving average for Generic and Specialty Drug stock series seems to be 7-MA.\nFinancial/Economy: Small and Medium-sized Biotech: The best choice of the moving average for Small and Medium-sized Biotech stock series seems to be 7-MA.\nPublic Health Outcomes: Average Life Expectancy: The best choice of the moving average for Average Life Expectancy series seems to be 7-MA.\nPublic Health Outcomes: Deaths of Major Diseases(Cancer, Heart Disease, Diabetes) The best choice of the moving average for Deaths of Major Diseases(Cancer, Heart Disease, Diabetes) series seems to be 7-MA.\nPublic Health Outcomes: Infant Mortality Rate The best choice of the moving average for Infant Mortality Rate series seems to be 7-MA.\nHealthcare Access and Utilization: Health Insurance Coverage The best choice of the moving average for Health Insurance Coverage series seems to be 7-MA.\nHealthcare Access and Utilization: Emergency Room Visits The best choice of the moving average for Emergency Room Visits series seems to be 7-MA.\nHealthcare Costs: Costs of Prescription Drugs The best choice of the moving average for Costs of Prescription Drugs series seems to be 7-MA.\nHealthcate Costs: GDP Contribution of Healthcare Industry The best choice of the moving average for GDP Contribution of Healthcare Industry series seems to be 7-MA.\nHealth Behavior and Risk Factors: Smoking Prevelance The best choice of the moving average for Smoking Prevelance series seems to be 7-MA.\nHealth Behavior and Risk Factors: Alcohol Consumpition The best choice of the moving average for Alcohol Consumpition series seems to be 7-MA.\nThe 3 moving average (3-MA) doesn’t show significant deviation from the original data, indicating it may not smooth the fluctuations effectively. On the other hand, the 15, 27, and 39 moving averages (15-MA, 27-MA, and 39-MA) overly smooth the data in my datasets, to the extent that finer details like seasonality are obscured. This makes it difficult to discern other underlying patterns within the data."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Importing Libraries\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(readxl)\nuse_python(\"/usr/local/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\nuse_condaenv(\"base\", required = TRUE)\n# py_install(\"tensorflow\")\n\n\n\n1. Financial/Economy(Healthcare Stocks with Shiny)\nYou can find the interactive version of this application here.\n    \nFrom the charts above, it is evident that nearly all stocks have shown an upward trajectory since 2010, despite some volatility, until they began to decline in 2020. The influence of the pandemic on these trends is also clearly noticeable. Notably, the highest stock prices belong to large pharmaceutical companies, followed by biotechnology firms. These sectors are expected to significantly influence and contribute to the national GDP.\nIn particular, the stock prices of MRK, PFE, and LLY have been on the rise since 2020. LLY has experienced a marked increase in its stock value in recent years, primarily due to its successful development of the GLP-1 drug, initially aimed at treating diabetes and now also marketed for weight loss. Similarly, PFE and MRNA have seen their stock prices increase following the development of the COVID-19 vaccine, reflecting their gains since 2020.\n\n\n2. Public Health Outcomes\n\nAverage Life Expectancy in the USTotal Deaths of Covid-19Deaths of Major Diseases(Cancer, Heart Disease, Diabetes)Infant Mortality Rates\n\n\n\n\nCode\n# Importing the dataset\naverage_life_expectancy &lt;- read_xlsx(\"data/life_expectancy_7countries_2000_2022.xlsx\")\n\n# Change it to long format\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Country\", \n               values_to = \"Life_Expectancy\")\n\naverage_life_expectancy$Year &lt;- as.Date(paste(average_life_expectancy$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\naverage_life_expectancy$Life_Expectancy &lt;- as.double(average_life_expectancy$Life_Expectancy)\n\n# Filter to only include the US\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  filter(Country == \"United States\")\n\n\n\n\nCode\n# Plotting the data\naverage_life_expectancy_plot &lt;- ggplot(average_life_expectancy,\n                                       aes(x = Year, y = Life_Expectancy)) +\n  geom_line(color = 'red') +\n  geom_point(color = 'red') +\n  labs(title = \"Average Life Expectancy in the US\",\n       x = \"Year\",\n       y = \"Life Expectancy(years)\") +\n  theme(\n    axis.text = element_text(size = 11),\n    axis.title = element_text(size = 11, face='bold'),\n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_life &lt;- ggplotly(average_life_expectancy_plot)\nplotly_life\n\n\n\n\n\n\n\n\nAn interactive version of this dashboard can be found here.\n\n\n\n\n\nCode\n# Importing the dataset\ndeath_cancer &lt;- read_xlsx(\"data/deaths_cancer_us_1950_2019.xlsx\")\ndeath_heart &lt;- read_xlsx(\"data/deaths_heart_diseases_us_1950_2019.xlsx\")\ndeath_diabetes &lt;- read_xlsx(\"data/deaths_diabetes_us_1950_2019.xlsx\")\n\n# Merge these three datasets\ndeath_major_diseases &lt;- merge(death_cancer, death_heart, by = \"Year\")\ndeath_major_diseases &lt;- merge(death_major_diseases, death_diabetes, by = \"Year\")\n\n# Change Date format\ndeath_major_diseases$Year &lt;- as.Date(paste(death_major_diseases$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ndeath_major_diseases$Death_Cancer &lt;- as.double(death_major_diseases$Death_Cancer)\ndeath_major_diseases$Death_Heart_Disease &lt;- as.double(death_major_diseases$Death_Heart_Disease)\ndeath_major_diseases$Death_Diabetes &lt;- as.double(death_major_diseases$Death_Diabetes)\n\n# Change it to long format\n\ndeath_major_diseases &lt;- death_major_diseases %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Disease\", \n               values_to = \"Deaths\")\n\n\n\n\nCode\n# Plotting the data\ndeath_major_diseases_plot &lt;- ggplot(death_major_diseases, \n                                       aes(x = Year, y = Deaths, color = Disease)) +\n  geom_line() +\n  geom_point(color = 'purple') +\n  labs(title = \"Deaths of Major Diseases in the US\",\n       x = \"Year\",\n       y = \"Number of Deaths(per 100,000)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_disease &lt;- ggplotly(death_major_diseases_plot)\nplotly_disease\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\ninfant_mortality &lt;- read_xlsx(\"data/infant_mortality_rate_us_1990_2021.xlsx\")\n\n# Change Date format\ninfant_mortality$Year &lt;- as.Date(paste(infant_mortality$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ninfant_mortality$Infant_Mortality_Rates &lt;- as.double(infant_mortality$Infant_Mortality_Rates)\n\n\n\n\nCode\n# Plotting the data, area plot\ninfant_mortality_plot &lt;- ggplot(infant_mortality, \n                                aes(x = Year, y = Infant_Mortality_Rates)) +\n  geom_area(fill = \"lightblue\", alpha = 0.6) +\n  labs(title = \"Infant Mortality Rates in the US\",\n       x = \"Year\",\n       y = \"Infant Deaths Number(per 1,000 live births)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_infant_mortality &lt;- ggplotly(infant_mortality_plot)\nplotly_infant_mortality\n\n\n\n\n\n\n\n\n\nWe can observe the public health outcomes of the United States from the graphs above. With improvements in quality of life and advances in medical technology, the average life expectancy in the U.S. has significantly increased over the past few decades, and the number of deaths due to cancer, diabetes, and heart disease has decreased. However, in 2020, there was a significant drop in the average life expectancy of Americans, from 79 to 77 years. We can speculate that this was caused by the COVID-19 pandemic. As shown by the Tableau dashboard, the United States has the highest number of COVID-19 related deaths globally. In addition, the infant mortality rate is also decreasing, but it remains an issue that warrants attention.\n\n\n3. Healthcare Access and Utilization\n\nHealth Insurance CoverageEmergency Room Visits\n\n\n\n\nCode\n# Importing the dataset\nhealth_insurance &lt;- read_xlsx(\"data/share_of_people_us_without_health_insurance_age_1997_2022.xlsx\")\n\n# Change Date format\nhealth_insurance$Year &lt;- as.Date(paste(health_insurance$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Change it to long format\nhealth_insurance &lt;- health_insurance %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Percentage\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\nhealth_insurance_plot &lt;- ggplot(health_insurance, \n                                aes(x = factor(Year), y = Percentage, fill = Age_Group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Health Insurance Uncoverage in the US\",\n       x = \"Year\",\n       y = \"Percentage of People without Health Insurance(%)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_health_insurance &lt;- ggplotly(health_insurance_plot)\nplotly_health_insurance\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nemergency_room &lt;- read_xlsx(\"data/us_emergency_room_visits_1997_2019_by_age.xlsx\")\n\n# Change it to long format\nemergency_room &lt;- emergency_room %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Visits_Percent\")\n\n# Change Date format\nemergency_room$Year &lt;- as.Date(paste(emergency_room$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\nemergency_room_plot &lt;- ggplot(emergency_room, \n                              aes(x = factor(Year), y = Visits_Percent, fill = Age_Group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Emergency Room Visits in the US\",\n       x = \"Year\",\n       y = \"Percentage of People Visited Emergency Room(%)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_emergency_room &lt;- ggplotly(emergency_room_plot)\nplotly_emergency_room\n\n\n\n\n\n\n\n\n\nThe rate of health insurance uncoverage in the United States has been on a decline since 2010, possibly due to the implementation of Barack Obama’s health care reforms. However, starting from 2017, the health insurance uncoverage rate began to rise slightly, which may be attributed to the policies of the Trump administration. It is noteworthy that the 18-64 age group has consistently had the highest rate of medical insurance non-coverage among all age groups. Based on the numbers of emergency room visits, those aged 65 and above have always been the group with the highest rate of emergency room visits, while the 45-64 age group has the lowest rate of emergency room visits.\n\n\n4. Healthcare Costs\n\nCosts of Prescription DrugsCosts of Drug DevelopmentGDP Contribution of Healthcare Industry\n\n\n\n\nCode\n# Importing the dataset\nprescription_drug &lt;- read_xlsx(\"data/prescription_drug_expenditure_us_1960_2022.xlsx\")\n\n# Change Date format\nprescription_drug$Year &lt;- as.Date(paste(prescription_drug$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use bar plot\nprescription_drug_plot &lt;- ggplot(prescription_drug, \n                              aes(x = factor(Year), y = Expenditure)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Prescription Drugs Expenditure in the US\",\n       x = \"Year\",\n       y = \"Expenditure(billion dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_prescription_drug&lt;- ggplotly(prescription_drug_plot)\nplotly_prescription_drug\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\ndrug_development &lt;- read_xlsx(\"data/pharmaceuticals_cost_of_drug_development_in_the_us_since_1975.xlsx\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\ndrug_development_plot &lt;- ggplot(drug_development, \n                              aes(x = factor(Year), y = Cost)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = 'blue') +\n  labs(title = \"Costs of Drug Development in the US\",\n       x = \"Year\",\n       y = \"Cost(million dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_drug_development &lt;- ggplotly(drug_development_plot)\nplotly_drug_development\n\n\n\n\n\n\n\n\nNote: You can click on the industry’s name to observe the specific line. Click again, the graph will be reset.\n\n\nCode\n# Importing the dataset\ngdp_industry &lt;- read_csv(\"data/GDP_by_Industry.csv\", skip = 4) %&gt;%\n  select(-Line)\n\ncolnames(gdp_industry) &lt;- c(\"Industry\", \"2018-Q1\", \"2018-Q2\", \"2018-Q3\", \"2018-Q4\", \"2019-Q1\", \"2019-Q2\", \"2019-Q3\", \"2019-Q4\", \"2020-Q1\", \"2020-Q2\", \"2020-Q3\", \"2020-Q4\", \"2021-Q1\", \"2021-Q2\", \"2021-Q3\", \"2021-Q4\", \"2022-Q1\", \"2022-Q2\", \"2022-Q3\", \"2022-Q4\", \"2023-Q1\", \"2023-Q2\", \"2023-Q3\")\n\ngdp_industry &lt;- gdp_industry %&gt;%\n  filter((Industry == \"Agriculture, forestry, fishing, and hunting\")|\n           (Industry == \"Mining\")|\n           (Industry == \"Utilities\")|\n           (Industry == \"Construction\")|\n           (Industry == \"Manufacturing\")|\n           (Industry == \"Wholesale trade\")|\n           (Industry == \"Retail trade\")|\n           (Industry == \"Transportation and warehousing\")|\n           (Industry == \"Information\")|\n           (Industry == \"Finance and insurance\")|\n           (Industry == \"Real estate and rental and leasing\")|\n           (Industry == \"Professional, scientific, and technical services\")|\n           (Industry == \"Management of companies and enterprises\")|\n           (Industry == \"Administrative and waste management services\")|\n           (Industry == \"Educational services\")|\n           (Industry == \"Health care and social assistance\")|\n           (Industry == \"Arts, entertainment, and recreation\")|\n           (Industry == \"Accommodation and food services\")|\n           (Industry == \"Other services, except government\")|\n           (Industry == \"Federal\")|\n           (Industry == \"State and local\")\n  )\n\n## Change it to long format\ngdp_industry &lt;- gdp_industry %&gt;%\n  pivot_longer(cols = -Industry, \n               names_to = \"Date\", \n               values_to = \"GDP\")\n\n# Change date to Date format\ngdp_industry$Date &lt;- as.yearqtr(gdp_industry$Date, format = \"%Y-Q%q\")\n\ngdp_industry$Date &lt;- as.Date(gdp_industry$Date)\ngdp_industry$GDP &lt;- as.double(gdp_industry$GDP)\n\ngdp_industry &lt;- gdp_industry %&gt;%\n  mutate(\n    year = year(Date)\n  )\n\n\n# Group by year and industry\n\ngdp_industry_group &lt;- gdp_industry %&gt;%\n  group_by(year, Industry) %&gt;%\n  dplyr::summarize(GDP = sum(GDP)) %&gt;%\n  ungroup()\n\n\n\n\nCode\n# Plotting the data\ngdp_plot &lt;- ggplot(gdp_industry, aes(x = Date, y = GDP, color = Industry)) +\n  geom_line() +\n  labs(title = \"GDP Contribution of Different Industries\",\n       x = \"Year\",\n       y = \"GDP (in billions of dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),\n    axis.title = element_text(size = 11, face='bold'),\n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\"),\n    legend.position = \"bottom\"\n  )\n\nplotly_gdp &lt;- ggplotly(gdp_plot)\nplotly_gdp %&gt;%\n  layout(\n  legend = list(\n    title = list(text = 'Industry'),\n    itemclick = \"toggleothers\",\n    itemdoubleclick = \"toggle\"\n  )\n)\n\n\n\n\n\n\n\n\n\nPrescription drug costs have been increasing over the years. We can also observe that after the COVID-19 pandemic, the increase in prescription drug costs has been greater than before. Meanwhile, the cost of drug development has also been increasing. In the decade from 2000 to 2010, the United States greatly increased its investment in drug development. The GDP contribution of the healthcare industry has been increasing as well, but there was a certain degree of decline in 2020 due to the pandemic. This reflects the economic impact of public health events on the entire healthcare industry. Despite a long-term trend of growth, there may be fluctuations and challenges in the short term.\nThis trend underlines the complex relationship between public health crises, pharmaceutical economics, and healthcare systems. The increase in prescription drug costs post-pandemic suggests that the healthcare sector is facing increasing pressures, possibly due to supply chain disruptions, increased demand for medical treatments, and the accelerated pace of pharmaceutical innovation in response to the crisis. While the infusion of investment into drug development signals a robust pipeline for new therapies and a strong response to health challenges, it also raises concerns about the sustainability of healthcare financing and the accessibility of new drugs to the general population. The slight dip in the healthcare industry’s GDP contribution in 2020 may indicate a temporary retraction or reallocation of resources in the face of the pandemic’s immediate impacts, yet the overarching trajectory remains one of growth, reflecting the sector’s resilience and critical importance.\n\n\n5. Health Behavior and Risk Factors\n\nNumber of Adult SmokersAlcohol Consumption\n\n\n\n\nCode\n# Importing the dataset\nadult_smokers &lt;- read_xlsx(\"data/number_of_adult_smokers_us_1965_2021.xlsx\")\n\n# Change Date format\nadult_smokers$Year &lt;- as.Date(paste(adult_smokers$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use a line plot\nadult_smokers_plot &lt;- ggplot(adult_smokers, \n                              aes(x = Year, y = Number)) +\n  geom_line(color = 'purple') +\n  labs(title = \"Number of Adult Smokers in the US\",\n       x = \"Year\",\n       y = \"Number of Smokers(million)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"1 years\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_adult_smokers &lt;- ggplotly(adult_smokers_plot)\nplotly_adult_smokers\n\n\n\n\n\n\n\n\n\n\nCode\n# Importing the dataset\nalcohol_consumption &lt;- read_xlsx(\"data/per_capita_alcohol_consumption_of_all_beverages_us_1850_2021.xlsx\")\n\n# Change Date format\nalcohol_consumption$Year &lt;- as.Date(paste(alcohol_consumption$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use bar plot\nalcohol_consumption_plot &lt;- ggplot(alcohol_consumption, \n                              aes(x = Year, y = Consumption)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = 'green') +\n  labs(title = \"Alcohol consumption per capita from all beverages in the US\",\n       x = \"Year\",\n       y = \"Consumption(gallons of ethanol)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_alcohol_consumption &lt;- ggplotly(alcohol_consumption_plot)\nplotly_alcohol_consumption\n\n\n\n\n\n\n\n\n\nBefore 2009, the number of smokers in the United States showed a slight declining trend with some fluctuations. Starting from 2010, there was a sharp decline in the number of smokers. This could be attributed to the passage of The Family Smoking Prevention and Tobacco Control Act by the U.S. government in 2010. The act implemented regulatory measures on tobacco products, including prohibiting advertising and promotional activities for tobacco products, and requiring health warnings on tobacco product packaging. Additionally, the U.S. government imposed high taxes on tobacco products. These measures might have contributed to the decline in the number of smokers.\nIt can also be observed that alcohol consumption in the United States was on the rise from 1935 to 1980, peaking in 1980. During the period from 1920 to 1933, the United States implemented Alcohol Prohibition, we could speculate a sharp decrease in alcohol consumption. The enactment of Prohibition might have been due to societal concerns over alcohol consumption and moral judgments about alcohol use. After the repeal of Alcohol Prohibition, alcohol consumption showed an upward trend. It is also noted that after reaching a low point in alcohol consumption in 1998, the trend has been increasing in recent years."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The health of the United States is a multifaceted and dynamic topic that encompasses the well-being of its citizens, the quality and accessibility of healthcare services, and the policies that shape the nation’s health systems. It’s an intricate topic, including public health initiatives, individual lifestyle choices, healthcare provision, and socioeconomic. At its core, the health of the US is reflected in the life expectancy of its people, the prevalence of diseases and conditions, and the measures taken to prevent and treat the diseases. This spans from the individual level, such as personal preventive health behaviors, to the systemic, including the structure and efficiency of healthcare delivery.\nThe US healthcare system is characterized by a blend of private and public funding, with medical services provided by a combination of private businesses and government programs. Accessibility and quality of care are critical issues, influenced by insurance coverage, healthcare costs, and the distribution of health resources.\nPublic health in the US is a collective effort to safeguard and improve community health through preventive medicine, health education, control of communicable diseases, application of sanitary measures, and monitoring of environmental hazards. These efforts are crucial in addressing chronic diseases, which are the leading causes of death and disability in the country, affecting the quality of life of millions of Americans.\nThe health status of the US can also be gauged through various health indicators such as infant mortality rates, obesity prevalence, and vaccination coverage, which provide insights into the nation’s health challenges and successes. Understanding the health of the US requires a consideration of the diverse factors that contribute to health outcomes, including genetic predispositions, lifestyle choices, environmental exposures, and social determinants such as income, education, and employment.\n\n\n\n\nAs show by the Big Picture, this project will focus on the following aspects(angles) of health in the United States:\nChanges in Public Health Outcomes: This aspect looks at the overall well-being of the population, measured by key indicators such as average life expectancy, mortality rates of major diseases like heart disease and cancer, diabetes prevalence, infant mortality rates, and the impact of significant events like the COVID-19 pandemic. These outcomes provide a snapshot of the nation’s health and are influenced by various factors, including healthcare policies, societal behavior, and environmental conditions.\nChanges in Healthcare Access and Utilization: Health insurance coverage is a critical component, particularly when considering the before and after effects of the Affordable Care Act (ACA). The ACA aimed to increase insurance coverage and access to healthcare services. Utilization patterns, such as emergency room visits, also shed light on how healthcare services are used and the efficiency of healthcare delivery systems.\nChanges in Healthcare Costs: Healthcare spending is a significant part of the U.S. economy, accounting for a considerable percentage of the Gross Domestic Product (GDP). This includes the costs associated with prescription drugs, which have evolved over time, and the broader costs of drug development and healthcare services. Understanding these costs is essential for policy-making and for addressing the affordability of healthcare.\nChanges in Health Behaviors and Risk Factors: Lifestyle choices and behaviors, such as smoking and alcohol consumption, play a vital role in determining health outcomes. Over time, changes in these behaviors can lead to shifts in the overall health profile of the population, with direct implications for public health strategies and healthcare needs.\nChanges in Financial/Economy: The economic implications of health are seen in areas like pharmaceutical stock prices, including major companies such as Pfizer and Johnson & Johnson. The stock market can be a reflection of the health sector’s performance and is influenced by drug approvals, public health crises, and policy changes. This financial perspective is intertwined with the broader aspects of health in the country.\n\n\n\nThe study of the United States’ health through time series analysis offers a unique perspective on the nation’s well-being, encapsulating the changes and trends over time. Historically, Starfield et al. (2005) have utilized time series methods to track the performance of healthcare systems, revealing the impact of policy changes and healthcare reforms on patient outcomes. More recent literature has shifted focus toward the role of technology and data analytics in understanding health trends, emphasizing the use of big data to predict and manage health crises (Krumholz, 2014).\nA significant area of exploration is the correlation between economic conditions and health. Ruhm (2000) famously posited the counterintuitive relationship where recessions appear to improve overall health outcomes, a hypothesis that has been the subject of ongoing debate and refinement through subsequent time series examinations. This economic-health relationship remains a contentious topic, with researchers like Xu et al. (2016) using time series models to assess the long-term effects of economic cycles on health behaviors and outcomes.\nTime series analyses have also been pivotal in epidemiology, especially for communicable diseases. Studies such as Viboud et al. (2006) have dissected the seasonality and temporal patterns of influenza outbreaks, informing public health responses and vaccination strategies. During COVID-19, such analyses have gained heightened importance, underscoring the need for robust forecasting models in pandemic preparedness and response (Covid-19 Forecast Hub, 2020).\nChronic diseases have not escaped the scrutiny of time series analysis either. The rise of conditions like obesity and diabetes has been meticulously charted over time, with studies by Gregg et al. (2014) highlighting the surge in prevalence and the subsequent stabilization trends, prompting discussions on the efficacy of public health interventions.\nWhat’s more, debates in the literature often revolve around the methodological rigor and the potential for confounding variables in time series analysis. The field continuously evolves with advancements in statistical methods, such as the application of autoregressive integrated moving average (ARIMA) models, which refine the precision of health trend analysis (Box et al., 2015).\n\n\n\n\n\n1. How has the average life expectancy in the US changed?\n2. What are the trends in the mortality rates of major diseases (e.g., heart disease, cancer, diabetes) over the last few decades?\n3. How have infant mortality rates varied over time?\n4. How did the Covid-19 influence the people’s health in the US? And how did the pandemic influence the GDP contribution of Health Industry?\n5. Are there seasonal patterns or cyclical fluctuations in the health industry’s GDP contribution, and do they correspond to specific events or seasons?\n6. How has the rate of health insurance coverage changed before and after the implementation of the Affordable Care Act (ACA)?\n7. What are the patterns in emergency room visits over the years, and how do they relate to health outcomes?\n8. How have the costs of prescription drugs evolved over the past 20 years? And How have the costs of drug development changed over time? Is there a relationship between the costs of prescription drugs and the average life expectancy in the US?\n9. How have smoking and alcohol consumption rates changed over time, and what impact have they had on public health?\n10. How to use time series analysis and forecasting models to predict the future contribution of the U.S. Health Industry to GDP based on historical data and key economic indicators?"
  },
  {
    "objectID": "intro.html#topic-explanation",
    "href": "intro.html#topic-explanation",
    "title": "Introduction",
    "section": "",
    "text": "The health of the United States is a multifaceted and dynamic topic that encompasses the well-being of its citizens, the quality and accessibility of healthcare services, and the policies that shape the nation’s health systems. It’s an intricate topic, including public health initiatives, individual lifestyle choices, healthcare provision, and socioeconomic. At its core, the health of the US is reflected in the life expectancy of its people, the prevalence of diseases and conditions, and the measures taken to prevent and treat the diseases. This spans from the individual level, such as personal preventive health behaviors, to the systemic, including the structure and efficiency of healthcare delivery.\nThe US healthcare system is characterized by a blend of private and public funding, with medical services provided by a combination of private businesses and government programs. Accessibility and quality of care are critical issues, influenced by insurance coverage, healthcare costs, and the distribution of health resources.\nPublic health in the US is a collective effort to safeguard and improve community health through preventive medicine, health education, control of communicable diseases, application of sanitary measures, and monitoring of environmental hazards. These efforts are crucial in addressing chronic diseases, which are the leading causes of death and disability in the country, affecting the quality of life of millions of Americans.\nThe health status of the US can also be gauged through various health indicators such as infant mortality rates, obesity prevalence, and vaccination coverage, which provide insights into the nation’s health challenges and successes. Understanding the health of the US requires a consideration of the diverse factors that contribute to health outcomes, including genetic predispositions, lifestyle choices, environmental exposures, and social determinants such as income, education, and employment."
  },
  {
    "objectID": "intro.html#big-picture",
    "href": "intro.html#big-picture",
    "title": "Introduction",
    "section": "",
    "text": "As show by the Big Picture, this project will focus on the following aspects(angles) of health in the United States:\nChanges in Public Health Outcomes: This aspect looks at the overall well-being of the population, measured by key indicators such as average life expectancy, mortality rates of major diseases like heart disease and cancer, diabetes prevalence, infant mortality rates, and the impact of significant events like the COVID-19 pandemic. These outcomes provide a snapshot of the nation’s health and are influenced by various factors, including healthcare policies, societal behavior, and environmental conditions.\nChanges in Healthcare Access and Utilization: Health insurance coverage is a critical component, particularly when considering the before and after effects of the Affordable Care Act (ACA). The ACA aimed to increase insurance coverage and access to healthcare services. Utilization patterns, such as emergency room visits, also shed light on how healthcare services are used and the efficiency of healthcare delivery systems.\nChanges in Healthcare Costs: Healthcare spending is a significant part of the U.S. economy, accounting for a considerable percentage of the Gross Domestic Product (GDP). This includes the costs associated with prescription drugs, which have evolved over time, and the broader costs of drug development and healthcare services. Understanding these costs is essential for policy-making and for addressing the affordability of healthcare.\nChanges in Health Behaviors and Risk Factors: Lifestyle choices and behaviors, such as smoking and alcohol consumption, play a vital role in determining health outcomes. Over time, changes in these behaviors can lead to shifts in the overall health profile of the population, with direct implications for public health strategies and healthcare needs.\nChanges in Financial/Economy: The economic implications of health are seen in areas like pharmaceutical stock prices, including major companies such as Pfizer and Johnson & Johnson. The stock market can be a reflection of the health sector’s performance and is influenced by drug approvals, public health crises, and policy changes. This financial perspective is intertwined with the broader aspects of health in the country."
  },
  {
    "objectID": "intro.html#literature-review",
    "href": "intro.html#literature-review",
    "title": "Introduction",
    "section": "",
    "text": "The study of the United States’ health through time series analysis offers a unique perspective on the nation’s well-being, encapsulating the changes and trends over time. Historically, Starfield et al. (2005) have utilized time series methods to track the performance of healthcare systems, revealing the impact of policy changes and healthcare reforms on patient outcomes. More recent literature has shifted focus toward the role of technology and data analytics in understanding health trends, emphasizing the use of big data to predict and manage health crises (Krumholz, 2014).\nA significant area of exploration is the correlation between economic conditions and health. Ruhm (2000) famously posited the counterintuitive relationship where recessions appear to improve overall health outcomes, a hypothesis that has been the subject of ongoing debate and refinement through subsequent time series examinations. This economic-health relationship remains a contentious topic, with researchers like Xu et al. (2016) using time series models to assess the long-term effects of economic cycles on health behaviors and outcomes.\nTime series analyses have also been pivotal in epidemiology, especially for communicable diseases. Studies such as Viboud et al. (2006) have dissected the seasonality and temporal patterns of influenza outbreaks, informing public health responses and vaccination strategies. During COVID-19, such analyses have gained heightened importance, underscoring the need for robust forecasting models in pandemic preparedness and response (Covid-19 Forecast Hub, 2020).\nChronic diseases have not escaped the scrutiny of time series analysis either. The rise of conditions like obesity and diabetes has been meticulously charted over time, with studies by Gregg et al. (2014) highlighting the surge in prevalence and the subsequent stabilization trends, prompting discussions on the efficacy of public health interventions.\nWhat’s more, debates in the literature often revolve around the methodological rigor and the potential for confounding variables in time series analysis. The field continuously evolves with advancements in statistical methods, such as the application of autoregressive integrated moving average (ARIMA) models, which refine the precision of health trend analysis (Box et al., 2015)."
  },
  {
    "objectID": "intro.html#guiding-question",
    "href": "intro.html#guiding-question",
    "title": "Introduction",
    "section": "",
    "text": "1. How has the average life expectancy in the US changed?\n2. What are the trends in the mortality rates of major diseases (e.g., heart disease, cancer, diabetes) over the last few decades?\n3. How have infant mortality rates varied over time?\n4. How did the Covid-19 influence the people’s health in the US? And how did the pandemic influence the GDP contribution of Health Industry?\n5. Are there seasonal patterns or cyclical fluctuations in the health industry’s GDP contribution, and do they correspond to specific events or seasons?\n6. How has the rate of health insurance coverage changed before and after the implementation of the Affordable Care Act (ACA)?\n7. What are the patterns in emergency room visits over the years, and how do they relate to health outcomes?\n8. How have the costs of prescription drugs evolved over the past 20 years? And How have the costs of drug development changed over time? Is there a relationship between the costs of prescription drugs and the average life expectancy in the US?\n9. How have smoking and alcohol consumption rates changed over time, and what impact have they had on public health?\n10. How to use time series analysis and forecasting models to predict the future contribution of the U.S. Health Industry to GDP based on historical data and key economic indicators?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular timestamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes."
  },
  {
    "objectID": "ds.html",
    "href": "ds.html",
    "title": "Data Sources",
    "section": "",
    "text": "Statista is an online platform specializing in data collection and visualization, offering a wide range of statistics, reports, market insights, consumer and company insights. The platform hosts an extensive database of over 1,000,000 statistics on more than 80,000 topics, sourced from over 22,500 providers across 150 countries. Statista’s offerings include statistics and survey results presented in charts, infographics, and tables, making it a valuable resource for business customers, educators, and researchers. The data spans various domains, however, we are going to focus solely on Health and Healthcare.\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Life Expectancy.\n\n\nCode\n# Importing the dataset\naverage_life_expectancy &lt;- read_xlsx(\"data/life_expectancy_7countries_2000_2022.xlsx\")\n\n# Change it to long format\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Country\", \n               values_to = \"Life_Expectancy\")\n\naverage_life_expectancy$Year &lt;- as.Date(paste(average_life_expectancy$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\naverage_life_expectancy$Life_Expectancy &lt;- as.double(average_life_expectancy$Life_Expectancy)\n\n# Filter to only include the US\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  filter(Country == \"United States\")\n\nhead(average_life_expectancy)\n\n\n# A tibble: 6 × 3\n  Year       Country       Life_Expectancy\n  &lt;date&gt;     &lt;chr&gt;                   &lt;dbl&gt;\n1 2000-01-01 United States            76.8\n2 2001-01-01 United States            76.9\n3 2002-01-01 United States            77  \n4 2003-01-01 United States            77.2\n5 2004-01-01 United States            77.6\n6 2005-01-01 United States            77.6\n\n\n\n\nCode\n# Plotting the data\naverage_life_expectancy_plot &lt;- ggplot(average_life_expectancy, \n                                       aes(x = Year, y = Life_Expectancy)) +\n  geom_line(color = 'red') +\n  geom_point(color = 'red') +\n  labs(title = \"Average Life Expectancy in the US\",\n       x = \"Year\",\n       y = \"Life Expectancy(years)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_life &lt;- ggplotly(average_life_expectancy_plot)\nplotly_life\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Cancer Deaths. Statista-Diabetes Deaths. Statista-Heart Disease Deaths.\n\n\nCode\n# Importing the dataset\ndeath_cancer &lt;- read_xlsx(\"data/deaths_cancer_us_1950_2019.xlsx\")\ndeath_heart &lt;- read_xlsx(\"data/deaths_heart_diseases_us_1950_2019.xlsx\")\ndeath_diabetes &lt;- read_xlsx(\"data/deaths_diabetes_us_1950_2019.xlsx\")\n\n# Merge these three datasets\ndeath_major_diseases &lt;- merge(death_cancer, death_heart, by = \"Year\")\ndeath_major_diseases &lt;- merge(death_major_diseases, death_diabetes, by = \"Year\")\n\n# Change Date format\ndeath_major_diseases$Year &lt;- as.Date(paste(death_major_diseases$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ndeath_major_diseases$Death_Cancer &lt;- as.double(death_major_diseases$Death_Cancer)\ndeath_major_diseases$Death_Heart_Disease &lt;- as.double(death_major_diseases$Death_Heart_Disease)\ndeath_major_diseases$Death_Diabetes &lt;- as.double(death_major_diseases$Death_Diabetes)\n\n# Change it to long format\n\ndeath_major_diseases &lt;- death_major_diseases %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Disease\", \n               values_to = \"Deaths\")\n\n\n\n\nCode\n# Plotting the data\ndeath_major_diseases_plot &lt;- ggplot(death_major_diseases, \n                                       aes(x = Year, y = Deaths, color = Disease)) +\n  geom_line() +\n  geom_point(color = 'purple') +\n  labs(title = \"Deaths of Major Diseases in the US\",\n       x = \"Year\",\n       y = \"Number of Deaths(per 100,000)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_disease &lt;- ggplotly(death_major_diseases_plot)\nplotly_disease\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Infant Mortality Rate.\n\n\nCode\n# Importing the dataset\ninfant_mortality &lt;- read_xlsx(\"data/infant_mortality_rate_us_1990_2021.xlsx\")\n\n# Change Date format\ninfant_mortality$Year &lt;- as.Date(paste(infant_mortality$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ninfant_mortality$Infant_Mortality_Rates &lt;- as.double(infant_mortality$Infant_Mortality_Rates)\n\n\n\n\nCode\n# Plotting the data, area plot\ninfant_mortality_plot &lt;- ggplot(infant_mortality, \n                                aes(x = Year, y = Infant_Mortality_Rates)) +\n  geom_area(fill = \"lightblue\", alpha = 0.6) +\n  labs(title = \"Infant Mortality Rates in the US\",\n       x = \"Year\",\n       y = \"Infant Deaths Number(per 1,000 live births)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_infant_mortality &lt;- ggplotly(infant_mortality_plot)\nplotly_infant_mortality\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Health Insurance Coverage.\n\n\nCode\n# Importing the dataset\nhealth_insurance &lt;- read_xlsx(\"data/share_of_people_us_without_health_insurance_age_1997_2022.xlsx\")\n\n# Change Date format\nhealth_insurance$Year &lt;- as.Date(paste(health_insurance$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Change it to long format\nhealth_insurance &lt;- health_insurance %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Percentage\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\nhealth_insurance_plot &lt;- ggplot(health_insurance, \n                                aes(x = factor(Year), y = Percentage, fill = Age_Group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Health Insurance Uncoverage in the US\",\n       x = \"Year\",\n       y = \"Percentage of People without Health Insurance(%)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_health_insurance &lt;- ggplotly(health_insurance_plot)\nplotly_health_insurance\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Emergency Room Visits.\n\n\nCode\n# Importing the dataset\nemergency_room &lt;- read_xlsx(\"data/us_emergency_room_visits_1997_2019_by_age.xlsx\")\n\n# Change it to long format\nemergency_room &lt;- emergency_room %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Visits_Percent\")\n\n# Change Date format\nemergency_room$Year &lt;- as.Date(paste(emergency_room$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\nemergency_room_plot &lt;- ggplot(emergency_room, \n                              aes(x = factor(Year), y = Visits_Percent, fill = Age_Group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Emergency Room Visits in the US\",\n       x = \"Year\",\n       y = \"Percentage of People Visited Emergency Room(%)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_emergency_room &lt;- ggplotly(emergency_room_plot)\nplotly_emergency_room\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Prescription drug expenditure.\n\n\nCode\n# Importing the dataset\nprescription_drug &lt;- read_xlsx(\"data/prescription_drug_expenditure_us_1960_2022.xlsx\")\n\n# Change Date format\nprescription_drug$Year &lt;- as.Date(paste(prescription_drug$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use bar plot\nprescription_drug_plot &lt;- ggplot(prescription_drug, \n                              aes(x = factor(Year), y = Expenditure)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Prescription Drugs Expenditure in the US\",\n       x = \"Year\",\n       y = \"Expenditure(billion dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_prescription_drug&lt;- ggplotly(prescription_drug_plot)\nplotly_prescription_drug\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Drug_development.\n\n\nCode\n# Importing the dataset\ndrug_development &lt;- read_xlsx(\"data/pharmaceuticals_cost_of_drug_development_in_the_us_since_1975.xlsx\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\ndrug_development_plot &lt;- ggplot(drug_development, \n                              aes(x = factor(Year), y = Cost)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = 'blue') +\n  labs(title = \"Costs of Drug Development in the US\",\n       x = \"Year\",\n       y = \"Cost(million dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_drug_development &lt;- ggplotly(drug_development_plot)\nplotly_drug_development\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Adult Smokers.\n\n\nCode\n# Importing the dataset\nadult_smokers &lt;- read_xlsx(\"data/number_of_adult_smokers_us_1965_2021.xlsx\")\n\n# Change Date format\nadult_smokers$Year &lt;- as.Date(paste(adult_smokers$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use a line plot\nadult_smokers_plot &lt;- ggplot(adult_smokers, \n                              aes(x = Year, y = Number)) +\n  geom_line(color = 'purple') +\n  labs(title = \"Number of Adult Smokers in the US\",\n       x = \"Year\",\n       y = \"Number of Smokers(million)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"1 years\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_adult_smokers &lt;- ggplotly(adult_smokers_plot)\nplotly_adult_smokers\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista. Alcohol consumption per capita from all beverages in the U.S. from 1850 to 2021 (in gallons of ethanol) Statista-Alcohol Consumption.\n\n\nCode\n# Importing the dataset\nalcohol_consumption &lt;- read_xlsx(\"data/per_capita_alcohol_consumption_of_all_beverages_us_1850_2021.xlsx\")\n\n# Change Date format\nalcohol_consumption$Year &lt;- as.Date(paste(alcohol_consumption$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use bar plot\nalcohol_consumption_plot &lt;- ggplot(alcohol_consumption, \n                              aes(x = Year, y = Consumption)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = 'green') +\n  labs(title = \"Alcohol consumption per capita from all beverages in the US\",\n       x = \"Year\",\n       y = \"Consumption(gallons of ethanol)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_alcohol_consumption &lt;- ggplotly(alcohol_consumption_plot)\nplotly_alcohol_consumption"
  },
  {
    "objectID": "ds.html#statista",
    "href": "ds.html#statista",
    "title": "Data Sources",
    "section": "",
    "text": "Statista is an online platform specializing in data collection and visualization, offering a wide range of statistics, reports, market insights, consumer and company insights. The platform hosts an extensive database of over 1,000,000 statistics on more than 80,000 topics, sourced from over 22,500 providers across 150 countries. Statista’s offerings include statistics and survey results presented in charts, infographics, and tables, making it a valuable resource for business customers, educators, and researchers. The data spans various domains, however, we are going to focus solely on Health and Healthcare.\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Life Expectancy.\n\n\nCode\n# Importing the dataset\naverage_life_expectancy &lt;- read_xlsx(\"data/life_expectancy_7countries_2000_2022.xlsx\")\n\n# Change it to long format\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Country\", \n               values_to = \"Life_Expectancy\")\n\naverage_life_expectancy$Year &lt;- as.Date(paste(average_life_expectancy$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\naverage_life_expectancy$Life_Expectancy &lt;- as.double(average_life_expectancy$Life_Expectancy)\n\n# Filter to only include the US\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  filter(Country == \"United States\")\n\nhead(average_life_expectancy)\n\n\n# A tibble: 6 × 3\n  Year       Country       Life_Expectancy\n  &lt;date&gt;     &lt;chr&gt;                   &lt;dbl&gt;\n1 2000-01-01 United States            76.8\n2 2001-01-01 United States            76.9\n3 2002-01-01 United States            77  \n4 2003-01-01 United States            77.2\n5 2004-01-01 United States            77.6\n6 2005-01-01 United States            77.6\n\n\n\n\nCode\n# Plotting the data\naverage_life_expectancy_plot &lt;- ggplot(average_life_expectancy, \n                                       aes(x = Year, y = Life_Expectancy)) +\n  geom_line(color = 'red') +\n  geom_point(color = 'red') +\n  labs(title = \"Average Life Expectancy in the US\",\n       x = \"Year\",\n       y = \"Life Expectancy(years)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_life &lt;- ggplotly(average_life_expectancy_plot)\nplotly_life\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Cancer Deaths. Statista-Diabetes Deaths. Statista-Heart Disease Deaths.\n\n\nCode\n# Importing the dataset\ndeath_cancer &lt;- read_xlsx(\"data/deaths_cancer_us_1950_2019.xlsx\")\ndeath_heart &lt;- read_xlsx(\"data/deaths_heart_diseases_us_1950_2019.xlsx\")\ndeath_diabetes &lt;- read_xlsx(\"data/deaths_diabetes_us_1950_2019.xlsx\")\n\n# Merge these three datasets\ndeath_major_diseases &lt;- merge(death_cancer, death_heart, by = \"Year\")\ndeath_major_diseases &lt;- merge(death_major_diseases, death_diabetes, by = \"Year\")\n\n# Change Date format\ndeath_major_diseases$Year &lt;- as.Date(paste(death_major_diseases$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ndeath_major_diseases$Death_Cancer &lt;- as.double(death_major_diseases$Death_Cancer)\ndeath_major_diseases$Death_Heart_Disease &lt;- as.double(death_major_diseases$Death_Heart_Disease)\ndeath_major_diseases$Death_Diabetes &lt;- as.double(death_major_diseases$Death_Diabetes)\n\n# Change it to long format\n\ndeath_major_diseases &lt;- death_major_diseases %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Disease\", \n               values_to = \"Deaths\")\n\n\n\n\nCode\n# Plotting the data\ndeath_major_diseases_plot &lt;- ggplot(death_major_diseases, \n                                       aes(x = Year, y = Deaths, color = Disease)) +\n  geom_line() +\n  geom_point(color = 'purple') +\n  labs(title = \"Deaths of Major Diseases in the US\",\n       x = \"Year\",\n       y = \"Number of Deaths(per 100,000)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_disease &lt;- ggplotly(death_major_diseases_plot)\nplotly_disease\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Infant Mortality Rate.\n\n\nCode\n# Importing the dataset\ninfant_mortality &lt;- read_xlsx(\"data/infant_mortality_rate_us_1990_2021.xlsx\")\n\n# Change Date format\ninfant_mortality$Year &lt;- as.Date(paste(infant_mortality$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ninfant_mortality$Infant_Mortality_Rates &lt;- as.double(infant_mortality$Infant_Mortality_Rates)\n\n\n\n\nCode\n# Plotting the data, area plot\ninfant_mortality_plot &lt;- ggplot(infant_mortality, \n                                aes(x = Year, y = Infant_Mortality_Rates)) +\n  geom_area(fill = \"lightblue\", alpha = 0.6) +\n  labs(title = \"Infant Mortality Rates in the US\",\n       x = \"Year\",\n       y = \"Infant Deaths Number(per 1,000 live births)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )\n\nplotly_infant_mortality &lt;- ggplotly(infant_mortality_plot)\nplotly_infant_mortality\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Health Insurance Coverage.\n\n\nCode\n# Importing the dataset\nhealth_insurance &lt;- read_xlsx(\"data/share_of_people_us_without_health_insurance_age_1997_2022.xlsx\")\n\n# Change Date format\nhealth_insurance$Year &lt;- as.Date(paste(health_insurance$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Change it to long format\nhealth_insurance &lt;- health_insurance %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Percentage\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\nhealth_insurance_plot &lt;- ggplot(health_insurance, \n                                aes(x = factor(Year), y = Percentage, fill = Age_Group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Health Insurance Uncoverage in the US\",\n       x = \"Year\",\n       y = \"Percentage of People without Health Insurance(%)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  )+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_health_insurance &lt;- ggplotly(health_insurance_plot)\nplotly_health_insurance\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Emergency Room Visits.\n\n\nCode\n# Importing the dataset\nemergency_room &lt;- read_xlsx(\"data/us_emergency_room_visits_1997_2019_by_age.xlsx\")\n\n# Change it to long format\nemergency_room &lt;- emergency_room %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Visits_Percent\")\n\n# Change Date format\nemergency_room$Year &lt;- as.Date(paste(emergency_room$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\nemergency_room_plot &lt;- ggplot(emergency_room, \n                              aes(x = factor(Year), y = Visits_Percent, fill = Age_Group)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Emergency Room Visits in the US\",\n       x = \"Year\",\n       y = \"Percentage of People Visited Emergency Room(%)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_emergency_room &lt;- ggplotly(emergency_room_plot)\nplotly_emergency_room\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Prescription drug expenditure.\n\n\nCode\n# Importing the dataset\nprescription_drug &lt;- read_xlsx(\"data/prescription_drug_expenditure_us_1960_2022.xlsx\")\n\n# Change Date format\nprescription_drug$Year &lt;- as.Date(paste(prescription_drug$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use bar plot\nprescription_drug_plot &lt;- ggplot(prescription_drug, \n                              aes(x = factor(Year), y = Expenditure)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Prescription Drugs Expenditure in the US\",\n       x = \"Year\",\n       y = \"Expenditure(billion dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_prescription_drug&lt;- ggplotly(prescription_drug_plot)\nplotly_prescription_drug\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Drug_development.\n\n\nCode\n# Importing the dataset\ndrug_development &lt;- read_xlsx(\"data/pharmaceuticals_cost_of_drug_development_in_the_us_since_1975.xlsx\")\n\n\n\n\nCode\n# Plotting the data， usea bar plot\ndrug_development_plot &lt;- ggplot(drug_development, \n                              aes(x = factor(Year), y = Cost)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = 'blue') +\n  labs(title = \"Costs of Drug Development in the US\",\n       x = \"Year\",\n       y = \"Cost(million dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_drug_development &lt;- ggplotly(drug_development_plot)\nplotly_drug_development\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista.\nStatista-Adult Smokers.\n\n\nCode\n# Importing the dataset\nadult_smokers &lt;- read_xlsx(\"data/number_of_adult_smokers_us_1965_2021.xlsx\")\n\n# Change Date format\nadult_smokers$Year &lt;- as.Date(paste(adult_smokers$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use a line plot\nadult_smokers_plot &lt;- ggplot(adult_smokers, \n                              aes(x = Year, y = Number)) +\n  geom_line(color = 'purple') +\n  labs(title = \"Number of Adult Smokers in the US\",\n       x = \"Year\",\n       y = \"Number of Smokers(million)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"1 years\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_adult_smokers &lt;- ggplotly(adult_smokers_plot)\nplotly_adult_smokers\n\n\n\n\n\n\n\n\n\nThe data used for the creation of the graph below were obtained from the website Statista. Alcohol consumption per capita from all beverages in the U.S. from 1850 to 2021 (in gallons of ethanol) Statista-Alcohol Consumption.\n\n\nCode\n# Importing the dataset\nalcohol_consumption &lt;- read_xlsx(\"data/per_capita_alcohol_consumption_of_all_beverages_us_1850_2021.xlsx\")\n\n# Change Date format\nalcohol_consumption$Year &lt;- as.Date(paste(alcohol_consumption$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n\n\n\nCode\n# Plotting the data， use bar plot\nalcohol_consumption_plot &lt;- ggplot(alcohol_consumption, \n                              aes(x = Year, y = Consumption)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = 'green') +\n  labs(title = \"Alcohol consumption per capita from all beverages in the US\",\n       x = \"Year\",\n       y = \"Consumption(gallons of ethanol)\") +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\")\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\nplotly_alcohol_consumption &lt;- ggplotly(alcohol_consumption_plot)\nplotly_alcohol_consumption"
  },
  {
    "objectID": "ds.html#bureau-of-economic-analysis",
    "href": "ds.html#bureau-of-economic-analysis",
    "title": "Data Sources",
    "section": "2 Bureau of Economic Analysis",
    "text": "2 Bureau of Economic Analysis\n\nThe Bureau of Economic Analysis (BEA) is an agency of the United States Department of Commerce that provides important statistical information about the U.S. economy. The BEA’s mission is to promote a better understanding of the U.S. economy by providing the most timely, relevant, and accurate economic accounts data in an objective and cost-effective manner. The BEA is responsible for the production of some of the most closely watched economic statistics that influence decisions of government officials, business people, and individuals. These statistics include the gross domestic product (GDP), which is the primary indicator used to gauge the health of the country’s economy. Other critical data produced by the BEA include: Personal Income and Outlays, Balance of Payments, Industry Accounts, and Regional Accounts.\n\n2.1 GDP Contribution of Healthcare Industry\nThe data used for the creation of the graph below were obtained from BEA.\nBEA-GDP-Value Added by Industry.\n\n\nCode\n# Importing the dataset\ngdp_industry &lt;- read_csv(\"data/GDP_by_Industry.csv\", skip = 4) %&gt;%\n  select(-Line)\n\ncolnames(gdp_industry) &lt;- c(\"Industry\", \"2018-Q1\", \"2018-Q2\", \"2018-Q3\", \"2018-Q4\", \"2019-Q1\", \"2019-Q2\", \"2019-Q3\", \"2019-Q4\", \"2020-Q1\", \"2020-Q2\", \"2020-Q3\", \"2020-Q4\", \"2021-Q1\", \"2021-Q2\", \"2021-Q3\", \"2021-Q4\", \"2022-Q1\", \"2022-Q2\", \"2022-Q3\", \"2022-Q4\", \"2023-Q1\", \"2023-Q2\", \"2023-Q3\")\n\ngdp_industry &lt;- gdp_industry %&gt;%\n  filter((Industry == \"Agriculture, forestry, fishing, and hunting\")|\n           (Industry == \"Mining\")|\n           (Industry == \"Utilities\")|\n           (Industry == \"Construction\")|\n           (Industry == \"Manufacturing\")|\n           (Industry == \"Wholesale trade\")|\n           (Industry == \"Retail trade\")|\n           (Industry == \"Transportation and warehousing\")|\n           (Industry == \"Information\")|\n           (Industry == \"Finance and insurance\")|\n           (Industry == \"Real estate and rental and leasing\")|\n           (Industry == \"Professional, scientific, and technical services\")|\n           (Industry == \"Management of companies and enterprises\")|\n           (Industry == \"Administrative and waste management services\")|\n           (Industry == \"Educational services\")|\n           (Industry == \"Health care and social assistance\")|\n           (Industry == \"Arts, entertainment, and recreation\")|\n           (Industry == \"Accommodation and food services\")|\n           (Industry == \"Other services, except government\")|\n           (Industry == \"Federal\")|\n           (Industry == \"State and local\")\n  )\n\n## Change it to long format\ngdp_industry &lt;- gdp_industry %&gt;%\n  pivot_longer(cols = -Industry, \n               names_to = \"Date\", \n               values_to = \"GDP\")\n\n# Change date to Date format\ngdp_industry$Date &lt;- as.yearqtr(gdp_industry$Date, format = \"%Y-Q%q\")\n\ngdp_industry$Date &lt;- as.Date(gdp_industry$Date)\ngdp_industry$GDP &lt;- as.double(gdp_industry$GDP)\n\n\n\n\nCode\n# Plotting the data\ngdp_plot &lt;- ggplot(gdp_industry, aes(x = Date, y = GDP, color = Industry)) +\n  geom_line() +\n  labs(title = \"GDP Contribution of Different Industries\",\n       x = \"Year\",\n       y = \"GDP (in billions of dollars)\") +\n  theme(\n    axis.text = element_text(size = 11),  \n    axis.title = element_text(size = 11, face='bold'), \n    title = element_text(size = 12, hjust = 0.5, face='bold'),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.minor = element_line(color = \"gray\", linetype = \"dotted\"),\n    panel.grid.major = element_line(color = \"gray\", linetype = \"dotted\"),\n    legend.position = \"bottom\"\n  )\n\nplotly_gdp &lt;- ggplotly(gdp_plot)\nplotly_gdp"
  },
  {
    "objectID": "ds.html#yahoo-stocks-data",
    "href": "ds.html#yahoo-stocks-data",
    "title": "Data Sources",
    "section": "3 Yahoo Stocks Data",
    "text": "3 Yahoo Stocks Data\n\nYahoo! Finance is a Yahoo! network media property that delivers financial news, data, commentary, stock quotes, press releases, financial reports, and original content. It also provides online tools for personal finance management. Alongside showcasing paid partner content, it publishes original stories by its in-house journalists. It holds the 20th spot in SimilarWeb’s ranking of major news and media websites. In 2017, Yahoo! Finance introduced cryptocurrency news coverage, featuring information on over 9,000 unique coins, including Bitcoin and Ethereum.Furthermore, Yahoo Finance’s video content is accessible via connected TVs and devices like Apple TV, Samsung TV Plus, Roku, and YouTube.\nThe data used below were obtained from Yahoo Finance.\nYahoo Finance.\n\n3.1 Large-scale comprehensive pharmaceutical companies\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"PFE\", \"MRK\", \"JNJ\", \"ABT\", \"TMO\", \"LLY\", \"BMY\", \"GILD\", \"AMGN\", \"ABBV\", \"REGN\", \"BIIB\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2020-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(PFE$PFE.Adjusted,\n                    MRK$MRK.Adjusted,\n                    JNJ$JNJ.Adjusted,\n                    ABT$ABT.Adjusted,\n                    TMO$TMO.Adjusted,\n                    LLY$LLY.Adjusted,\n                    BMY$BMY.Adjusted,\n                    GILD$GILD.Adjusted,\n                    AMGN$AMGN.Adjusted,\n                    ABBV$ABBV.Adjusted,\n                    REGN$REGN.Adjusted,\n                    BIIB$BIIB.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\nhead(stock)\n\n\n                PFE      MRK      JNJ      ABT      TMO      LLY      BMY\n2020-01-02 31.33902 77.41459 130.0991 80.88947 323.3522 124.5060 55.52568\n2020-01-03 31.17089 76.75012 128.5928 79.90334 319.8350 124.0916 55.03478\n2020-01-06 31.13085 77.07813 128.4324 80.32200 322.1336 124.5531 55.21010\n2020-01-07 31.02676 75.02586 129.2167 79.87547 323.9566 124.7885 56.04290\n2020-01-08 31.27497 74.52121 129.1990 80.20107 324.4718 125.9186 55.98153\n2020-01-09 31.13885 75.17727 129.5822 80.41503 326.5623 127.9998 57.37538\n               GILD     AMGN     ABBV   REGN   BIIB      Dates       date\n2020-01-02 55.35456 210.6853 74.19762 373.35 294.24 2020-01-02 2020-01-02\n2020-01-03 55.21877 209.2550 73.49334 369.16 290.85 2020-01-03 2020-01-03\n2020-01-06 55.71097 210.8608 74.07335 371.74 290.82 2020-01-06 2020-01-06\n2020-01-07 55.26121 208.8777 73.65076 373.13 290.09 2020-01-07 2020-01-07\n2020-01-08 55.83827 209.0356 74.17278 382.42 292.66 2020-01-08 2020-01-08\n2020-01-09 55.87220 209.6586 74.74447 383.47 294.30 2020-01-09 2020-01-09\n\n\n\n\n3.2 Biotechnology companies\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"VRTX\", \"MRNA\", \"BNTX\", \"ILMN\", \"ALNY\", \"INCY\", \"NBIX\", \"NKTR\", \"IONS\")\n\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2020-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(VRTX$VRTX.Adjusted,\n                    MRNA$MRNA.Adjusted,\n                    BNTX$BNTX.Adjusted,\n                    ILMN$ILMN.Adjusted,\n                    ALNY$ALNY.Adjusted,\n                    INCY$INCY.Adjusted,\n                    NBIX$NBIX.Adjusted,\n                    NKTR$NKTR.Adjusted,\n                    IONS$IONS.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\nhead(stock)\n\n\n             VRTX  MRNA     BNTX   ILMN   ALNY  INCY   NBIX  NKTR  IONS\n2020-01-02 219.45 19.23 37.99673 327.00 115.62 85.97 108.30 20.96 60.89\n2020-01-03 217.98 18.89 39.52647 322.73 115.68 77.90 109.78 20.53 60.44\n2020-01-06 224.03 18.13 43.99725 325.53 115.97 77.34 110.77 21.00 60.51\n2020-01-07 223.79 17.78 42.77346 329.69 115.29 77.14 110.06 21.26 61.19\n2020-01-08 231.09 17.98 44.51045 332.16 115.47 76.53 111.29 21.76 62.44\n2020-01-09 230.26 18.40 42.70438 334.60 118.06 77.95 113.76 21.59 63.27\n                Dates       date\n2020-01-02 2020-01-02 2020-01-02\n2020-01-03 2020-01-03 2020-01-03\n2020-01-06 2020-01-06 2020-01-06\n2020-01-07 2020-01-07 2020-01-07\n2020-01-08 2020-01-08 2020-01-08\n2020-01-09 2020-01-09 2020-01-09\n\n\n\n\n3.3 Generic and specialty drug companies\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"TEVA\", \"PRGO\", \"CTLT\", \"BHC\", \"JAZZ\", \"VTRS\")\n\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2020-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(TEVA$TEVA.Adjusted,\n                    PRGO$PRGO.Adjusted,\n                    CTLT$CTLT.Adjusted,\n                    BHC$BHC.Adjusted,\n                    JAZZ$JAZZ.Adjusted,\n                    VTRS$VTRS.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\nhead(stock)\n\n\n           TEVA     PRGO  CTLT   BHC   JAZZ     VTRS      Dates       date\n2020-01-02 9.56 46.31985 56.60 29.91 148.43 18.46621 2020-01-02 2020-01-02\n2020-01-03 9.09 45.11873 55.98 29.42 146.09 18.06380 2020-01-03 2020-01-03\n2020-01-06 9.07 44.50463 55.99 28.86 145.84 18.66294 2020-01-06 2020-01-06\n2020-01-07 9.10 45.11873 56.29 28.57 144.28 18.93122 2020-01-07 2020-01-07\n2020-01-08 8.92 44.63106 56.68 28.50 144.12 18.97593 2020-01-08 2020-01-08\n2020-01-09 9.02 44.96521 56.97 28.01 145.59 18.71659 2020-01-09 2020-01-09\n\n\n\n\n3.4 Small and medium-sized biotech companies\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"SAGE\",\"EXEL\", \"UTHR\", \"ACAD\", \"LGND\", \"FOLD\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2020-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(\n                    SAGE$SAGE.Adjusted,\n                    EXEL$EXEL.Adjusted,\n                    UTHR$UTHR.Adjusted,\n                    ACAD$ACAD.Adjusted,\n                    LGND$LGND.Adjusted,\n                    FOLD$FOLD.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\nhead(stock)\n\n\n            SAGE  EXEL  UTHR  ACAD     LGND FOLD      Dates       date\n2020-01-02 73.16 17.66 87.66 41.11 65.33999 9.55 2020-01-02 2020-01-02\n2020-01-03 73.53 17.01 86.46 40.35 63.11291 9.18 2020-01-03 2020-01-03\n2020-01-06 75.40 17.56 86.57 42.81 63.21896 9.38 2020-01-06 2020-01-06\n2020-01-07 74.82 18.68 86.91 42.18 61.74672 9.37 2020-01-07 2020-01-07\n2020-01-08 75.64 18.95 87.16 44.41 60.77355 9.46 2020-01-08 2020-01-08\n2020-01-09 76.12 18.63 87.57 45.58 59.69432 9.67 2020-01-09 2020-01-09"
  },
  {
    "objectID": "ds.html#world-health-organization-coronavirus-covid-19",
    "href": "ds.html#world-health-organization-coronavirus-covid-19",
    "title": "Data Sources",
    "section": "4 World Health Organization: Coronavirus (COVID-19)",
    "text": "4 World Health Organization: Coronavirus (COVID-19)\n\n4.1 COVID-19 Cases and Deaths in the US\n\nThe WHO coronavirus (COVID-19) dashboard presents official daily counts of COVID-19 cases, deaths and vaccine utilisation reported by countries, territories and areas. Through the dashboard, they aim to provide a frequently updated data visualization, data dissemination and data exploration resource, while linking users to other useful and informative resources.\nThe data used for the creation of the map below were obtained from WHO.\nWHO-Covid-19."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About You",
    "section": "",
    "text": "About Me\nYetong Xu\nPhone: (703) 517-0606\nEmail: yx322@georgetown.edu\n\n\n\nAt the Conference(2024, May)\n\n\n\n\nEducation\nGEORGETOWN UNIVERSITY\nMaster of Science in Data Science for Public Policy\nSep 2022 - Jun 2024, Washington DC\n\nGPA: 3.963/4.00\nScholarship: Office of the Dean Scholarship($15,000), McCourt Fund for Experiential Learning($6,000)\nRelevant Courses: Accelerated Statistics for Public Policy, Geographic Data in Program R, Intermediate Microeconomics, Comparative Policy Process, Data Science: Advanced Modeling Techniques, Data Visualization, Time Series, Databases, Massive Data Fundamentals\n\n\n\nExperience\nAssess the impact of the International Rescue Committee’s Refugees in East Africa: Boosting Urban Innovations for Livelihoods Development (Re:Build) program\nResearch Assistant\nAdvisor: Andrew Zeitlin\nGU Initiative on Innovation, Development and Evaluation\nApr 2023 - Present, Washington DC\n\nCollaborated with psychologists from Princeton University and members of the International Rescue Committee during a tenure in Nairobi, Kenya from July to September - Design survey instruments to measure study outcomes and code them in SurveyCTO\nOversee field data collection, conducting ‘high frequency’ and other quality checks, and provided support in data cleaning and analysis.\nContribute to research reports, policy briefs, and blog entries for reporting and dissemination\nPartner with the Senior Data and Research Manager and Principal Investigators to enhance research designs, sampling methodologies, data management procedures, and overall research protocols\n\nThe importance of Continuity between Prenatal Care and Labor&Delivery in Serving a High- Risk Urban Population\nResearch Assistant\nAdvisor: Thomas Deleire\nHealth Care Finance Initiative, Georgetown University\nOct 2022 - Present, Washington DC\n\nAcquired comprehensive datasets from Medstar Medical Center, encompassing patient demographics, encounter records, cost metrics, and outcomes\nProcessed raw data to address missing and incorrect entries. Utilized Python to formulate structured datasets (including Person Level, Encounter Level, and Pregnancy Level) tailored for economic research\nConducted statistical analyses using STATA and developed a comprehensive codebook to streamline our group’s analytical processes\nShowcased our research poster in the MedStar Health – Georgetown University Research & Education Symposium and the 2023 Academy Health Conference\nConduct advanced cost-effectiveness analysis for the Safe Baby Safe Mom (SBSM) program in the DMV region\n\nMachine Learning of elderly people’s willingness to delay retirement–Based on Charls and HRS\nGeorgetown University\nFeb 2023 - May 2023, Washington DC\n\nEmployed Python and Stata to assess the delayed retirement intentions among elderly populations in China and the US, focusing on parameters like gender, age, education, workplace nature, pension, and COVID-19 related factors, using Charls 2018 and HRS 2020 datasets\nApplied advanced analytical techniques, leveraging three non-parametric models: Random Forest, Neural Networks, and XG-Boost, complemented by a parametric approach via Logistic Regression\nSynthesized findings to propose and implement a progressive and adaptable delayed retirement strategy\n\nThe world’s first database of development finance institutions\nResearch Assistant\nThe Institute of New Structural Economics at Peking University\nApr 2022 - Jul 2022, Beijing\n\nSpearheaded data collection efforts in the public policy realm, focusing on development finance institutions, and performed comprehensive analyses of their annual and financial reports\nPlayed a pivotal role in enhancing natural language processing methodologies for streamlined financial data extraction\nContributed significantly to the creation and launch of the world’s premier database on development finance institutions, showcased through a visual data platform: click here\n\nResearch on the rehabilitation of stroke patients in China\nResearch Assistant\nPKU Institute for Global Health and Development\nMar 2022 - Sep 2022, Beijing\n\nDelved deeply into the repercussions of rehabilitation healthcare policies, particularly analyzing their effects on service provider selection criteria and discerning key determinants influencing stroke patients’ decisions when choosing a rehabilitation service\nConducted a thorough analysis of the supply-demand dynamics of rehabilitation treatments, coupled with a health economics evaluation of rehabilitation service provisions for stroke patients\n\nData analysis of Tmall Taobao beauty care plate\nTeam Leader\nPeking University National School of Development\nMar 2021 - Jun 2021, Beijing\n\nLed an analysis on an expansive dataset of over 270,000 entries from the beauty care sector on Taobao and Tmall, focusing on lipstick economy trends during the epidemic\nEmployed a multitude of analytical techniques including word frequency statistics, word cloud visualizations, SPSS regression analysis, origin analysis, and keyword analytics\nDetermined that the majority of beauty care products in the world originate from China, predominantly within its eastern coastal regions. Synthesized these findings into a comprehensive report, complemented by effective data visualizations\n\nResearch on Resource Scheduling Strategy for Energy Consumption Optimization of Big Data\nUndergraduate Thesis\nDec 2020 - Jun 2021, Beijing\n\nSelected “Energy consumption optimization strategy of big data systems” as the focal research area, in line with the growing trends in big data and green computing\nUtilized the MapReduce parallel computing model for experimental validation, demonstrating that the ODTC enhances task execution efficiency and reduces energy consumption by approximately 15% when compared to FIFO and other prevalent algorithms - Actively participated in drafting and finalizing a manuscript prepared for submission\n\nChina Central Television(CCTV)Economic life survey\nResearch Assistant\nPeking University National School of Development\nJul 2019 - Apr 2020, Beijing\n\nPerformed statistical analysis on data collected from the 2019-2020 CCTV Economic Life survey\nOversaw the “real estate market” segment and authored specialized reports for CCTV\n\nResearch on Chinese Enterprise Innovation and Entrepreneurship\nResearch Assistant\nPeking University Enterprise Big Data Research Center\nJul 2019 - Apr 2020, Beijing\n\nConducted on-site visits to nearly 70 private enterprises in Beijing in 2019, interviewing business leaders to gain insights into the domestic entrepreneurial journey and current management practices\nFollowed up with the enterprises in early 2020 to gather information on their investment, financing, and operational statuses amid the pandemic\nContributed to research publications, offering solutions for medium and micro enterprises at risk of closure due to the COVID-19 pandemic\n\n2019 American College Student Mathematics Modeling Competition\nTeam Leader\nJan 2019 - May 2019, Beijing\n\nDeveloped a disaster relief response system leveraging rotor-wing drones in the aftermath of the devastating hurricane that impacted Puerto Rico, resulting in significant infrastructure damage and approximately 2,900 fatalities\nOptimized space utilization across five local hospitals, considering factors such as the type and volume of medical packages, drone capabilities, and hospital locations\nEmployed MATLAB and the Gravity Center Model (GCM) to pinpoint the optimal container placement locations\nWon Meritorious Award (top 5% among global competitors)\n\n\n\nProfessional Experience\nHaitong Securities\nAsset Management Department, Intern\nJul 2021 - Sep 2021, Shanghai\n\nDid Research on the weekly data in the articles “Crude oil Imports” and “US Crude oil Production” from the official website of EIA\nObtained the data on crude oil, fuel oil, power futures and spot prices, gasoline inventory, and commercial crude oil inventory from Wind\nUndertook extensive data collection, and executed both single-factor and multi-factor strategy analyses of crude oil price timing by combining methods in the research of “Whether short-term supply and demand Data can be used for crude oil timing” Ministry of Education of the People’s Republic of China\n\nNational Study Fund Management Committee, System Administrator\nJul 2019 - Sep 2019, Beijing\n\nManaged daily system updates and handled database queries for the CSC using SQL\nGuaranteed the seamless operation of China’s online scholarship application system for college students by identifying and resolving system bugs and user requirements\n\n\n\nExtracurricular Activities\nBeijing EV Youth Charity Organization\nMar 2018 - Sep 2018, Beijing\n\nEngaged in volunteer teaching in mountainous regions for six months, covering subjects such as moral education, sports, arts, handicrafts, and English\nSpearheaded group discussions and crafted bespoke courses and educational materials tailored for the children\n\n\n\nSkills & Languages\n\nSkills: Microsoft Office (Excel/PowerPoint/Words), Python, R, GIS, iMovie, Event Planning, Public Speaking, Wind Database, C, C ++, Java, C #, SQL Database, Adobe Series Software, HTML / CSS, Visio, Stata, SPSS, Axure, XMind, Eclipse, Eview\nLanguages: English(Fluent); Mandarin Chinese(Native)"
  },
  {
    "objectID": "uni_TS_model.html",
    "href": "uni_TS_model.html",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "",
    "text": "Code\n# Import Stock Price data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"PFE\", \"VRTX\", \"TEVA\", \"SAGE\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2019-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(PFE$PFE.Adjusted,\n                    VRTX$VRTX.Adjusted,\n                    TEVA$TEVA.Adjusted,\n                    SAGE$SAGE.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock &lt;- stock %&gt;%\n  rownames_to_column(var = \"date\")\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\npharma_ts &lt;- ts(stock$PFE, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 365)\n\nbiotech_ts &lt;- ts(stock$VRTX, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 365)\n\ndrug_ts &lt;- ts(stock$TEVA, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 365)\n\nsmall_biotech_ts &lt;- ts(stock$SAGE, start = c(year(min(stock$date)), month(min(stock$date))), end = c(year(max(stock$date)), month(max(stock$date))), frequency = 365)\n\n# Import data of Public Health Outcomes\n# Average Life Expectancy\naverage_life_expectancy &lt;- read_xlsx(\"data/life_expectancy_7countries_2000_2022.xlsx\")\n# Change it to long format\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Country\", \n               values_to = \"Life_Expectancy\")\n\naverage_life_expectancy$Year &lt;- as.Date(paste(average_life_expectancy$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\naverage_life_expectancy$Life_Expectancy &lt;- as.double(average_life_expectancy$Life_Expectancy)\n\n# Filter to only include the US\naverage_life_expectancy &lt;- average_life_expectancy %&gt;% \n  filter(Country == \"United States\")\n\nale_ts &lt;- ts(average_life_expectancy$Life_Expectancy, start = c(year(min(average_life_expectancy$Year)), month(min(average_life_expectancy$Year))), end = c(year(max(average_life_expectancy$Year)), month(max(average_life_expectancy$Year))), frequency = 1) # Yearly data\n\n# Importing the dataset of deaths due to major diseases\ndeath_cancer &lt;- read_xlsx(\"data/deaths_cancer_us_1950_2019.xlsx\")\ndeath_heart &lt;- read_xlsx(\"data/deaths_heart_diseases_us_1950_2019.xlsx\")\ndeath_diabetes &lt;- read_xlsx(\"data/deaths_diabetes_us_1950_2019.xlsx\")\n\n# Merge these three datasets\ndeath_major_diseases &lt;- merge(death_cancer, death_heart, by = \"Year\")\ndeath_major_diseases &lt;- merge(death_major_diseases, death_diabetes, by = \"Year\")\n\n# Create a new column for total deaths\ndeath_major_diseases$Total_Deaths &lt;- death_major_diseases$Death_Cancer + death_major_diseases$Death_Heart_Disease + death_major_diseases$Death_Diabetes\n\n# Change Date format\ndeath_major_diseases$Year &lt;- as.Date(paste(death_major_diseases$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ndeath_major_diseases$Death_Cancer &lt;- as.double(death_major_diseases$Death_Cancer)\ndeath_major_diseases$Death_Heart_Disease &lt;- as.double(death_major_diseases$Death_Heart_Disease)\ndeath_major_diseases$Death_Diabetes &lt;- as.double(death_major_diseases$Death_Diabetes)\n\n# Create time series\n# cancer_ts &lt;- ts(death_major_diseases$Death_Cancer, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 1)\n# heart_ts &lt;- ts(death_major_diseases$Death_Heart_Disease, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 1)\n# diabetes_ts &lt;- ts(death_major_diseases$Death_Diabetes, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 1) # Yearly data\n\nmajor_diseases_ts &lt;- ts(death_major_diseases$Total_Deaths, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 1) # Yearly data\n\n# Importing the dataset of infant mortality rates\ninfant_mortality &lt;- read_xlsx(\"data/infant_mortality_rate_us_1990_2021.xlsx\")\n\n# Change Date format\ninfant_mortality$Year &lt;- as.Date(paste(infant_mortality$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\ninfant_mortality$Infant_Mortality_Rates &lt;- as.double(infant_mortality$Infant_Mortality_Rates)\n\n# Create time series\ninfant_mortality_ts &lt;- ts(infant_mortality$Infant_Mortality_Rates, start = c(year(min(infant_mortality$Year)), month(min(infant_mortality$Year))), end = c(year(max(infant_mortality$Year)), month(max(infant_mortality$Year))), frequency = 1) # Yearly data\n\n# Importing the dataset of share of people without health insurance\nhealth_insurance &lt;- read_xlsx(\"data/share_of_people_us_without_health_insurance_age_1997_2022.xlsx\")\n\n# Change Date format\nhealth_insurance$Year &lt;- as.Date(paste(health_insurance$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Change it to long format\nhealth_insurance &lt;- health_insurance %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Percentage\")\n\n# Filter to all ages \nhealth_insurance &lt;- health_insurance %&gt;% \n  filter(Age_Group == \"All ages\")\n\n# Create time series\nhealth_insurance_ts &lt;- ts(health_insurance$Percentage, start = c(year(min(health_insurance$Year)), month(min(health_insurance$Year))), end = c(year(max(health_insurance$Year)), month(max(health_insurance$Year))), frequency = 1) # Yearly data\n\n# Importing the dataset of emergency room visits\nemergency_room &lt;- read_xlsx(\"data/us_emergency_room_visits_1997_2019_by_age.xlsx\")\n\n# Change it to long format\nemergency_room &lt;- emergency_room %&gt;% \n  pivot_longer(cols = -Year, \n               names_to = \"Age_Group\", \n               values_to = \"Visits_Percent\")\n\n# Change Date format\nemergency_room$Year &lt;- as.Date(paste(emergency_room$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Get the average visits percentage for all ages\nemergency_room &lt;- emergency_room %&gt;% \n  group_by(Year) %&gt;%\n  summarise(Average_Visits_Percent = mean(Visits_Percent))\n\n# Create time series\nemergency_room_ts &lt;- ts(emergency_room$Average_Visits_Percent, start = c(year(min(emergency_room$Year)), month(min(emergency_room$Year))), end = c(year(max(emergency_room$Year)), month(max(emergency_room$Year))), frequency = 1)\n\n# Importing the dataset of prescription drug expenditure\nprescription_drug &lt;- read_xlsx(\"data/prescription_drug_expenditure_us_1960_2022.xlsx\")\n\n# Change Date format\nprescription_drug$Year &lt;- as.Date(paste(prescription_drug$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nprescription_drug_ts &lt;- ts(prescription_drug$Expenditure, start = c(year(min(prescription_drug$Year)), month(min(prescription_drug$Year))), end = c(year(max(prescription_drug$Year)), month(max(prescription_drug$Year))), frequency = 1)\n\n# Importing the dataset of GDP by industry\ngdp_industry &lt;- read_csv(\"data/GDP_by_Industry.csv\", skip = 4) %&gt;%\n  select(-Line)\n\ncolnames(gdp_industry) &lt;- c(\"Industry\", \"2018-Q1\", \"2018-Q2\", \"2018-Q3\", \"2018-Q4\", \"2019-Q1\", \"2019-Q2\", \"2019-Q3\", \"2019-Q4\", \"2020-Q1\", \"2020-Q2\", \"2020-Q3\", \"2020-Q4\", \"2021-Q1\", \"2021-Q2\", \"2021-Q3\", \"2021-Q4\", \"2022-Q1\", \"2022-Q2\", \"2022-Q3\", \"2022-Q4\", \"2023-Q1\", \"2023-Q2\", \"2023-Q3\")\n\ngdp_healthcare &lt;- gdp_industry %&gt;%\n  filter(\n           (Industry == \"Health care and social assistance\")\n  )\n\n## Change it to long format\ngdp_healthcare &lt;- gdp_healthcare %&gt;%\n  pivot_longer(cols = -Industry, \n               names_to = \"Date\", \n               values_to = \"GDP\")\n\n# Change date to Date format\ngdp_healthcare$Date &lt;- as.yearqtr(gdp_healthcare$Date, format = \"%Y-Q%q\")\n\ngdp_healthcare$Date &lt;- as.Date(gdp_healthcare$Date)\ngdp_healthcare$GDP &lt;- as.double(gdp_healthcare$GDP)\n\ngdp_healthcare &lt;- gdp_healthcare %&gt;%\n  mutate(\n    year = year(Date)\n  )\n\n# Create time series\ngdp_healthcare_ts &lt;- ts(gdp_healthcare$GDP, start = c(year(min(gdp_healthcare$Date))), end = c(year(max(gdp_healthcare$Date))), frequency = 4) # Quarterly data\n\n# Importing the dataset of number of adult smokers\nadult_smokers &lt;- read_xlsx(\"data/number_of_adult_smokers_us_1965_2021.xlsx\")\n\n# Change Date format\nadult_smokers$Year &lt;- as.Date(paste(adult_smokers$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nadult_smokers_ts &lt;- ts(adult_smokers$Number, start = c(year(min(adult_smokers$Year)), month(min(adult_smokers$Year))), end = c(year(max(adult_smokers$Year)), month(max(adult_smokers$Year))), frequency = 1)\n\n# Importing the dataset of per capita alcohol consumption\nalcohol_consumption &lt;- read_xlsx(\"data/per_capita_alcohol_consumption_of_all_beverages_us_1850_2021.xlsx\")\n\n# Change Date format\nalcohol_consumption$Year &lt;- as.Date(paste(alcohol_consumption$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n\n# Create time series\nalcohol_consumption_ts &lt;- ts(alcohol_consumption$Consumption, start = c(year(min(alcohol_consumption$Year)), month(min(alcohol_consumption$Year))), end = c(year(max(alcohol_consumption$Year)), month(max(alcohol_consumption$Year))), frequency = 1)"
  },
  {
    "objectID": "multi_TS_model.html",
    "href": "multi_TS_model.html",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "",
    "text": "Code\n# # Get the covid data\n# covid &lt;- read_csv(\"data/COVID_usa_Vacc_new.csv\")\n# \n# covid &lt;- covid %&gt;% \n#   dplyr::select(date, new_cases, new_vaccinations)\n# \n# covid$date &lt;- as.Date(covid$date, format = \"%Y-%m-%d\")\n# covid$new_cases &lt;- as.double(covid$new_cases)\n# covid$new_vaccacination &lt;- as.double(covid$new_vaccinations)\n# \n# #replace 0's with nas\n# covid$new_cases[covid$new_cases==0] = NA\n# covid$new_vaccinations[covid$new_vaccinations==0] = NA\n# \n# #Missing Value Imputation by Last Observation Carried Forward\n# \n# covid[,\"new_cases\"] = na_locf(covid[,\"new_cases\"],option = 'locf',na_remaining=\"rev\")\n# covid[,\"new_vaccinations\"] = na_locf(covid[,\"new_vaccinations\"],option = 'locf',na_remaining=\"rev\")\n# covid = covid[, c('date', 'new_cases', 'new_vaccinations')]\n# health_unique &lt;- health[!duplicated(health$date), ]\n# covid_unique &lt;- covid[!duplicated(covid$date), ]\n# health_stock_covid &lt;- merge(health_unique, covid_unique, by = \"date\")\n# \n# # Import data of Public Health Outcomes\n# # Average Life Expectancy\n# average_life_expectancy &lt;- read_xlsx(\"data/life_expectancy_7countries_2000_2022.xlsx\")\n# # Change it to long format\n# average_life_expectancy &lt;- average_life_expectancy %&gt;% \n#   pivot_longer(cols = -Year, \n#                names_to = \"Country\", \n#                values_to = \"Life_Expectancy\")\n# \n# average_life_expectancy$Year &lt;- as.Date(paste(average_life_expectancy$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# average_life_expectancy$Life_Expectancy &lt;- as.double(average_life_expectancy$Life_Expectancy)\n# \n# # Filter to only include the US\n# average_life_expectancy &lt;- average_life_expectancy %&gt;% \n#   filter(Country == \"United States\")\n# \n# ale_ts &lt;- ts(average_life_expectancy$Life_Expectancy, start = c(year(min(average_life_expectancy$Year)), month(min(average_life_expectancy$Year))), end = c(year(max(average_life_expectancy$Year)), month(max(average_life_expectancy$Year))), frequency = 1) # Yearly data\n# \n# # Importing the dataset of deaths due to major diseases\n# death_cancer &lt;- read_xlsx(\"data/deaths_cancer_us_1950_2019.xlsx\")\n# death_heart &lt;- read_xlsx(\"data/deaths_heart_diseases_us_1950_2019.xlsx\")\n# death_diabetes &lt;- read_xlsx(\"data/deaths_diabetes_us_1950_2019.xlsx\")\n# \n# # Merge these three datasets\n# death_major_diseases &lt;- merge(death_cancer, death_heart, by = \"Year\")\n# death_major_diseases &lt;- merge(death_major_diseases, death_diabetes, by = \"Year\")\n# \n# # Create a new column for total deaths\n# death_major_diseases$Total_Deaths &lt;- death_major_diseases$Death_Cancer + death_major_diseases$Death_Heart_Disease + death_major_diseases$Death_Diabetes\n# \n# # Change Date format\n# death_major_diseases$Year &lt;- as.Date(paste(death_major_diseases$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# death_major_diseases$Death_Cancer &lt;- as.double(death_major_diseases$Death_Cancer)\n# death_major_diseases$Death_Heart_Disease &lt;- as.double(death_major_diseases$Death_Heart_Disease)\n# death_major_diseases$Death_Diabetes &lt;- as.double(death_major_diseases$Death_Diabetes)\n# \n# \n# major_diseases_ts &lt;- ts(death_major_diseases$Total_Deaths, start = c(year(min(death_major_diseases$Year)), month(min(death_major_diseases$Year))), end = c(year(max(death_major_diseases$Year)), month(max(death_major_diseases$Year))), frequency = 1) # Yearly data\n# \n# # Importing the dataset of infant mortality rates\n# infant_mortality &lt;- read_xlsx(\"data/infant_mortality_rate_us_1990_2021.xlsx\")\n# \n# # Change Date format\n# infant_mortality$Year &lt;- as.Date(paste(infant_mortality$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# infant_mortality$Infant_Mortality_Rates &lt;- as.double(infant_mortality$Infant_Mortality_Rates)\n# \n# # Create time series\n# infant_mortality_ts &lt;- ts(infant_mortality$Infant_Mortality_Rates, start = c(year(min(infant_mortality$Year)), month(min(infant_mortality$Year))), end = c(year(max(infant_mortality$Year)), month(max(infant_mortality$Year))), frequency = 1) # Yearly data\n# \n# # Importing the dataset of share of people without health insurance\n# health_insurance &lt;- read_xlsx(\"data/share_of_people_us_without_health_insurance_age_1997_2022.xlsx\")\n# \n# # Change Date format\n# health_insurance$Year &lt;- as.Date(paste(health_insurance$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# \n# # Change it to long format\n# health_insurance &lt;- health_insurance %&gt;% \n#   pivot_longer(cols = -Year, \n#                names_to = \"Age_Group\", \n#                values_to = \"Percentage\")\n# \n# # Filter to all ages \n# health_insurance &lt;- health_insurance %&gt;% \n#   filter(Age_Group == \"All ages\")\n# \n# # Create time series\n# health_insurance_ts &lt;- ts(health_insurance$Percentage, start = c(year(min(health_insurance$Year)), month(min(health_insurance$Year))), end = c(year(max(health_insurance$Year)), month(max(health_insurance$Year))), frequency = 1) # Yearly data\n# \n# # Importing the dataset of emergency room visits\n# emergency_room &lt;- read_xlsx(\"data/us_emergency_room_visits_1997_2019_by_age.xlsx\")\n# \n# # Change it to long format\n# emergency_room &lt;- emergency_room %&gt;% \n#   pivot_longer(cols = -Year, \n#                names_to = \"Age_Group\", \n#                values_to = \"Visits_Percent\")\n# \n# # Change Date format\n# emergency_room$Year &lt;- as.Date(paste(emergency_room$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# \n# # Get the average visits percentage for all ages\n# emergency_room &lt;- emergency_room %&gt;% \n#   group_by(Year) %&gt;%\n#   summarise(Average_Visits_Percent = mean(Visits_Percent))\n# \n# # Create time series\n# emergency_room_ts &lt;- ts(emergency_room$Average_Visits_Percent, start = c(year(min(emergency_room$Year)), month(min(emergency_room$Year))), end = c(year(max(emergency_room$Year)), month(max(emergency_room$Year))), frequency = 1)\n# \n# # Importing the dataset of prescription drug expenditure\n# prescription_drug &lt;- read_xlsx(\"data/prescription_drug_expenditure_us_1960_2022.xlsx\")\n# \n# # Change Date format\n# prescription_drug$Year &lt;- as.Date(paste(prescription_drug$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# \n# # Create time series\n# prescription_drug_ts &lt;- ts(prescription_drug$Expenditure, start = c(year(min(prescription_drug$Year)), month(min(prescription_drug$Year))), end = c(year(max(prescription_drug$Year)), month(max(prescription_drug$Year))), frequency = 1)\n# \n# # Importing the dataset of GDP by industry\n# gdp_industry &lt;- read_csv(\"data/GDP_by_Industry.csv\", skip = 4) %&gt;%\n#   dplyr::select(-Line)\n# \n# colnames(gdp_industry) &lt;- c(\"Industry\", \"2018-Q1\", \"2018-Q2\", \"2018-Q3\", \"2018-Q4\", \"2019-Q1\", \"2019-Q2\", \"2019-Q3\", \"2019-Q4\", \"2020-Q1\", \"2020-Q2\", \"2020-Q3\", \"2020-Q4\", \"2021-Q1\", \"2021-Q2\", \"2021-Q3\", \"2021-Q4\", \"2022-Q1\", \"2022-Q2\", \"2022-Q3\", \"2022-Q4\", \"2023-Q1\", \"2023-Q2\", \"2023-Q3\")\n# \n# gdp_healthcare &lt;- gdp_industry %&gt;%\n#   filter(\n#            (Industry == \"Health care and social assistance\")\n#   )\n# \n# ## Change it to long format\n# gdp_healthcare &lt;- gdp_healthcare %&gt;%\n#   pivot_longer(cols = -Industry, \n#                names_to = \"Date\", \n#                values_to = \"GDP\")\n# \n# # Change date to Date format\n# gdp_healthcare$Date &lt;- as.yearqtr(gdp_healthcare$Date, format = \"%Y-Q%q\")\n# \n# gdp_healthcare$Date &lt;- as.Date(gdp_healthcare$Date)\n# gdp_healthcare$GDP &lt;- as.double(gdp_healthcare$GDP)\n# \n# gdp_healthcare &lt;- gdp_healthcare %&gt;%\n#   mutate(\n#     year = year(Date)\n#   )\n# \n# # Create time series\n# gdp_healthcare_ts &lt;- ts(gdp_healthcare$GDP, start = c(year(min(gdp_healthcare$Date))), end = c(year(max(gdp_healthcare$Date))), frequency = 4) # Quarterly data\n# \n# # Importing the dataset of number of adult smokers\n# adult_smokers &lt;- read_xlsx(\"data/number_of_adult_smokers_us_1965_2021.xlsx\")\n# \n# # Change Date format\n# adult_smokers$Year &lt;- as.Date(paste(adult_smokers$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# \n# # Create time series\n# adult_smokers_ts &lt;- ts(adult_smokers$Number, start = c(year(min(adult_smokers$Year)), month(min(adult_smokers$Year))), end = c(year(max(adult_smokers$Year)), month(max(adult_smokers$Year))), frequency = 1)\n# \n# # Importing the dataset of per capita alcohol consumption\n# alcohol_consumption &lt;- read_xlsx(\"data/per_capita_alcohol_consumption_of_all_beverages_us_1850_2021.xlsx\")\n# \n# # Change Date format\n# alcohol_consumption$Year &lt;- as.Date(paste(alcohol_consumption$Year, \"01\", \"01\", sep = \"-\"), format = \"%Y-%m-%d\")\n# \n# # Create time series\n# alcohol_consumption_ts &lt;- ts(alcohol_consumption$Consumption, start = c(year(min(alcohol_consumption$Year)), month(min(alcohol_consumption$Year))), end = c(year(max(alcohol_consumption$Year)), month(max(alcohol_consumption$Year))), frequency = 1)\n\n\n\nKey Questions and Model Equations:\n\nWhat’s the relationship between different parts of the Healthcare Industry? (Yousaf, Pham, & Goodwill, 2023)\n\n\n(VAR) Pfizer Stock ~ Vertex stock + TEVA stock + SAGE stock\n\n\nHow the lifestyle influences the mortality rates of major diseases?\n\n\n(VAR) Mortality rates of major disease(Cancer, Heart Disease, Diabetes) ~ Smoking Prevalance + Alcohol Consumption\n\n\nHow the average life expectancy of the US be influenced by the mortality rates of Cancer, Heart Diseases, Diabetes, and infant mortality rates? (Adu, Appiahene, Afrifa, 2023)\n\n\n(ARIMAX) Average Life Expectancy ~ Mortality rates of major disease(Cancer, Heart Diseases, Diabetes) + Infant Mortality rates\n\n\nWill the US Average Life Expectancy Increase with Prescription Drug Expenditure?\n\n\n(VAR) Average Life Expectancy ~ Prescription Drug Expenditure\n\n\nHow Public Health Crisis(Covid-19) influences the financial market of Healthcare (Still have two parts to finish)\n\n\n(ARIMAX) Pfizer Stock ~ Vertex stock + TEVA stock + SAGE stock + Covid-19 Cases\n\n\n\n1. Relationship between different parts of the Healthcare Industry\n(VAR)Pfizer stock ~ Vertex stock + Teva stock + SAGE stock\nWe chose these stocks because they can each represent different pharmaceutical companies. Pfizer is a large pharmaceutical company, Vertex is a biotech company, Teva is a drug company, and SAGE is a small biotech company. They play different roles in the pharmaceutical industry, and Pfizer was chosen as our primary research subject because it made the COVID-19 vaccines. It has substantial market cap and prominent role in pharmaceuticals.\n\nData VisualizationVARselectInitial Model SelectionCross-validationModel Creation and DiagnosticsForecasting\n\n\nPfizer (PFE) shows a generally positive trend over a long period with some volatility, indicating periods of growth and market confidence, reflecting successful product launches(vaccine). Vertex Pharmaceuticals (VRTX) shows steady growth over time, but a huge drop in 2022. Teva Pharmaceutical(TEVA)shows a peak followed by a decline and then a period of stable with volatility. Sage Therapeutics (SAGE), as a smaller biotech company, it shows significant volatility, which is typical for such firms. Spikes in the stock price might correspond to positive news about drug trials or FDA approvals, while declines may indicate setbacks in development or other challenges.\n\n\nCode\nstock_ts &lt;- cbind(pharma_ts, biotech_ts, drug_ts, small_biotech_ts)\ncolnames(stock_ts) &lt;- c(\"pharma\", \"biotech\", \"drug\", \"small_biotech\")\n\nautoplot(stock_ts)\n\n\n\n\n\n\n\n\n\n\n\nAs we can see from the results, the p recommended by VARselect() are 10 and 1.\n\n\nCode\nVARselect(stock_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      1      1     10 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n)  2.357832  2.352112  2.358823  2.355605  2.356543  2.354853  2.358775\nHQ(n)   2.390402  2.406394  2.434818  2.453314  2.475964  2.495988  2.521623\nSC(n)   2.445117  2.497586  2.562486  2.617458  2.676585  2.733085  2.795197\nFPE(n) 10.568020 10.507745 10.578518 10.544564 10.554498 10.536746 10.578239\n               8         9        10\nAIC(n)  2.365996  2.371767  2.349877\nHQ(n)   2.550557  2.578041  2.577863\nSC(n)   2.860607  2.924568  2.960867\nFPE(n) 10.655015 10.716828 10.484958\n\n\n\n\nAs we can see from the results, VAR(1) model is better than VAR(10) model, based on the residual errors and significance of the coefficients. What’s more, except small-biotech stock, all the other stocks are correlated with each other.\n\n\nCode\nsummary(fitvar1&lt;-vars::VAR(stock_ts, p=10, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: pharma, biotech, drug, small_biotech \nDeterministic variables: both \nSample size: 1452 \nLog Likelihood: -9779.206 \nRoots of the characteristic polynomial:\n0.9925 0.9925 0.9897 0.8963 0.8963 0.8844 0.8844 0.8304 0.8304 0.8153 0.8153 0.8109 0.8109 0.7968 0.7968 0.7622 0.7622 0.7443 0.7443 0.7417 0.7417 0.7309 0.7309 0.7126 0.7126 0.7122 0.7122 0.7091 0.7091 0.6601 0.6292 0.6292 0.6236 0.6236 0.6151 0.6151 0.5956 0.5956 0.5617 0.5617\nCall:\nvars::VAR(y = stock_ts, p = 10, type = \"both\")\n\n\nEstimation results for equation pharma: \n======================================= \npharma = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1          9.655e-01  2.714e-02  35.574  &lt; 2e-16 ***\nbiotech.l1        -5.299e-03  2.463e-03  -2.151 0.031614 *  \ndrug.l1           -6.689e-02  5.996e-02  -1.116 0.264722    \nsmall_biotech.l1  -6.720e-03  7.606e-03  -0.884 0.377107    \npharma.l2          3.605e-03  3.796e-02   0.095 0.924352    \nbiotech.l2         4.074e-03  3.393e-03   1.201 0.230079    \ndrug.l2            1.020e-01  8.545e-02   1.194 0.232600    \nsmall_biotech.l2   1.078e-02  1.089e-02   0.989 0.322663    \npharma.l3          7.828e-02  3.799e-02   2.060 0.039556 *  \nbiotech.l3         5.924e-04  3.393e-03   0.175 0.861434    \ndrug.l3            4.240e-02  8.557e-02   0.495 0.620355    \nsmall_biotech.l3   7.559e-03  1.090e-02   0.693 0.488192    \npharma.l4         -5.744e-02  3.813e-02  -1.506 0.132203    \nbiotech.l4         3.194e-03  3.394e-03   0.941 0.346825    \ndrug.l4           -2.166e-01  8.554e-02  -2.532 0.011449 *  \nsmall_biotech.l4  -1.360e-02  1.089e-02  -1.249 0.211983    \npharma.l5          5.466e-02  3.812e-02   1.434 0.151815    \nbiotech.l5        -6.002e-03  3.404e-03  -1.763 0.078050 .  \ndrug.l5            3.085e-01  8.568e-02   3.601 0.000328 ***\nsmall_biotech.l5  -4.774e-03  1.092e-02  -0.437 0.662152    \npharma.l6         -8.343e-02  3.800e-02  -2.195 0.028307 *  \nbiotech.l6         2.726e-03  3.406e-03   0.800 0.423636    \ndrug.l6           -3.008e-01  8.586e-02  -3.503 0.000474 ***\nsmall_biotech.l6   1.137e-02  1.093e-02   1.041 0.298131    \npharma.l7          1.758e-02  3.793e-02   0.463 0.643151    \nbiotech.l7        -1.808e-03  3.398e-03  -0.532 0.594844    \ndrug.l7            6.673e-02  8.569e-02   0.779 0.436274    \nsmall_biotech.l7  -1.537e-03  1.093e-02  -0.141 0.888194    \npharma.l8         -1.153e-02  3.788e-02  -0.304 0.760957    \nbiotech.l8         2.563e-03  3.404e-03   0.753 0.451754    \ndrug.l8           -6.665e-02  8.564e-02  -0.778 0.436508    \nsmall_biotech.l8  -6.567e-03  1.093e-02  -0.601 0.548074    \npharma.l9          1.085e-01  3.782e-02   2.867 0.004201 ** \nbiotech.l9         5.615e-03  3.406e-03   1.648 0.099474 .  \ndrug.l9            1.352e-01  8.545e-02   1.582 0.113822    \nsmall_biotech.l9   2.195e-02  1.094e-02   2.007 0.044956 *  \npharma.l10        -8.871e-02  2.695e-02  -3.292 0.001021 ** \nbiotech.l10       -7.045e-03  2.461e-03  -2.862 0.004271 ** \ndrug.l10          -5.664e-02  6.006e-02  -0.943 0.345758    \nsmall_biotech.l10 -2.088e-02  7.663e-03  -2.725 0.006513 ** \nconst              1.468e+00  2.893e-01   5.075 4.39e-07 ***\ntrend              8.407e-06  4.015e-05   0.209 0.834193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6332 on 1410 degrees of freedom\nMultiple R-Squared: 0.9917, Adjusted R-squared: 0.9915 \nF-statistic:  4119 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation biotech: \n======================================== \nbiotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         -0.5394701  0.3307112  -1.631  0.10306    \nbiotech.l1         0.9374168  0.0300145  31.232  &lt; 2e-16 ***\ndrug.l1           -1.4363837  0.7305535  -1.966  0.04948 *  \nsmall_biotech.l1  -0.1375213  0.0926745  -1.484  0.13805    \npharma.l2          0.2732731  0.4625105   0.591  0.55472    \nbiotech.l2         0.0799656  0.0413446   1.934  0.05330 .  \ndrug.l2            0.6845236  1.0411831   0.657  0.51100    \nsmall_biotech.l2   0.2115728  0.1327460   1.594  0.11120    \npharma.l3          0.6956494  0.4629651   1.503  0.13317    \nbiotech.l3        -0.1185444  0.0413455  -2.867  0.00420 ** \ndrug.l3            2.5388051  1.0426828   2.435  0.01502 *  \nsmall_biotech.l3  -0.1735611  0.1328426  -1.307  0.19159    \npharma.l4         -0.8436640  0.4646582  -1.816  0.06963 .  \nbiotech.l4         0.0852964  0.0413570   2.062  0.03935 *  \ndrug.l4           -1.5481429  1.0423288  -1.485  0.13770    \nsmall_biotech.l4   0.1004235  0.1327192   0.757  0.44938    \npharma.l5          0.5638751  0.4644401   1.214  0.22491    \nbiotech.l5        -0.0008106  0.0414726  -0.020  0.98441    \ndrug.l5            0.2396961  1.0440281   0.230  0.81845    \nsmall_biotech.l5  -0.0933255  0.1330992  -0.701  0.48331    \npharma.l6         -0.7427872  0.4630731  -1.604  0.10893    \nbiotech.l6        -0.0503808  0.0415044  -1.214  0.22500    \ndrug.l6           -0.7064417  1.0462163  -0.675  0.49964    \nsmall_biotech.l6   0.0652105  0.1331520   0.490  0.62439    \npharma.l7          0.3714212  0.4622259   0.804  0.42179    \nbiotech.l7         0.1121950  0.0414046   2.710  0.00682 ** \ndrug.l7           -0.7947298  1.0440870  -0.761  0.44668    \nsmall_biotech.l7   0.1869697  0.1332014   1.404  0.16064    \npharma.l8         -0.2217761  0.4616055  -0.480  0.63099    \nbiotech.l8        -0.1051238  0.0414831  -2.534  0.01138 *  \ndrug.l8           -0.3286745  1.0434655  -0.315  0.75282    \nsmall_biotech.l8  -0.3386215  0.1331950  -2.542  0.01112 *  \npharma.l9          0.8171618  0.4608944   1.773  0.07645 .  \nbiotech.l9         0.1072026  0.0415069   2.583  0.00990 ** \ndrug.l9            1.1643985  1.0411697   1.118  0.26361    \nsmall_biotech.l9   0.3889292  0.1332582   2.919  0.00357 ** \npharma.l10        -0.4393476  0.3283823  -1.338  0.18114    \nbiotech.l10       -0.0658918  0.0299908  -2.197  0.02818 *  \ndrug.l10          -0.3742151  0.7317882  -0.511  0.60917    \nsmall_biotech.l10 -0.2463090  0.0933743  -2.638  0.00843 ** \nconst             14.5976426  3.5245187   4.142 3.65e-05 ***\ntrend             -0.0000865  0.0004893  -0.177  0.85970    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 7.716 on 1410 degrees of freedom\nMultiple R-Squared: 0.9808, Adjusted R-squared: 0.9802 \nF-statistic:  1756 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation drug: \n===================================== \ndrug = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         -1.128e-02  1.321e-02  -0.854  0.39310    \nbiotech.l1        -8.603e-04  1.199e-03  -0.718  0.47305    \ndrug.l1            9.889e-01  2.917e-02  33.897  &lt; 2e-16 ***\nsmall_biotech.l1   3.841e-03  3.701e-03   1.038  0.29952    \npharma.l2          1.334e-02  1.847e-02   0.722  0.47039    \nbiotech.l2         4.731e-03  1.651e-03   2.865  0.00423 ** \ndrug.l2           -6.208e-02  4.158e-02  -1.493  0.13566    \nsmall_biotech.l2   3.736e-03  5.301e-03   0.705  0.48107    \npharma.l3         -2.260e-02  1.849e-02  -1.223  0.22167    \nbiotech.l3        -3.639e-03  1.651e-03  -2.204  0.02768 *  \ndrug.l3            7.669e-02  4.164e-02   1.842  0.06571 .  \nsmall_biotech.l3  -1.811e-03  5.305e-03  -0.341  0.73284    \npharma.l4          3.595e-02  1.856e-02   1.937  0.05291 .  \nbiotech.l4        -2.728e-03  1.652e-03  -1.652  0.09878 .  \ndrug.l4            5.328e-02  4.163e-02   1.280  0.20075    \nsmall_biotech.l4  -1.595e-02  5.300e-03  -3.009  0.00267 ** \npharma.l5         -1.959e-02  1.855e-02  -1.056  0.29096    \nbiotech.l5         1.852e-03  1.656e-03   1.118  0.26366    \ndrug.l5           -1.293e-02  4.169e-02  -0.310  0.75659    \nsmall_biotech.l5   7.352e-03  5.315e-03   1.383  0.16683    \npharma.l6          1.189e-02  1.849e-02   0.643  0.52027    \nbiotech.l6        -1.978e-03  1.657e-03  -1.193  0.23301    \ndrug.l6           -8.702e-02  4.178e-02  -2.083  0.03745 *  \nsmall_biotech.l6   4.756e-03  5.317e-03   0.894  0.37127    \npharma.l7         -2.629e-02  1.846e-02  -1.424  0.15464    \nbiotech.l7         2.646e-03  1.653e-03   1.600  0.10980    \ndrug.l7            5.727e-02  4.170e-02   1.374  0.16980    \nsmall_biotech.l7   6.565e-03  5.319e-03   1.234  0.21736    \npharma.l8          1.040e-02  1.843e-02   0.564  0.57279    \nbiotech.l8        -1.959e-03  1.657e-03  -1.183  0.23712    \ndrug.l8           -4.526e-02  4.167e-02  -1.086  0.27765    \nsmall_biotech.l8  -1.129e-02  5.319e-03  -2.123  0.03394 *  \npharma.l9          3.580e-02  1.841e-02   1.945  0.05200 .  \nbiotech.l9         4.124e-03  1.658e-03   2.488  0.01296 *  \ndrug.l9           -4.726e-02  4.158e-02  -1.137  0.25592    \nsmall_biotech.l9   6.399e-03  5.322e-03   1.203  0.22936    \npharma.l10        -3.207e-02  1.311e-02  -2.446  0.01458 *  \nbiotech.l10       -2.331e-03  1.198e-03  -1.946  0.05184 .  \ndrug.l10           3.304e-02  2.922e-02   1.130  0.25849    \nsmall_biotech.l10 -3.268e-03  3.729e-03  -0.876  0.38101    \nconst              6.302e-01  1.408e-01   4.477 8.17e-06 ***\ntrend             -3.409e-06  1.954e-05  -0.174  0.86153    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3081 on 1410 degrees of freedom\nMultiple R-Squared: 0.9525, Adjusted R-squared: 0.9511 \nF-statistic: 689.9 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation small_biotech: \n============================================== \nsmall_biotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         -1.711e-01  1.015e-01  -1.686   0.0921 .  \nbiotech.l1         7.012e-03  9.214e-03   0.761   0.4468    \ndrug.l1            7.975e-03  2.243e-01   0.036   0.9716    \nsmall_biotech.l1   1.039e+00  2.845e-02  36.509   &lt;2e-16 ***\npharma.l2          2.631e-01  1.420e-01   1.853   0.0641 .  \nbiotech.l2        -1.640e-02  1.269e-02  -1.293   0.1964    \ndrug.l2            9.605e-02  3.196e-01   0.301   0.7638    \nsmall_biotech.l2  -6.604e-02  4.075e-02  -1.621   0.1053    \npharma.l3         -1.736e-01  1.421e-01  -1.221   0.2222    \nbiotech.l3         2.317e-02  1.269e-02   1.826   0.0681 .  \ndrug.l3           -3.106e-01  3.201e-01  -0.970   0.3320    \nsmall_biotech.l3   3.491e-02  4.078e-02   0.856   0.3921    \npharma.l4          1.145e-01  1.426e-01   0.803   0.4221    \nbiotech.l4        -1.092e-02  1.270e-02  -0.860   0.3900    \ndrug.l4            2.837e-01  3.200e-01   0.886   0.3755    \nsmall_biotech.l4  -2.376e-02  4.074e-02  -0.583   0.5599    \npharma.l5         -1.511e-01  1.426e-01  -1.060   0.2895    \nbiotech.l5        -8.980e-03  1.273e-02  -0.705   0.4807    \ndrug.l5            2.016e-01  3.205e-01   0.629   0.5294    \nsmall_biotech.l5  -3.130e-02  4.086e-02  -0.766   0.4438    \npharma.l6          7.105e-02  1.422e-01   0.500   0.6173    \nbiotech.l6        -5.298e-03  1.274e-02  -0.416   0.6776    \ndrug.l6           -4.770e-01  3.212e-01  -1.485   0.1377    \nsmall_biotech.l6   2.295e-02  4.088e-02   0.561   0.5746    \npharma.l7          5.261e-02  1.419e-01   0.371   0.7109    \nbiotech.l7         1.037e-02  1.271e-02   0.816   0.4147    \ndrug.l7            2.947e-01  3.205e-01   0.919   0.3580    \nsmall_biotech.l7   7.281e-03  4.089e-02   0.178   0.8587    \npharma.l8          5.686e-02  1.417e-01   0.401   0.6883    \nbiotech.l8        -1.173e-02  1.273e-02  -0.921   0.3571    \ndrug.l8            2.023e-01  3.203e-01   0.632   0.5278    \nsmall_biotech.l8   1.783e-03  4.089e-02   0.044   0.9652    \npharma.l9          5.435e-02  1.415e-01   0.384   0.7009    \nbiotech.l9         1.833e-02  1.274e-02   1.438   0.1506    \ndrug.l9           -1.789e-01  3.196e-01  -0.560   0.5757    \nsmall_biotech.l9   5.691e-02  4.091e-02   1.391   0.1644    \npharma.l10        -1.250e-01  1.008e-01  -1.240   0.2153    \nbiotech.l10       -3.745e-03  9.207e-03  -0.407   0.6843    \ndrug.l10          -1.382e-01  2.246e-01  -0.615   0.5386    \nsmall_biotech.l10 -4.822e-02  2.866e-02  -1.682   0.0928 .  \nconst              3.010e-01  1.082e+00   0.278   0.7809    \ntrend              3.827e-05  1.502e-04   0.255   0.7989    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2.369 on 1410 degrees of freedom\nMultiple R-Squared: 0.9842, Adjusted R-squared: 0.9838 \nF-statistic:  2149 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n               pharma biotech    drug small_biotech\npharma        0.40096  0.2439 0.02854       0.27136\nbiotech       0.24387 59.5311 0.87782      -4.91190\ndrug          0.02854  0.8778 0.09494       0.03908\nsmall_biotech 0.27136 -4.9119 0.03908       5.61017\n\nCorrelation matrix of residuals:\n               pharma  biotech    drug small_biotech\npharma        1.00000  0.04992 0.14629       0.18093\nbiotech       0.04992  1.00000 0.36924      -0.26878\ndrug          0.14629  0.36924 1.00000       0.05355\nsmall_biotech 0.18093 -0.26878 0.05355       1.00000\n\n\nCode\nsummary(fitvar2&lt;-vars::VAR(stock_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: pharma, biotech, drug, small_biotech \nDeterministic variables: both \nSample size: 1461 \nLog Likelihood: -9983.058 \nRoots of the characteristic polynomial:\n0.9909 0.9906 0.9906 0.9577\nCall:\nvars::VAR(y = stock_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation pharma: \n======================================= \npharma = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         9.893e-01  2.904e-03 340.707  &lt; 2e-16 ***\nbiotech.l1       -1.155e-03  3.870e-04  -2.985  0.00288 ** \ndrug.l1          -4.275e-02  1.430e-02  -2.989  0.00285 ** \nsmall_biotech.l1 -1.824e-03  1.195e-03  -1.526  0.12722    \nconst             1.200e+00  2.582e-01   4.647 3.68e-06 ***\ntrend             4.345e-06  4.038e-05   0.108  0.91432    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6451 on 1455 degrees of freedom\nMultiple R-Squared: 0.9912, Adjusted R-squared: 0.9911 \nF-statistic: 3.259e+04 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation biotech: \n======================================== \nbiotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1        -0.0618969  0.0352756  -1.755  0.07953 .  \nbiotech.l1        0.9826416  0.0047016 209.002  &lt; 2e-16 ***\ndrug.l1          -0.5104846  0.1737440  -2.938  0.00335 ** \nsmall_biotech.l1 -0.0319371  0.0145236  -2.199  0.02804 *  \nconst            13.4257467  3.1367749   4.280 1.99e-05 ***\ntrend            -0.0001188  0.0004906  -0.242  0.80864    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 7.837 on 1455 degrees of freedom\nMultiple R-Squared: 0.9796, Adjusted R-squared: 0.9795 \nF-statistic: 1.396e+04 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation drug: \n===================================== \ndrug = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1        -2.885e-03  1.416e-03  -2.037 0.041826 *  \nbiotech.l1        2.219e-05  1.888e-04   0.118 0.906420    \ndrug.l1           9.642e-01  6.976e-03 138.222  &lt; 2e-16 ***\nsmall_biotech.l1  6.788e-04  5.831e-04   1.164 0.244607    \nconst             4.220e-01  1.259e-01   3.351 0.000826 ***\ntrend            -6.794e-06  1.970e-05  -0.345 0.730207    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3147 on 1455 degrees of freedom\nMultiple R-Squared: 0.949,  Adjusted R-squared: 0.9488 \nF-statistic:  5413 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation small_biotech: \n============================================== \nsmall_biotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1        -6.191e-03  1.062e-02  -0.583    0.560    \nbiotech.l1        1.889e-03  1.416e-03   1.334    0.183    \ndrug.l1           5.867e-03  5.233e-02   0.112    0.911    \nsmall_biotech.l1  9.937e-01  4.374e-03 227.149   &lt;2e-16 ***\nconst            -5.266e-02  9.448e-01  -0.056    0.956    \ntrend             3.358e-05  1.478e-04   0.227    0.820    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2.361 on 1455 degrees of freedom\nMultiple R-Squared: 0.984,  Adjusted R-squared: 0.984 \nF-statistic: 1.794e+04 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n               pharma biotech    drug small_biotech\npharma        0.41618  0.4514 0.03365       0.27877\nbiotech       0.45144 61.4255 0.92748      -4.64132\ndrug          0.03365  0.9275 0.09902       0.04706\nsmall_biotech 0.27877 -4.6413 0.04706       5.57253\n\nCorrelation matrix of residuals:\n               pharma  biotech    drug small_biotech\npharma        1.00000  0.08929 0.16577       0.18305\nbiotech       0.08929  1.00000 0.37608      -0.25087\ndrug          0.16577  0.37608 1.00000       0.06336\nsmall_biotech 0.18305 -0.25087 0.06336       1.00000\n\n\n\n\nAfter doing the cross-validation test, we can see that the VAR(1) model has a better performance than the VAR(10) model, which is consistent with the initial model selection.\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(stock_ts)/folds)\n\nyr = rep(c(2020:2024),each =365) #year\n\nrmse1 = data.frame()\nrmse2 = data.frame()\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- stock_ts[-(start:end), ]\n  test_model &lt;- stock_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  # Define the fit models\n  fit &lt;- vars::VAR(train_model, p=10, type= \"both\", season = NULL, exog = NULL)\n  fit2 &lt;- vars::VAR(train_model, p=1, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  \n  # Define the prediction\n  pred &lt;- predict(fit, n.ahead = h)\n  pred2 &lt;- predict(fit2, n.ahead = h)\n  \n  pred_pfe &lt;- pred$fcst$pharma[,1]\n  pred_pfe2 &lt;- pred2$fcst$pharma[,1]\n  \n  mse &lt;- mean((pred_pfe - test_model[, \"pharma\"])^2)\n  mse2 &lt;- mean((pred_pfe2 - test_model[, \"pharma\"])^2)\n  \n  rmse1 &lt;- rbind(rmse1, data.frame(pharma = sqrt(mse)))\n  rmse2 &lt;- rbind(rmse2, data.frame(pharma = sqrt(mse2)))\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  } else if(mse2 &lt; best_performance){\n    best_model &lt;- fit2\n    best_performance &lt;- mse2\n  }\n\n}\n\n# Plot the RMSE of two models\n\nplot(rmse1$pharma, type = \"l\", col = \"blue\", xlab = \"Fold\", ylab = \"RMSE\", main = \"RMSE of VAR(10) and VAR(1)\")\nlines(rmse2$pharma, col = \"red\")\nlegend(\"topright\", legend = c(\"VAR(10)\", \"VAR(1)\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\n\n\nThe models are created using the VAR(10) and VAR(1) models, and the serial correlation test is performed. The results show that the residuals are not serially correlated, which is a good sign. What’s more, the plots show that the residuals are normally distributed. Last but not least, VAR(1) has a much lower p-value than VAR(10).\n\n\nCode\nvar_model_1 &lt;- vars::VAR(stock_ts, p=10, type= \"both\", season = NULL, exog = NULL)\nstock.serial &lt;- serial.test(var_model_1, lags.pt = 12, type = \"PT.asymptotic\") \nstock.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_1\nChi-squared = 65.342, df = 32, p-value = 0.0004536\n\n\nCode\nplot(stock.serial, names = \"pharma\", mar = c(2,2,2,2)) \n\n\n\n\n\n\n\n\n\nCode\nplot(stock.serial, names = \"biotech\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(stock.serial, names = \"drug\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(stock.serial, names = \"small_biotech\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nvar_model_2 &lt;- vars::VAR(stock_ts, p=1, type= \"both\", season = NULL, exog = NULL)\nstock_2.serial &lt;- serial.test(var_model_2, lags.pt = 12, type = \"PT.asymptotic\") \nstock_2.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_2\nChi-squared = 366.69, df = 176, p-value = 1.665e-15\n\n\nCode\nplot(stock_2.serial, names = \"pharma\", mar = c(2,2,2,2)) \n\n\n\n\n\n\n\n\n\nCode\nplot(stock_2.serial, names = \"biotech\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(stock_2.serial, names = \"drug\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(stock_2.serial, names = \"small_biotech\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\n\n\nThe Pfizer (pharma) fan chart shows a first increase then decrease trend with more significant fluctuation in the forecasted period, suggesting higher volatility or uncertainty in its future stock price. On the other hand, Vertex (biotech) demonstrates a first increase then a little decrease trend with less forecasted variance, which may indicate market confidence in its stability. Teva (drug) shows a first little decrease then increase trend with some level of volatility, though not as pronounced as Pfizer, which could reflect a moderate level of uncertainty in its market dynamics. Lastly, SAGE (small_biotech) exhibits small increase trend with greater uncertainty, which is typical for smaller biotech companies that may have high growth potential but also higher risks due to factors such as funding, clinical trial outcomes, and regulatory approvals. For the relationships between these segments, the forecasted stock prices of Pfizer and Vertex are positively correlated, while the stock prices of Teva and SAGE are negatively correlated.\n\n\nCode\n(fit.pr = predict(fitvar2, n.ahead = 365, ci = 0.95))\n\n\n$pharma\n           fcst    lower    upper        CI\n  [1,] 39.51463 38.25022 40.77903  1.264405\n  [2,] 39.60336 37.82967 41.37705  1.773690\n  [3,] 39.68967 37.53401 41.84533  2.155660\n  [4,] 39.77363 37.30260 42.24467  2.471035\n  [5,] 39.85532 37.11168 42.59895  2.743638\n  [6,] 39.93478 36.94895 42.92062  2.985836\n  [7,] 40.01210 36.80704 43.21715  3.205054\n  [8,] 40.08732 36.68114 43.49349  3.406175\n  [9,] 40.16051 36.56789 43.75312  3.592617\n [10,] 40.23171 36.46483 43.99860  3.766884\n [11,] 40.30100 36.37012 44.23187  3.930872\n [12,] 40.36841 36.28235 44.45446  4.086058\n [13,] 40.43400 36.20038 44.66761  4.233616\n [14,] 40.49781 36.12332 44.87231  4.374495\n [15,] 40.55990 36.05042 45.06938  4.509476\n [16,] 40.62031 35.98110 45.25952  4.639206\n [17,] 40.67908 35.91485 45.44331  4.764231\n [18,] 40.73626 35.85125 45.62127  4.885011\n [19,] 40.79188 35.78994 45.79382  5.001939\n [20,] 40.84599 35.73063 45.96135  5.115356\n [21,] 40.89862 35.67307 46.12418  5.225554\n [22,] 40.94982 35.61703 46.28261  5.332788\n [23,] 40.99961 35.56233 46.43689  5.437281\n [24,] 41.04804 35.50881 46.58727  5.539230\n [25,] 41.09513 35.45632 46.73394  5.638809\n [26,] 41.14091 35.40475 46.87708  5.736169\n [27,] 41.18543 35.35398 47.01688  5.831449\n [28,] 41.22871 35.30394 47.15348  5.924770\n [29,] 41.27078 35.25454 47.28702  6.016242\n [30,] 41.31166 35.20570 47.41762  6.105962\n [31,] 41.35139 35.15737 47.54541  6.194021\n [32,] 41.39000 35.10950 47.67050  6.280499\n [33,] 41.42750 35.06203 47.79297  6.365467\n [34,] 41.46393 35.01493 47.91292  6.448994\n [35,] 41.49930 34.96816 48.03044  6.531139\n [36,] 41.53365 34.92169 48.14561  6.611959\n [37,] 41.56699 34.87549 48.25850  6.691504\n [38,] 41.59936 34.82953 48.36918  6.769822\n [39,] 41.63076 34.78380 48.47771  6.846956\n [40,] 41.66122 34.73827 48.58417  6.922947\n [41,] 41.69076 34.69293 48.68860  6.997831\n [42,] 41.71941 34.64777 48.79106  7.071645\n [43,] 41.74718 34.60276 48.89160  7.144419\n [44,] 41.77409 34.55790 48.99027  7.216185\n [45,] 41.80015 34.51318 49.08712  7.286972\n [46,] 41.82539 34.46859 49.18220  7.356806\n [47,] 41.84983 34.42412 49.27554  7.425712\n [48,] 41.87348 34.37976 49.36719  7.493714\n [49,] 41.89635 34.33552 49.45718  7.560834\n [50,] 41.91847 34.29137 49.54556  7.627095\n [51,] 41.93984 34.24733 49.63236  7.692516\n [52,] 41.96049 34.20338 49.71761  7.757116\n [53,] 41.98043 34.15952 49.80135  7.820914\n [54,] 41.99968 34.11575 49.88360  7.883928\n [55,] 42.01824 34.07206 49.96441  7.946174\n [56,] 42.03613 34.02846 50.04380  8.007668\n [57,] 42.05337 33.98494 50.12179  8.068425\n [58,] 42.06996 33.94150 50.19843  8.128460\n [59,] 42.08593 33.89814 50.27372  8.187788\n [60,] 42.10128 33.85486 50.34770  8.246421\n [61,] 42.11603 33.81166 50.42040  8.304374\n [62,] 42.13019 33.76853 50.49184  8.361658\n [63,] 42.14376 33.72548 50.56205  8.418285\n [64,] 42.15677 33.68250 50.63103  8.474269\n [65,] 42.16921 33.63960 50.69883  8.529619\n [66,] 42.18111 33.59677 50.76546  8.584347\n [67,] 42.19248 33.55402 50.83094  8.638463\n [68,] 42.20332 33.51134 50.89530  8.691979\n [69,] 42.21364 33.46874 50.95854  8.744903\n [70,] 42.22346 33.42621 51.02070  8.797245\n [71,] 42.23278 33.38377 51.08180  8.849016\n [72,] 42.24162 33.34139 51.14184  8.900223\n [73,] 42.24998 33.29910 51.20085  8.950877\n [74,] 42.25787 33.25688 51.25885  9.000985\n [75,] 42.26530 33.21475 51.31586  9.050556\n [76,] 42.27228 33.17269 51.37188  9.099598\n [77,] 42.27883 33.13071 51.42695  9.148119\n [78,] 42.28494 33.08881 51.48106  9.196128\n [79,] 42.29062 33.04699 51.53425  9.243630\n [80,] 42.29589 33.00526 51.58652  9.290635\n [81,] 42.30075 32.96360 51.63790  9.337148\n [82,] 42.30521 32.92203 51.68839  9.383178\n [83,] 42.30928 32.88055 51.73801  9.428731\n [84,] 42.31296 32.83915 51.78677  9.473813\n [85,] 42.31627 32.79784 51.83470  9.518431\n [86,] 42.31920 32.75661 51.88179  9.562592\n [87,] 42.32177 32.71547 51.92807  9.606301\n [88,] 42.32399 32.67442 51.97355  9.649565\n [89,] 42.32585 32.63346 52.01824  9.692390\n [90,] 42.32738 32.59260 52.06216  9.734782\n [91,] 42.32856 32.55182 52.10531  9.776746\n [92,] 42.32942 32.51113 52.14771  9.818288\n [93,] 42.32996 32.47054 52.18937  9.859413\n [94,] 42.33018 32.43005 52.23030  9.900126\n [95,] 42.33008 32.38965 52.27052  9.940434\n [96,] 42.32969 32.34935 52.31003  9.980340\n [97,] 42.32899 32.30914 52.34884 10.019851\n [98,] 42.32800 32.26903 52.38697 10.058971\n [99,] 42.32673 32.22902 52.42443 10.097704\n[100,] 42.32517 32.18912 52.46123 10.136056\n[101,] 42.32334 32.14931 52.49737 10.174031\n[102,] 42.32124 32.10960 52.53287 10.211633\n[103,] 42.31887 32.07000 52.56774 10.248868\n[104,] 42.31625 32.03051 52.60198 10.285739\n[105,] 42.31337 31.99112 52.63562 10.322251\n[106,] 42.31024 31.95183 52.66864 10.358408\n[107,] 42.30686 31.91265 52.70108 10.394213\n[108,] 42.30325 31.87358 52.73293 10.429672\n[109,] 42.29941 31.83462 52.76420 10.464788\n[110,] 42.29533 31.79577 52.79490 10.499566\n[111,] 42.29103 31.75703 52.82504 10.534007\n[112,] 42.28651 31.71840 52.85463 10.568118\n[113,] 42.28178 31.67988 52.88368 10.601901\n[114,] 42.27684 31.64148 52.91220 10.635360\n[115,] 42.27169 31.60319 52.94018 10.668498\n[116,] 42.26633 31.56501 52.96765 10.701319\n[117,] 42.26078 31.52696 52.99461 10.733827\n[118,] 42.25504 31.48901 53.02106 10.766025\n[119,] 42.24911 31.45119 53.04702 10.797916\n[120,] 42.24299 31.41349 53.07249 10.829504\n[121,] 42.23669 31.37590 53.09748 10.860791\n[122,] 42.23022 31.33844 53.12200 10.891782\n[123,] 42.22357 31.30109 53.14605 10.922478\n[124,] 42.21675 31.26387 53.16964 10.952884\n[125,] 42.20977 31.22677 53.19277 10.983001\n[126,] 42.20263 31.18979 53.21546 11.012835\n[127,] 42.19533 31.15294 53.23771 11.042386\n[128,] 42.18787 31.11622 53.25953 11.071658\n[129,] 42.18027 31.07962 53.28092 11.100654\n[130,] 42.17252 31.04314 53.30189 11.129377\n[131,] 42.16462 31.00679 53.32245 11.157830\n[132,] 42.15659 30.97057 53.34260 11.186014\n[133,] 42.14842 30.93448 53.36235 11.213934\n[134,] 42.14011 30.89852 53.38170 11.241591\n[135,] 42.13168 30.86269 53.40066 11.268988\n[136,] 42.12312 30.82699 53.41924 11.296128\n[137,] 42.11443 30.79142 53.43744 11.323014\n[138,] 42.10563 30.75598 53.45527 11.349647\n[139,] 42.09670 30.72067 53.47273 11.376031\n[140,] 42.08767 30.68550 53.48983 11.402167\n[141,] 42.07852 30.65046 53.50658 11.428059\n[142,] 42.06926 30.61556 53.52297 11.453708\n[143,] 42.05990 30.58079 53.53902 11.479117\n[144,] 42.05044 30.54615 53.55473 11.504288\n[145,] 42.04088 30.51165 53.57010 11.529223\n[146,] 42.03122 30.47729 53.58514 11.553925\n[147,] 42.02146 30.44307 53.59986 11.578397\n[148,] 42.01162 30.40898 53.61426 11.602639\n[149,] 42.00168 30.37503 53.62834 11.626655\n[150,] 41.99167 30.34122 53.64211 11.650446\n[151,] 41.98156 30.30755 53.65558 11.674015\n[152,] 41.97138 30.27402 53.66874 11.697363\n[153,] 41.96112 30.24062 53.68161 11.720493\n[154,] 41.95078 30.20737 53.69419 11.743407\n[155,] 41.94037 30.17426 53.70647 11.766107\n[156,] 41.92988 30.14129 53.71848 11.788594\n[157,] 41.91933 30.10846 53.73021 11.810871\n[158,] 41.90872 30.07578 53.74166 11.832940\n[159,] 41.89803 30.04323 53.75284 11.854803\n[160,] 41.88729 30.01083 53.76375 11.876461\n[161,] 41.87649 29.97857 53.77440 11.897916\n[162,] 41.86563 29.94646 53.78480 11.919170\n[163,] 41.85471 29.91449 53.79494 11.940226\n[164,] 41.84375 29.88266 53.80483 11.961085\n[165,] 41.83273 29.85098 53.81447 11.981748\n[166,] 41.82166 29.81944 53.82388 12.002217\n[167,] 41.81054 29.78805 53.83304 12.022495\n[168,] 41.79938 29.75680 53.84197 12.042583\n[169,] 41.78818 29.72570 53.85066 12.062483\n[170,] 41.77694 29.69474 53.85913 12.082195\n[171,] 41.76565 29.66393 53.86738 12.101723\n[172,] 41.75433 29.63326 53.87540 12.121068\n[173,] 41.74298 29.60275 53.88321 12.140231\n[174,] 41.73159 29.57237 53.89080 12.159213\n[175,] 41.72017 29.54215 53.89818 12.178018\n[176,] 41.70872 29.51207 53.90536 12.196645\n[177,] 41.69724 29.48214 53.91233 12.215098\n[178,] 41.68573 29.45236 53.91911 12.233376\n[179,] 41.67420 29.42272 53.92568 12.251483\n[180,] 41.66265 29.39323 53.93207 12.269419\n[181,] 41.65107 29.36389 53.93826 12.287185\n[182,] 41.63948 29.33469 53.94426 12.304784\n[183,] 41.62786 29.30565 53.95008 12.322217\n[184,] 41.61623 29.27675 53.95572 12.339486\n[185,] 41.60459 29.24800 53.96118 12.356591\n[186,] 41.59293 29.21939 53.96646 12.373534\n[187,] 41.58126 29.19094 53.97157 12.390317\n[188,] 41.56957 29.16263 53.97651 12.406941\n[189,] 41.55788 29.13447 53.98129 12.423407\n[190,] 41.54618 29.10646 53.98590 12.439718\n[191,] 41.53447 29.07860 53.99035 12.455873\n[192,] 41.52276 29.05089 53.99464 12.471875\n[193,] 41.51104 29.02332 53.99877 12.487725\n[194,] 41.49932 28.99590 54.00275 12.503424\n[195,] 41.48760 28.96863 54.00658 12.518974\n[196,] 41.47588 28.94151 54.01026 12.534376\n[197,] 41.46416 28.91453 54.01379 12.549630\n[198,] 41.45245 28.88771 54.01719 12.564740\n[199,] 41.44073 28.86103 54.02044 12.579705\n[200,] 41.42902 28.83450 54.02355 12.594526\n[201,] 41.41732 28.80811 54.02653 12.609207\n[202,] 41.40562 28.78188 54.02937 12.623746\n[203,] 41.39394 28.75579 54.03208 12.638146\n[204,] 41.38226 28.72985 54.03467 12.652409\n[205,] 41.37059 28.70405 54.03712 12.666534\n[206,] 41.35893 28.67841 54.03945 12.680523\n[207,] 41.34729 28.65291 54.04166 12.694378\n[208,] 41.33565 28.62755 54.04375 12.708100\n[209,] 41.32404 28.60235 54.04573 12.721689\n[210,] 41.31244 28.57729 54.04758 12.735148\n[211,] 41.30085 28.55237 54.04933 12.748476\n[212,] 41.28928 28.52761 54.05096 12.761676\n[213,] 41.27773 28.50299 54.05248 12.774748\n[214,] 41.26620 28.47851 54.05390 12.787693\n[215,] 41.25469 28.45418 54.05521 12.800513\n[216,] 41.24320 28.42999 54.05641 12.813209\n[217,] 41.23174 28.40596 54.05752 12.825781\n[218,] 41.22029 28.38206 54.05852 12.838231\n[219,] 41.20887 28.35831 54.05943 12.850560\n[220,] 41.19747 28.33470 54.06024 12.862769\n[221,] 41.18610 28.31124 54.06096 12.874859\n[222,] 41.17475 28.28792 54.06159 12.886831\n[223,] 41.16344 28.26475 54.06212 12.898685\n[224,] 41.15214 28.24172 54.06257 12.910424\n[225,] 41.14088 28.21883 54.06293 12.922048\n[226,] 41.12964 28.19609 54.06320 12.933558\n[227,] 41.11844 28.17348 54.06339 12.944955\n[228,] 41.10726 28.15102 54.06350 12.956240\n[229,] 41.09612 28.12871 54.06353 12.967414\n[230,] 41.08501 28.10653 54.06348 12.978477\n[231,] 41.07393 28.08449 54.06336 12.989432\n[232,] 41.06288 28.06260 54.06316 13.000279\n[233,] 41.05187 28.04085 54.06288 13.011018\n[234,] 41.04089 28.01923 54.06254 13.021651\n[235,] 41.02994 27.99776 54.06212 13.032179\n[236,] 41.01903 27.97643 54.06163 13.042603\n[237,] 41.00816 27.95523 54.06108 13.052923\n[238,] 40.99732 27.93418 54.06046 13.063140\n[239,] 40.98652 27.91326 54.05977 13.073256\n[240,] 40.97576 27.89248 54.05903 13.083271\n[241,] 40.96503 27.87184 54.05822 13.093186\n[242,] 40.95434 27.85134 54.05734 13.103002\n[243,] 40.94370 27.83098 54.05641 13.112719\n[244,] 40.93309 27.81075 54.05543 13.122340\n[245,] 40.92252 27.79066 54.05438 13.131864\n[246,] 40.91199 27.77070 54.05328 13.141292\n[247,] 40.90150 27.75088 54.05213 13.150626\n[248,] 40.89106 27.73119 54.05092 13.159866\n[249,] 40.88065 27.71164 54.04967 13.169012\n[250,] 40.87029 27.69223 54.04836 13.178067\n[251,] 40.85997 27.67294 54.04700 13.187030\n[252,] 40.84970 27.65379 54.04560 13.195902\n[253,] 40.83946 27.63478 54.04415 13.204684\n[254,] 40.82927 27.61590 54.04265 13.213378\n[255,] 40.81913 27.59714 54.04111 13.221983\n[256,] 40.80903 27.57853 54.03953 13.230500\n[257,] 40.79897 27.56004 54.03790 13.238931\n[258,] 40.78896 27.54168 54.03623 13.247276\n[259,] 40.77899 27.52346 54.03453 13.255536\n[260,] 40.76907 27.50536 54.03278 13.263711\n[261,] 40.75920 27.48739 54.03100 13.271803\n[262,] 40.74937 27.46956 54.02918 13.279812\n[263,] 40.73959 27.45185 54.02733 13.287738\n[264,] 40.72985 27.43427 54.02544 13.295584\n[265,] 40.72016 27.41682 54.02351 13.303348\n[266,] 40.71052 27.39949 54.02156 13.311033\n[267,] 40.70093 27.38229 54.01957 13.318638\n[268,] 40.69139 27.36522 54.01755 13.326164\n[269,] 40.68189 27.34828 54.01550 13.333613\n[270,] 40.67244 27.33146 54.01343 13.340985\n[271,] 40.66304 27.31476 54.01132 13.348280\n[272,] 40.65369 27.29819 54.00919 13.355500\n[273,] 40.64439 27.28174 54.00703 13.362644\n[274,] 40.63513 27.26542 54.00485 13.369714\n[275,] 40.62593 27.24922 54.00264 13.376710\n[276,] 40.61678 27.23314 54.00041 13.383633\n[277,] 40.60767 27.21719 53.99816 13.390484\n[278,] 40.59862 27.20135 53.99588 13.397263\n[279,] 40.58961 27.18564 53.99358 13.403971\n[280,] 40.58066 27.17005 53.99126 13.410609\n[281,] 40.57175 27.15457 53.98893 13.417177\n[282,] 40.56290 27.13922 53.98657 13.423675\n[283,] 40.55409 27.12399 53.98420 13.430105\n[284,] 40.54534 27.10887 53.98181 13.436468\n[285,] 40.53664 27.09387 53.97940 13.442763\n[286,] 40.52799 27.07900 53.97698 13.448991\n[287,] 40.51939 27.06423 53.97454 13.455153\n[288,] 40.51084 27.04959 53.97209 13.461250\n[289,] 40.50234 27.03506 53.96962 13.467282\n[290,] 40.49389 27.02064 53.96714 13.473250\n[291,] 40.48549 27.00634 53.96465 13.479154\n[292,] 40.47715 26.99215 53.96214 13.484995\n[293,] 40.46886 26.97808 53.95963 13.490773\n[294,] 40.46061 26.96412 53.95710 13.496490\n[295,] 40.45242 26.95028 53.95457 13.502145\n[296,] 40.44429 26.93655 53.95203 13.507740\n[297,] 40.43620 26.92292 53.94947 13.513275\n[298,] 40.42816 26.90941 53.94691 13.518750\n[299,] 40.42018 26.89601 53.94435 13.524165\n[300,] 40.41225 26.88273 53.94177 13.529523\n[301,] 40.40437 26.86955 53.93919 13.534822\n[302,] 40.39654 26.85648 53.93660 13.540064\n[303,] 40.38876 26.84351 53.93401 13.545249\n[304,] 40.38104 26.83066 53.93142 13.550378\n[305,] 40.37337 26.81792 53.92882 13.555451\n[306,] 40.36575 26.80528 53.92621 13.560469\n[307,] 40.35818 26.79274 53.92361 13.565432\n[308,] 40.35066 26.78032 53.92100 13.570341\n[309,] 40.34319 26.76800 53.91839 13.575196\n[310,] 40.33578 26.75578 53.91578 13.579998\n[311,] 40.32842 26.74367 53.91316 13.584747\n[312,] 40.32111 26.73166 53.91055 13.589444\n[313,] 40.31385 26.71976 53.90794 13.594090\n[314,] 40.30664 26.70796 53.90532 13.598684\n[315,] 40.29948 26.69626 53.90271 13.603227\n[316,] 40.29238 26.68466 53.90010 13.607720\n[317,] 40.28533 26.67316 53.89749 13.612164\n[318,] 40.27833 26.66177 53.89488 13.616558\n[319,] 40.27138 26.65047 53.89228 13.620904\n[320,] 40.26448 26.63928 53.88968 13.625201\n[321,] 40.25763 26.62818 53.88708 13.629450\n[322,] 40.25083 26.61718 53.88449 13.633652\n[323,] 40.24409 26.60628 53.88190 13.637807\n[324,] 40.23739 26.59548 53.87931 13.641916\n[325,] 40.23075 26.58477 53.87673 13.645979\n[326,] 40.22416 26.57416 53.87416 13.649996\n[327,] 40.21762 26.56365 53.87159 13.653968\n[328,] 40.21113 26.55323 53.86902 13.657895\n[329,] 40.20469 26.54291 53.86647 13.661778\n[330,] 40.19830 26.53268 53.86392 13.665617\n[331,] 40.19196 26.52255 53.86137 13.669413\n[332,] 40.18567 26.51250 53.85884 13.673166\n[333,] 40.17943 26.50256 53.85631 13.676877\n[334,] 40.17324 26.49270 53.85379 13.680545\n[335,] 40.16711 26.48293 53.85128 13.684172\n[336,] 40.16102 26.47326 53.84877 13.687757\n[337,] 40.15498 26.46368 53.84628 13.691302\n[338,] 40.14899 26.45418 53.84380 13.694806\n[339,] 40.14305 26.44478 53.84132 13.698270\n[340,] 40.13716 26.43547 53.83886 13.701694\n[341,] 40.13132 26.42624 53.83640 13.705080\n[342,] 40.12553 26.41710 53.83396 13.708426\n[343,] 40.11979 26.40805 53.83152 13.711734\n[344,] 40.11410 26.39909 53.82910 13.715004\n[345,] 40.10845 26.39022 53.82669 13.718236\n[346,] 40.10286 26.38142 53.82429 13.721431\n[347,] 40.09731 26.37272 53.82190 13.724589\n[348,] 40.09181 26.36410 53.81952 13.727710\n[349,] 40.08636 26.35556 53.81716 13.730796\n[350,] 40.08096 26.34711 53.81480 13.733845\n[351,] 40.07560 26.33875 53.81246 13.736859\n[352,] 40.07030 26.33046 53.81014 13.739838\n[353,] 40.06504 26.32226 53.80782 13.742782\n[354,] 40.05983 26.31414 53.80552 13.745692\n[355,] 40.05467 26.30610 53.80323 13.748568\n[356,] 40.04955 26.29814 53.80096 13.751410\n[357,] 40.04448 26.29026 53.79870 13.754218\n[358,] 40.03946 26.28247 53.79645 13.756994\n[359,] 40.03448 26.27475 53.79422 13.759737\n[360,] 40.02956 26.26711 53.79200 13.762448\n[361,] 40.02467 26.25955 53.78980 13.765127\n[362,] 40.01984 26.25206 53.78761 13.767774\n[363,] 40.01505 26.24466 53.78544 13.770390\n[364,] 40.01031 26.23733 53.78328 13.772975\n[365,] 40.00561 26.23008 53.78114 13.775530\n\n$biotech\n           fcst    lower    upper        CI\n  [1,] 184.8246 169.4635 200.1857  15.36110\n  [2,] 186.2117 164.7341 207.6892  21.47755\n  [3,] 187.5721 161.5568 213.5874  26.01531\n  [4,] 188.9066 159.1869 218.6262  29.71961\n  [5,] 190.2158 157.3318 223.0998  32.88396\n  [6,] 191.5005 155.8392 227.1617  35.66127\n  [7,] 192.7613 154.6175 230.9050  38.14377\n  [8,] 193.9988 153.6064 234.3912  40.39238\n  [9,] 195.2137 152.7637 237.6636  42.44995\n [10,] 196.4065 152.0584 240.7545  44.34804\n [11,] 197.5778 151.4671 243.6885  46.11069\n [12,] 198.7282 150.9714 246.4849  47.75674\n [13,] 199.8581 150.5569 249.1594  49.30126\n [14,] 200.9682 150.2116 251.7247  50.75653\n [15,] 202.0588 149.9261 254.1915  52.13270\n [16,] 203.1305 149.6922 256.5687  53.43828\n [17,] 204.1836 149.5032 258.8641  54.68045\n [18,] 205.2188 149.3534 261.0841  55.86536\n [19,] 206.2363 149.2380 263.2346  56.99829\n [20,] 207.2366 149.1527 265.3204  58.08381\n [21,] 208.2200 149.0941 267.3459  59.12592\n [22,] 209.1870 149.0589 269.3152  60.12813\n [23,] 210.1380 149.0445 271.2315  61.09352\n [24,] 211.0732 149.0484 273.0981  62.02482\n [25,] 211.9931 149.0687 274.9176  62.92447\n [26,] 212.8980 149.1033 276.6926  63.79464\n [27,] 213.7881 149.1509 278.4254  64.63726\n [28,] 214.6639 149.2098 280.1180  65.45409\n [29,] 215.5255 149.2788 281.7722  66.24670\n [30,] 216.3734 149.3569 283.3899  67.01651\n [31,] 217.2077 149.4429 284.9726  67.76483\n [32,] 218.0288 149.5360 286.5217  68.49282\n [33,] 218.8370 149.6354 288.0385  69.20156\n [34,] 219.6324 149.7403 289.5244  69.89202\n [35,] 220.4153 149.8502 290.9804  70.56510\n [36,] 221.1860 149.9644 292.4076  71.22164\n [37,] 221.9447 150.0823 293.8071  71.86238\n [38,] 222.6917 150.2036 295.1797  72.48803\n [39,] 223.4271 150.3278 296.5263  73.09923\n [40,] 224.1511 150.4545 297.8477  73.69659\n [41,] 224.8641 150.5834 299.1448  74.28066\n [42,] 225.5661 150.7142 300.4181  74.85196\n [43,] 226.2574 150.8465 301.6684  75.41097\n [44,] 226.9382 150.9801 302.8963  75.95814\n [45,] 227.6086 151.1147 304.1025  76.49389\n [46,] 228.2689 151.2503 305.2875  77.01862\n [47,] 228.9192 151.3864 306.4519  77.53270\n [48,] 229.5596 151.5231 307.5961  78.03648\n [49,] 230.1904 151.6601 308.7207  78.53028\n [50,] 230.8117 151.7973 309.8261  79.01442\n [51,] 231.4236 151.9345 310.9128  79.48918\n [52,] 232.0264 152.0716 311.9813  79.95485\n [53,] 232.6202 152.2085 313.0318  80.41169\n [54,] 233.2050 152.3451 314.0649  80.85994\n [55,] 233.7811 152.4813 315.0809  81.29984\n [56,] 234.3486 152.6170 316.0802  81.73161\n [57,] 234.9076 152.7522 317.0631  82.15548\n [58,] 235.4583 152.8867 318.0300  82.57165\n [59,] 236.0008 153.0205 318.9811  82.98030\n [60,] 236.5352 153.1535 319.9168  83.38163\n [61,] 237.0616 153.2858 320.8374  83.77582\n [62,] 237.5802 153.4171 321.7432  84.16303\n [63,] 238.0910 153.5476 322.6344  84.54344\n [64,] 238.5942 153.6770 323.5114  84.91719\n [65,] 239.0900 153.8055 324.3744  85.28445\n [66,] 239.5783 153.9330 325.2237  85.64536\n [67,] 240.0594 154.0593 326.0594  86.00005\n [68,] 240.5333 154.1846 326.8819  86.34866\n [69,] 241.0001 154.3087 327.6914  86.69133\n [70,] 241.4599 154.4317 328.4881  87.02817\n [71,] 241.9128 154.5535 329.2722  87.35932\n [72,] 242.3590 154.6741 330.0439  87.68489\n [73,] 242.7985 154.7935 330.8035  88.00499\n [74,] 243.2314 154.9116 331.5511  88.31973\n [75,] 243.6578 155.0285 332.2870  88.62922\n [76,] 244.0777 155.1442 333.0113  88.93356\n [77,] 244.4914 155.2585 333.7242  89.23286\n [78,] 244.8988 155.3716 334.4260  89.52721\n [79,] 245.3000 155.4833 335.1167  89.81670\n [80,] 245.6952 155.5938 335.7967  90.10143\n [81,] 246.0844 155.7029 336.4659  90.38149\n [82,] 246.4677 155.8107 337.1246  90.65696\n [83,] 246.8451 155.9172 337.7731  90.92793\n [84,] 247.2168 156.0223 338.4113  91.19448\n [85,] 247.5828 156.1261 339.0395  91.45669\n [86,] 247.9432 156.2286 339.6578  91.71463\n [87,] 248.2981 156.3297 340.2665  91.96839\n [88,] 248.6475 156.4294 340.8655  92.21804\n [89,] 248.9915 156.5278 341.4551  92.46365\n [90,] 249.3302 156.6249 342.0355  92.70529\n [91,] 249.6636 156.7206 342.6067  92.94302\n [92,] 249.9919 156.8150 343.1688  93.17693\n [93,] 250.3150 156.9080 343.7221  93.40706\n [94,] 250.6331 156.9996 344.2666  93.63350\n [95,] 250.9462 157.0899 344.8025  93.85629\n [96,] 251.2544 157.1789 345.3299  94.07550\n [97,] 251.5577 157.2665 345.8489  94.29120\n [98,] 251.8562 157.3528 346.3597  94.50343\n [99,] 252.1500 157.4377 346.8623  94.71225\n[100,] 252.4391 157.5214 347.3568  94.91774\n[101,] 252.7236 157.6036 347.8435  95.11992\n[102,] 253.0035 157.6846 348.3223  95.31887\n[103,] 253.2789 157.7642 348.7935  95.51463\n[104,] 253.5498 157.8426 349.2571  95.70726\n[105,] 253.8164 157.9196 349.7132  95.89681\n[106,] 254.0786 157.9953 350.1619  96.08332\n[107,] 254.3366 158.0697 350.6034  96.26684\n[108,] 254.5903 158.1428 351.0377  96.44743\n[109,] 254.8398 158.2147 351.4649  96.62513\n[110,] 255.0852 158.2852 351.8852  96.79998\n[111,] 255.3265 158.3545 352.2986  96.97203\n[112,] 255.5639 158.4225 352.7052  97.14132\n[113,] 255.7972 158.4893 353.1051  97.30791\n[114,] 256.0266 158.5548 353.4984  97.47182\n[115,] 256.2521 158.6190 353.8852  97.63311\n[116,] 256.4738 158.6820 354.2657  97.79181\n[117,] 256.6918 158.7438 354.6397  97.94796\n[118,] 256.9060 158.8044 355.0076  98.10161\n[119,] 257.1165 158.8637 355.3693  98.25279\n[120,] 257.3234 158.9218 355.7249  98.40154\n[121,] 257.5266 158.9788 356.0745  98.54790\n[122,] 257.7264 159.0345 356.4183  98.69190\n[123,] 257.9226 159.0890 356.7562  98.83359\n[124,] 258.1154 159.1424 357.0884  98.97300\n[125,] 258.3047 159.1946 357.4149  99.11015\n[126,] 258.4907 159.2456 357.7358  99.24510\n[127,] 258.6733 159.2954 358.0512  99.37787\n[128,] 258.8527 159.3442 358.3612  99.50849\n[129,] 259.0288 159.3917 358.6658  99.63701\n[130,] 259.2016 159.4382 358.9651  99.76344\n[131,] 259.3714 159.4835 359.2592  99.88783\n[132,] 259.5379 159.5277 359.5481 100.01020\n[133,] 259.7014 159.5708 359.8320 100.13059\n[134,] 259.8619 159.6128 360.1109 100.24902\n[135,] 260.0193 159.6538 360.3848 100.36554\n[136,] 260.1738 159.6936 360.6539 100.48015\n[137,] 260.3253 159.7324 360.9182 100.59290\n[138,] 260.4739 159.7701 361.1777 100.70382\n[139,] 260.6196 159.8067 361.4326 100.81293\n[140,] 260.7626 159.8423 361.6828 100.92026\n[141,] 260.9027 159.8769 361.9285 101.02583\n[142,] 261.0401 159.9104 362.1697 101.12968\n[143,] 261.1747 159.9429 362.4066 101.23183\n[144,] 261.3067 159.9744 362.6390 101.33231\n[145,] 261.4360 160.0049 362.8671 101.43114\n[146,] 261.5627 160.0343 363.0911 101.52835\n[147,] 261.6868 160.0629 363.3108 101.62397\n[148,] 261.8084 160.0904 363.5264 101.71801\n[149,] 261.9274 160.1169 363.7379 101.81051\n[150,] 262.0440 160.1425 363.9455 101.90148\n[151,] 262.1581 160.1671 364.1490 101.99095\n[152,] 262.2698 160.1908 364.3487 102.07895\n[153,] 262.3791 160.2136 364.5446 102.16549\n[154,] 262.4860 160.2354 364.7366 102.25061\n[155,] 262.5906 160.2563 364.9249 102.33431\n[156,] 262.6929 160.2763 365.1095 102.41663\n[157,] 262.7930 160.2954 365.2905 102.49759\n[158,] 262.8908 160.3136 365.4680 102.57720\n[159,] 262.9863 160.3309 365.6418 102.65549\n[160,] 263.0798 160.3473 365.8122 102.73249\n[161,] 263.1710 160.3628 365.9792 102.80820\n[162,] 263.2602 160.3775 366.1428 102.88265\n[163,] 263.3472 160.3913 366.3031 102.95586\n[164,] 263.4322 160.4043 366.4600 103.02785\n[165,] 263.5151 160.4165 366.6138 103.09864\n[166,] 263.5961 160.4278 366.7643 103.16824\n[167,] 263.6750 160.4383 366.9117 103.23669\n[168,] 263.7520 160.4480 367.0560 103.30399\n[169,] 263.8270 160.4569 367.1972 103.37016\n[170,] 263.9002 160.4650 367.3354 103.43523\n[171,] 263.9715 160.4723 367.4707 103.49920\n[172,] 264.0409 160.4788 367.6030 103.56210\n[173,] 264.1085 160.4845 367.7324 103.62395\n[174,] 264.1742 160.4895 367.8590 103.68476\n[175,] 264.2382 160.4937 367.9828 103.74454\n[176,] 264.3005 160.4972 368.1038 103.80332\n[177,] 264.3610 160.4999 368.2221 103.86112\n[178,] 264.4198 160.5019 368.3378 103.91793\n[179,] 264.4770 160.5032 368.4508 103.97380\n[180,] 264.5324 160.5037 368.5611 104.02872\n[181,] 264.5863 160.5035 368.6690 104.08271\n[182,] 264.6385 160.5027 368.7743 104.13580\n[183,] 264.6891 160.5011 368.8771 104.18799\n[184,] 264.7382 160.4989 368.9775 104.23930\n[185,] 264.7857 160.4959 369.0754 104.28974\n[186,] 264.8317 160.4923 369.1710 104.33933\n[187,] 264.8762 160.4881 369.2642 104.38808\n[188,] 264.9192 160.4832 369.3552 104.43601\n[189,] 264.9607 160.4776 369.4438 104.48313\n[190,] 265.0008 160.4714 369.5303 104.52945\n[191,] 265.0395 160.4645 369.6145 104.57499\n[192,] 265.0768 160.4571 369.6966 104.61976\n[193,] 265.1127 160.4490 369.7765 104.66377\n[194,] 265.1473 160.4402 369.8543 104.70703\n[195,] 265.1805 160.4309 369.9301 104.74957\n[196,] 265.2124 160.4210 370.0038 104.79138\n[197,] 265.2430 160.4105 370.0755 104.83249\n[198,] 265.2723 160.3994 370.1452 104.87290\n[199,] 265.3004 160.3878 370.2130 104.91263\n[200,] 265.3272 160.3755 370.2789 104.95168\n[201,] 265.3528 160.3627 370.3429 104.99008\n[202,] 265.3772 160.3494 370.4050 105.02782\n[203,] 265.4004 160.3355 370.4653 105.06493\n[204,] 265.4224 160.3210 370.5238 105.10141\n[205,] 265.4433 160.3060 370.5806 105.13727\n[206,] 265.4630 160.2905 370.6356 105.17253\n[207,] 265.4817 160.2745 370.6889 105.20719\n[208,] 265.4992 160.2579 370.7405 105.24127\n[209,] 265.5157 160.2409 370.7904 105.27478\n[210,] 265.5310 160.2233 370.8387 105.30771\n[211,] 265.5454 160.2053 370.8855 105.34010\n[212,] 265.5587 160.1867 370.9306 105.37194\n[213,] 265.5709 160.1677 370.9742 105.40324\n[214,] 265.5822 160.1482 371.0162 105.43401\n[215,] 265.5925 160.1283 371.0568 105.46427\n[216,] 265.6019 160.1078 371.0959 105.49402\n[217,] 265.6102 160.0870 371.1335 105.52327\n[218,] 265.6177 160.0656 371.1697 105.55203\n[219,] 265.6242 160.0439 371.2045 105.58031\n[220,] 265.6298 160.0216 371.2379 105.60812\n[221,] 265.6345 159.9990 371.2699 105.63546\n[222,] 265.6383 159.9760 371.3006 105.66234\n[223,] 265.6413 159.9525 371.3300 105.68878\n[224,] 265.6434 159.9286 371.3581 105.71478\n[225,] 265.6446 159.9043 371.3850 105.74034\n[226,] 265.6451 159.8796 371.4106 105.76548\n[227,] 265.6447 159.8545 371.4349 105.79020\n[228,] 265.6436 159.8291 371.4581 105.81451\n[229,] 265.6416 159.8032 371.4800 105.83842\n[230,] 265.6389 159.7770 371.5008 105.86193\n[231,] 265.6354 159.7504 371.5205 105.88506\n[232,] 265.6312 159.7234 371.5390 105.90780\n[233,] 265.6263 159.6961 371.5565 105.93018\n[234,] 265.6206 159.6685 371.5728 105.95218\n[235,] 265.6143 159.6404 371.5881 105.97382\n[236,] 265.6072 159.6121 371.6023 105.99511\n[237,] 265.5995 159.5834 371.6155 106.01605\n[238,] 265.5911 159.5544 371.6277 106.03665\n[239,] 265.5820 159.5251 371.6389 106.05691\n[240,] 265.5723 159.4954 371.6491 106.07685\n[241,] 265.5619 159.4655 371.6584 106.09646\n[242,] 265.5510 159.4352 371.6667 106.11575\n[243,] 265.5394 159.4046 371.6741 106.13474\n[244,] 265.5272 159.3738 371.6806 106.15341\n[245,] 265.5144 159.3426 371.6862 106.17179\n[246,] 265.5011 159.3112 371.6909 106.18987\n[247,] 265.4872 159.2795 371.6948 106.20767\n[248,] 265.4727 159.2475 371.6979 106.22518\n[249,] 265.4577 159.2152 371.7001 106.24241\n[250,] 265.4421 159.1827 371.7015 106.25937\n[251,] 265.4260 159.1500 371.7021 106.27605\n[252,] 265.4094 159.1169 371.7019 106.29248\n[253,] 265.3923 159.0837 371.7009 106.30865\n[254,] 265.3747 159.0501 371.6993 106.32456\n[255,] 265.3566 159.0164 371.6968 106.34022\n[256,] 265.3380 158.9824 371.6937 106.35564\n[257,] 265.3190 158.9482 371.6898 106.37082\n[258,] 265.2995 158.9137 371.6853 106.38576\n[259,] 265.2796 158.8791 371.6800 106.40047\n[260,] 265.2592 158.8442 371.6741 106.41496\n[261,] 265.2384 158.8091 371.6676 106.42922\n[262,] 265.2171 158.7739 371.6604 106.44326\n[263,] 265.1955 158.7384 371.6526 106.45709\n[264,] 265.1734 158.7027 371.6441 106.47071\n[265,] 265.1510 158.6669 371.6351 106.48412\n[266,] 265.1281 158.6308 371.6255 106.49733\n[267,] 265.1049 158.5946 371.6153 106.51034\n[268,] 265.0813 158.5582 371.6045 106.52315\n[269,] 265.0574 158.5216 371.5931 106.53577\n[270,] 265.0331 158.4849 371.5813 106.54821\n[271,] 265.0084 158.4480 371.5689 106.56045\n[272,] 264.9834 158.4109 371.5559 106.57252\n[273,] 264.9581 158.3737 371.5425 106.58441\n[274,] 264.9324 158.3363 371.5286 106.59612\n[275,] 264.9065 158.2988 371.5141 106.60767\n[276,] 264.8802 158.2612 371.4992 106.61904\n[277,] 264.8536 158.2234 371.4839 106.63025\n[278,] 264.8267 158.1854 371.4680 106.64130\n[279,] 264.7996 158.1474 371.4518 106.65218\n[280,] 264.7721 158.1092 371.4351 106.66292\n[281,] 264.7444 158.0709 371.4179 106.67349\n[282,] 264.7164 158.0325 371.4004 106.68392\n[283,] 264.6882 157.9940 371.3824 106.69420\n[284,] 264.6597 157.9554 371.3640 106.70433\n[285,] 264.6309 157.9166 371.3453 106.71432\n[286,] 264.6020 157.8778 371.3261 106.72418\n[287,] 264.5727 157.8388 371.3066 106.73389\n[288,] 264.5433 157.7998 371.2868 106.74347\n[289,] 264.5136 157.7607 371.2665 106.75292\n[290,] 264.4837 157.7215 371.2460 106.76223\n[291,] 264.4536 157.6822 371.2250 106.77142\n[292,] 264.4233 157.6428 371.2038 106.78049\n[293,] 264.3928 157.6034 371.1822 106.78943\n[294,] 264.3621 157.5639 371.1604 106.79825\n[295,] 264.3312 157.5243 371.1382 106.80695\n[296,] 264.3002 157.4846 371.1157 106.81554\n[297,] 264.2689 157.4449 371.0929 106.82401\n[298,] 264.2375 157.4051 371.0699 106.83237\n[299,] 264.2059 157.3653 371.0466 106.84062\n[300,] 264.1742 157.3254 371.0230 106.84876\n[301,] 264.1423 157.2855 370.9991 106.85679\n[302,] 264.1103 157.2455 370.9750 106.86472\n[303,] 264.0781 157.2055 370.9506 106.87255\n[304,] 264.0458 157.1655 370.9260 106.88027\n[305,] 264.0133 157.1254 370.9012 106.88790\n[306,] 263.9807 157.0853 370.8761 106.89543\n[307,] 263.9480 157.0451 370.8508 106.90286\n[308,] 263.9151 157.0049 370.8253 106.91020\n[309,] 263.8822 156.9647 370.7996 106.91745\n[310,] 263.8491 156.9245 370.7737 106.92461\n[311,] 263.8159 156.8843 370.7476 106.93167\n[312,] 263.7827 156.8440 370.7213 106.93865\n[313,] 263.7493 156.8037 370.6948 106.94555\n[314,] 263.7158 156.7635 370.6682 106.95236\n[315,] 263.6823 156.7232 370.6413 106.95908\n[316,] 263.6486 156.6829 370.6143 106.96573\n[317,] 263.6149 156.6426 370.5872 106.97229\n[318,] 263.5811 156.6023 370.5599 106.97877\n[319,] 263.5472 156.5620 370.5324 106.98518\n[320,] 263.5133 156.5217 370.5048 106.99151\n[321,] 263.4792 156.4815 370.4770 106.99777\n[322,] 263.4452 156.4412 370.4491 107.00395\n[323,] 263.4110 156.4010 370.4211 107.01006\n[324,] 263.3768 156.3607 370.3929 107.01609\n[325,] 263.3426 156.3205 370.3647 107.02206\n[326,] 263.3083 156.2803 370.3363 107.02796\n[327,] 263.2740 156.2402 370.3078 107.03379\n[328,] 263.2396 156.2000 370.2791 107.03955\n[329,] 263.2052 156.1599 370.2504 107.04525\n[330,] 263.1707 156.1198 370.2216 107.05088\n[331,] 263.1362 156.0798 370.1927 107.05645\n[332,] 263.1017 156.0398 370.1637 107.06195\n[333,] 263.0672 155.9998 370.1346 107.06740\n[334,] 263.0326 155.9599 370.1054 107.07278\n[335,] 262.9981 155.9200 370.0762 107.07810\n[336,] 262.9635 155.8801 370.0468 107.08336\n[337,] 262.9289 155.8403 370.0174 107.08857\n[338,] 262.8943 155.8005 369.9880 107.09372\n[339,] 262.8596 155.7608 369.9585 107.09881\n[340,] 262.8250 155.7212 369.9289 107.10385\n[341,] 262.7904 155.6816 369.8992 107.10883\n[342,] 262.7558 155.6420 369.8695 107.11376\n[343,] 262.7211 155.6025 369.8398 107.11864\n[344,] 262.6865 155.5631 369.8100 107.12346\n[345,] 262.6519 155.5237 369.7801 107.12823\n[346,] 262.6173 155.4844 369.7503 107.13295\n[347,] 262.5827 155.4451 369.7204 107.13762\n[348,] 262.5482 155.4059 369.6904 107.14225\n[349,] 262.5136 155.3668 369.6604 107.14682\n[350,] 262.4791 155.3277 369.6304 107.15135\n[351,] 262.4446 155.2888 369.6004 107.15582\n[352,] 262.4101 155.2499 369.5704 107.16026\n[353,] 262.3757 155.2110 369.5403 107.16464\n[354,] 262.3412 155.1722 369.5102 107.16898\n[355,] 262.3068 155.1336 369.4801 107.17328\n[356,] 262.2725 155.0950 369.4500 107.17753\n[357,] 262.2382 155.0564 369.4199 107.18174\n[358,] 262.2039 155.0180 369.3898 107.18591\n[359,] 262.1696 154.9796 369.3597 107.19003\n[360,] 262.1354 154.9413 369.3296 107.19412\n[361,] 262.1013 154.9031 369.2994 107.19816\n[362,] 262.0672 154.8650 369.2693 107.20216\n[363,] 262.0331 154.8270 369.2392 107.20612\n[364,] 261.9991 154.7891 369.2091 107.21004\n[365,] 261.9651 154.7512 369.1791 107.21392\n\n$drug\n           fcst    lower     upper        CI\n  [1,] 9.243400 8.626662  9.860138 0.6167380\n  [2,] 9.246342 8.389797 10.102887 0.8565454\n  [3,] 9.248851 8.218328 10.279374 1.0305233\n  [4,] 9.250951 8.081680 10.420222 1.1692710\n  [5,] 9.252664 7.967723 10.537606 1.2849413\n  [6,] 9.254013 7.870094 10.637933 1.3839198\n  [7,] 9.255019 7.784928 10.725109 1.4700904\n  [8,] 9.255700 7.709659 10.801742 1.5460415\n  [9,] 9.256077 7.642467 10.869687 1.6136102\n [10,] 9.256167 7.582006 10.930329 1.6741613\n [11,] 9.255988 7.527244 10.984733 1.7287445\n [12,] 9.255556 7.477366 11.033746 1.7781896\n [13,] 9.254887 7.431720 11.078053 1.8231666\n [14,] 9.253995 7.389769 11.118221 1.8642263\n [15,] 9.252896 7.351067 11.154724 1.9018283\n [16,] 9.251602 7.315242 11.187962 1.9363605\n [17,] 9.250127 7.281973 11.218281 1.9681539\n [18,] 9.248483 7.250990 11.245977 1.9974937\n [19,] 9.246682 7.222055 11.271309 2.0246269\n [20,] 9.244736 7.194966 11.294505 2.0497695\n [21,] 9.242654 7.169543 11.315765 2.0731110\n [22,] 9.240447 7.145628 11.335265 2.0948187\n [23,] 9.238125 7.123084 11.353166 2.1150410\n [24,] 9.235697 7.101787 11.369607 2.1339100\n [25,] 9.233172 7.081628 11.384715 2.1515438\n [26,] 9.230558 7.062509 11.398606 2.1680482\n [27,] 9.227863 7.044345 11.411381 2.1835182\n [28,] 9.225095 7.027055 11.423134 2.1980395\n [29,] 9.222260 7.010571 11.433950 2.2116896\n [30,] 9.219367 6.994828 11.443905 2.2245385\n [31,] 9.216421 6.979771 11.453070 2.2366499\n [32,] 9.213428 6.965346 11.461509 2.2480814\n [33,] 9.210394 6.951508 11.469280 2.2588857\n [34,] 9.207325 6.938215 11.476436 2.2691107\n [35,] 9.204226 6.925426 11.483027 2.2788001\n [36,] 9.201103 6.913109 11.489097 2.2879940\n [37,] 9.197959 6.901230 11.494688 2.2967290\n [38,] 9.194799 6.889761 11.499838 2.3050385\n [39,] 9.191628 6.878675 11.504582 2.3129534\n [40,] 9.188450 6.867948 11.508951 2.3205018\n [41,] 9.185267 6.857558 11.512977 2.3277098\n [42,] 9.182085 6.847484 11.516686 2.3346012\n [43,] 9.178906 6.837708 11.520104 2.3411979\n [44,] 9.175734 6.828214 11.523254 2.3475202\n [45,] 9.172571 6.818984 11.526158 2.3535867\n [46,] 9.169420 6.810006 11.528835 2.3594147\n [47,] 9.166285 6.801265 11.531305 2.3650200\n [48,] 9.163167 6.792750 11.533584 2.3704173\n [49,] 9.160069 6.784449 11.535689 2.3756202\n [50,] 9.156994 6.776352 11.537635 2.3806412\n [51,] 9.153942 6.768450 11.539434 2.3854920\n [52,] 9.150917 6.760734 11.541100 2.3901832\n [53,] 9.147920 6.753195 11.542645 2.3947249\n [54,] 9.144953 6.745826 11.544079 2.3991263\n [55,] 9.142017 6.738621 11.545413 2.4033958\n [56,] 9.139114 6.731572 11.546655 2.4075416\n [57,] 9.136245 6.724674 11.547816 2.4115709\n [58,] 9.133412 6.717921 11.548902 2.4154905\n [59,] 9.130615 6.711308 11.549922 2.4193070\n [60,] 9.127857 6.704831 11.550883 2.4230261\n [61,] 9.125137 6.698484 11.551790 2.4266533\n [62,] 9.122457 6.692263 11.552651 2.4301939\n [63,] 9.119818 6.686166 11.553470 2.4336524\n [64,] 9.117220 6.680187 11.554254 2.4370335\n [65,] 9.114665 6.674324 11.555006 2.4403411\n [66,] 9.112152 6.668573 11.555731 2.4435792\n [67,] 9.109683 6.662931 11.556434 2.4467513\n [68,] 9.107257 6.657397 11.557118 2.4498608\n [69,] 9.104876 6.651966 11.557787 2.4529108\n [70,] 9.102540 6.646636 11.558444 2.4559042\n [71,] 9.100249 6.641405 11.559093 2.4588437\n [72,] 9.098004 6.636272 11.559735 2.4617319\n [73,] 9.095804 6.631233 11.560375 2.4645711\n [74,] 9.093650 6.626286 11.561014 2.4673636\n [75,] 9.091542 6.621431 11.561654 2.4701115\n [76,] 9.089481 6.616664 11.562297 2.4728166\n [77,] 9.087466 6.611985 11.562946 2.4754809\n [78,] 9.085497 6.607391 11.563603 2.4781059\n [79,] 9.083574 6.602881 11.564268 2.4806934\n [80,] 9.081699 6.598454 11.564943 2.4832447\n [81,] 9.079869 6.594108 11.565630 2.4857613\n [82,] 9.078086 6.589841 11.566331 2.4882446\n [83,] 9.076349 6.585653 11.567045 2.4906956\n [84,] 9.074658 6.581542 11.567774 2.4931157\n [85,] 9.073013 6.577507 11.568519 2.4955059\n [86,] 9.071414 6.573547 11.569281 2.4978672\n [87,] 9.069861 6.569660 11.570061 2.5002005\n [88,] 9.068353 6.565846 11.570859 2.5025068\n [89,] 9.066890 6.562103 11.571677 2.5047870\n [90,] 9.065471 6.558430 11.572513 2.5070418\n [91,] 9.064098 6.554826 11.573370 2.5092719\n [92,] 9.062769 6.551291 11.574247 2.5114781\n [93,] 9.061484 6.547823 11.575145 2.5136611\n [94,] 9.060242 6.544421 11.576064 2.5158215\n [95,] 9.059044 6.541085 11.577004 2.5179599\n [96,] 9.057890 6.537813 11.577966 2.5200768\n [97,] 9.056777 6.534604 11.578950 2.5221729\n [98,] 9.055708 6.531459 11.579956 2.5242485\n [99,] 9.054680 6.528375 11.580984 2.5263042\n[100,] 9.053693 6.525353 11.582034 2.5283404\n[101,] 9.052748 6.522391 11.583106 2.5303575\n[102,] 9.051844 6.519488 11.584200 2.5323560\n[103,] 9.050980 6.516644 11.585316 2.5343362\n[104,] 9.050156 6.513858 11.586455 2.5362985\n[105,] 9.049372 6.511129 11.587615 2.5382433\n[106,] 9.048627 6.508456 11.588798 2.5401709\n[107,] 9.047920 6.505839 11.590002 2.5420815\n[108,] 9.047252 6.503277 11.591228 2.5439756\n[109,] 9.046622 6.500769 11.592475 2.5458533\n[110,] 9.046029 6.498314 11.593744 2.5477150\n[111,] 9.045473 6.495912 11.595034 2.5495608\n[112,] 9.044953 6.493562 11.596345 2.5513912\n[113,] 9.044470 6.491264 11.597676 2.5532062\n[114,] 9.044022 6.489016 11.599028 2.5550062\n[115,] 9.043610 6.486818 11.600401 2.5567912\n[116,] 9.043232 6.484670 11.601793 2.5585617\n[117,] 9.042888 6.482570 11.603206 2.5603176\n[118,] 9.042578 6.480519 11.604637 2.5620593\n[119,] 9.042301 6.478515 11.606088 2.5637870\n[120,] 9.042058 6.476557 11.607558 2.5655007\n[121,] 9.041846 6.474646 11.609047 2.5672007\n[122,] 9.041667 6.472780 11.610554 2.5688871\n[123,] 9.041519 6.470959 11.612079 2.5705601\n[124,] 9.041402 6.469183 11.613622 2.5722199\n[125,] 9.041316 6.467450 11.615183 2.5738665\n[126,] 9.041260 6.465760 11.616760 2.5755002\n[127,] 9.041234 6.464112 11.618355 2.5771211\n[128,] 9.041236 6.462507 11.619966 2.5787292\n[129,] 9.041268 6.460943 11.621593 2.5803248\n[130,] 9.041328 6.459420 11.623236 2.5819080\n[131,] 9.041415 6.457937 11.624894 2.5834788\n[132,] 9.041531 6.456493 11.626568 2.5850374\n[133,] 9.041673 6.455089 11.628257 2.5865840\n[134,] 9.041841 6.453723 11.629960 2.5881186\n[135,] 9.042036 6.452395 11.631678 2.5896413\n[136,] 9.042257 6.451104 11.633409 2.5911523\n[137,] 9.042502 6.449851 11.635154 2.5926516\n[138,] 9.042773 6.448633 11.636912 2.5941394\n[139,] 9.043068 6.447452 11.638684 2.5956158\n[140,] 9.043387 6.446306 11.640467 2.5970808\n[141,] 9.043729 6.445195 11.642263 2.5985345\n[142,] 9.044095 6.444117 11.644072 2.5999771\n[143,] 9.044483 6.443074 11.645891 2.6014086\n[144,] 9.044893 6.442064 11.647722 2.6028291\n[145,] 9.045326 6.441087 11.649564 2.6042387\n[146,] 9.045779 6.440142 11.651417 2.6056376\n[147,] 9.046254 6.439229 11.653280 2.6070257\n[148,] 9.046750 6.438347 11.655153 2.6084032\n[149,] 9.047266 6.437496 11.657036 2.6097701\n[150,] 9.047802 6.436675 11.658928 2.6111265\n[151,] 9.048357 6.435884 11.660830 2.6124726\n[152,] 9.048931 6.435123 11.662740 2.6138083\n[153,] 9.049525 6.434391 11.664658 2.6151338\n[154,] 9.050136 6.433687 11.666585 2.6164492\n[155,] 9.050766 6.433011 11.668520 2.6177544\n[156,] 9.051413 6.432363 11.670462 2.6190497\n[157,] 9.052077 6.431742 11.672412 2.6203350\n[158,] 9.052758 6.431148 11.674369 2.6216104\n[159,] 9.053456 6.430580 11.676332 2.6228760\n[160,] 9.054170 6.430038 11.678302 2.6241319\n[161,] 9.054900 6.429522 11.680278 2.6253782\n[162,] 9.055645 6.429030 11.682260 2.6266148\n[163,] 9.056406 6.428564 11.684248 2.6278420\n[164,] 9.057181 6.428121 11.686241 2.6290596\n[165,] 9.057971 6.427703 11.688238 2.6302679\n[166,] 9.058774 6.427307 11.690241 2.6314669\n[167,] 9.059592 6.426935 11.692249 2.6326566\n[168,] 9.060423 6.426586 11.694260 2.6338371\n[169,] 9.061267 6.426259 11.696276 2.6350085\n[170,] 9.062124 6.425953 11.698295 2.6361708\n[171,] 9.062994 6.425669 11.700318 2.6373241\n[172,] 9.063875 6.425407 11.702344 2.6384685\n[173,] 9.064769 6.425165 11.704373 2.6396040\n[174,] 9.065674 6.424943 11.706405 2.6407307\n[175,] 9.066591 6.424742 11.708439 2.6418486\n[176,] 9.067518 6.424560 11.710476 2.6429578\n[177,] 9.068456 6.424398 11.712515 2.6440584\n[178,] 9.069405 6.424255 11.714555 2.6451504\n[179,] 9.070364 6.424130 11.716598 2.6462338\n[180,] 9.071332 6.424023 11.718641 2.6473088\n[181,] 9.072310 6.423935 11.720686 2.6483754\n[182,] 9.073298 6.423864 11.722732 2.6494337\n[183,] 9.074294 6.423811 11.724778 2.6504836\n[184,] 9.075300 6.423774 11.726825 2.6515253\n[185,] 9.076313 6.423754 11.728872 2.6525588\n[186,] 9.077335 6.423751 11.730919 2.6535842\n[187,] 9.078365 6.423764 11.732967 2.6546015\n[188,] 9.079403 6.423792 11.735014 2.6556108\n[189,] 9.080448 6.423836 11.737060 2.6566121\n[190,] 9.081500 6.423895 11.739106 2.6576055\n[191,] 9.082559 6.423968 11.741150 2.6585911\n[192,] 9.083625 6.424057 11.743194 2.6595688\n[193,] 9.084698 6.424159 11.745237 2.6605388\n[194,] 9.085777 6.424276 11.747278 2.6615011\n[195,] 9.086861 6.424406 11.749317 2.6624557\n[196,] 9.087952 6.424549 11.751355 2.6634028\n[197,] 9.089048 6.424706 11.753391 2.6643422\n[198,] 9.090150 6.424876 11.755424 2.6652742\n[199,] 9.091256 6.425058 11.757455 2.6661988\n[200,] 9.092368 6.425252 11.759484 2.6671159\n[201,] 9.093484 6.425459 11.761510 2.6680257\n[202,] 9.094605 6.425677 11.763533 2.6689281\n[203,] 9.095730 6.425907 11.765554 2.6698234\n[204,] 9.096860 6.426148 11.767571 2.6707114\n[205,] 9.097993 6.426400 11.769585 2.6715923\n[206,] 9.099129 6.426663 11.771595 2.6724660\n[207,] 9.100270 6.426937 11.773602 2.6733327\n[208,] 9.101413 6.427221 11.775606 2.6741924\n[209,] 9.102560 6.427515 11.777605 2.6750451\n[210,] 9.103710 6.427819 11.779601 2.6758909\n[211,] 9.104862 6.428132 11.781592 2.6767298\n[212,] 9.106017 6.428455 11.783579 2.6775619\n[213,] 9.107174 6.428787 11.785562 2.6783872\n[214,] 9.108334 6.429128 11.787540 2.6792058\n[215,] 9.109496 6.429478 11.789513 2.6800177\n[216,] 9.110659 6.429836 11.791482 2.6808229\n[217,] 9.111824 6.430203 11.793446 2.6816216\n[218,] 9.112991 6.430577 11.795405 2.6824136\n[219,] 9.114159 6.430960 11.797358 2.6831992\n[220,] 9.115328 6.431350 11.799307 2.6839783\n[221,] 9.116499 6.431748 11.801250 2.6847509\n[222,] 9.117670 6.432153 11.803187 2.6855172\n[223,] 9.118842 6.432565 11.805119 2.6862771\n[224,] 9.120015 6.432984 11.807046 2.6870308\n[225,] 9.121188 6.433410 11.808966 2.6877781\n[226,] 9.122362 6.433842 11.810881 2.6885193\n[227,] 9.123535 6.434281 11.812790 2.6892543\n[228,] 9.124709 6.434726 11.814692 2.6899831\n[229,] 9.125883 6.435177 11.816589 2.6907059\n[230,] 9.127056 6.435634 11.818479 2.6914226\n[231,] 9.128229 6.436096 11.820362 2.6921333\n[232,] 9.129402 6.436564 11.822240 2.6928381\n[233,] 9.130574 6.437037 11.824110 2.6935369\n[234,] 9.131745 6.437515 11.825975 2.6942298\n[235,] 9.132915 6.437998 11.827832 2.6949169\n[236,] 9.134084 6.438486 11.829682 2.6955981\n[237,] 9.135253 6.438979 11.831526 2.6962736\n[238,] 9.136419 6.439476 11.833363 2.6969434\n[239,] 9.137585 6.439978 11.835193 2.6976075\n[240,] 9.138749 6.440483 11.837015 2.6982659\n[241,] 9.139912 6.440993 11.838831 2.6989187\n[242,] 9.141073 6.441507 11.840639 2.6995660\n[243,] 9.142232 6.442024 11.842439 2.7002077\n[244,] 9.143389 6.442545 11.844233 2.7008439\n[245,] 9.144544 6.443070 11.846019 2.7014746\n[246,] 9.145697 6.443597 11.847797 2.7020999\n[247,] 9.146848 6.444128 11.849568 2.7027198\n[248,] 9.147997 6.444662 11.851331 2.7033344\n[249,] 9.149143 6.445199 11.853087 2.7039437\n[250,] 9.150287 6.445739 11.854835 2.7045477\n[251,] 9.151428 6.446282 11.856575 2.7051464\n[252,] 9.152567 6.446827 11.858307 2.7057400\n[253,] 9.153702 6.447374 11.860031 2.7063283\n[254,] 9.154835 6.447924 11.861747 2.7069116\n[255,] 9.155965 6.448476 11.863455 2.7074897\n[256,] 9.157092 6.449030 11.865155 2.7080628\n[257,] 9.158216 6.449585 11.866847 2.7086309\n[258,] 9.159337 6.450143 11.868531 2.7091939\n[259,] 9.160454 6.450702 11.870206 2.7097520\n[260,] 9.161569 6.451263 11.871874 2.7103052\n[261,] 9.162679 6.451826 11.873533 2.7108535\n[262,] 9.163787 6.452390 11.875184 2.7113969\n[263,] 9.164890 6.452955 11.876826 2.7119356\n[264,] 9.165990 6.453521 11.878460 2.7124694\n[265,] 9.167087 6.454088 11.880085 2.7129985\n[266,] 9.168180 6.454657 11.881702 2.7135228\n[267,] 9.169268 6.455226 11.883311 2.7140425\n[268,] 9.170353 6.455796 11.884911 2.7145575\n[269,] 9.171434 6.456366 11.886502 2.7150680\n[270,] 9.172511 6.456938 11.888085 2.7155738\n[271,] 9.173584 6.457509 11.889659 2.7160750\n[272,] 9.174653 6.458081 11.891225 2.7165718\n[273,] 9.175718 6.458654 11.892782 2.7170640\n[274,] 9.176778 6.459226 11.894330 2.7175518\n[275,] 9.177834 6.459799 11.895870 2.7180352\n[276,] 9.178886 6.460372 11.897400 2.7185142\n[277,] 9.179934 6.460945 11.898922 2.7189888\n[278,] 9.180976 6.461517 11.900436 2.7194591\n[279,] 9.182015 6.462090 11.901940 2.7199251\n[280,] 9.183049 6.462662 11.903436 2.7203868\n[281,] 9.184078 6.463234 11.904922 2.7208443\n[282,] 9.185103 6.463805 11.906400 2.7212975\n[283,] 9.186123 6.464376 11.907869 2.7217466\n[284,] 9.187138 6.464946 11.909329 2.7221916\n[285,] 9.188148 6.465516 11.910781 2.7226324\n[286,] 9.189154 6.466085 11.912223 2.7230692\n[287,] 9.190155 6.466653 11.913656 2.7235019\n[288,] 9.191150 6.467220 11.915081 2.7239306\n[289,] 9.192141 6.467786 11.916497 2.7243552\n[290,] 9.193127 6.468351 11.917903 2.7247759\n[291,] 9.194108 6.468916 11.919301 2.7251927\n[292,] 9.195084 6.469479 11.920690 2.7256056\n[293,] 9.196055 6.470041 11.922070 2.7260146\n[294,] 9.197021 6.470601 11.923441 2.7264197\n[295,] 9.197982 6.471161 11.924803 2.7268210\n[296,] 9.198937 6.471718 11.926156 2.7272185\n[297,] 9.199887 6.472275 11.927500 2.7276123\n[298,] 9.200832 6.472830 11.928835 2.7280023\n[299,] 9.201772 6.473384 11.930161 2.7283886\n[300,] 9.202707 6.473935 11.931478 2.7287713\n[301,] 9.203636 6.474486 11.932786 2.7291503\n[302,] 9.204560 6.475034 11.934085 2.7295256\n[303,] 9.205478 6.475581 11.935376 2.7298974\n[304,] 9.206391 6.476126 11.936657 2.7302656\n[305,] 9.207299 6.476669 11.937929 2.7306302\n[306,] 9.208202 6.477210 11.939193 2.7309914\n[307,] 9.209099 6.477750 11.940448 2.7313490\n[308,] 9.209990 6.478287 11.941693 2.7317032\n[309,] 9.210876 6.478822 11.942930 2.7320540\n[310,] 9.211756 6.479355 11.944158 2.7324013\n[311,] 9.212631 6.479886 11.945377 2.7327453\n[312,] 9.213501 6.480415 11.946587 2.7330859\n[313,] 9.214365 6.480942 11.947788 2.7334232\n[314,] 9.215223 6.481466 11.948980 2.7337572\n[315,] 9.216076 6.481988 11.950164 2.7340879\n[316,] 9.216923 6.482508 11.951338 2.7344153\n[317,] 9.217765 6.483025 11.952504 2.7347396\n[318,] 9.218601 6.483540 11.953661 2.7350606\n[319,] 9.219431 6.484053 11.954810 2.7353785\n[320,] 9.220256 6.484563 11.955949 2.7356932\n[321,] 9.221075 6.485070 11.957080 2.7360048\n[322,] 9.221888 6.485575 11.958202 2.7363133\n[323,] 9.222696 6.486078 11.959315 2.7366187\n[324,] 9.223498 6.486577 11.960419 2.7369210\n[325,] 9.224295 6.487074 11.961515 2.7372204\n[326,] 9.225086 6.487569 11.962602 2.7375167\n[327,] 9.225871 6.488061 11.963681 2.7378101\n[328,] 9.226650 6.488550 11.964751 2.7381005\n[329,] 9.227424 6.489036 11.965812 2.7383879\n[330,] 9.228192 6.489519 11.966864 2.7386725\n[331,] 9.228954 6.490000 11.967908 2.7389542\n[332,] 9.229711 6.490478 11.968944 2.7392331\n[333,] 9.230462 6.490953 11.969971 2.7395091\n[334,] 9.231207 6.491425 11.970989 2.7397822\n[335,] 9.231946 6.491894 11.971999 2.7400527\n[336,] 9.232680 6.492360 11.973000 2.7403203\n[337,] 9.233408 6.492823 11.973993 2.7405852\n[338,] 9.234130 6.493283 11.974978 2.7408474\n[339,] 9.234847 6.493740 11.975954 2.7411069\n[340,] 9.235558 6.494194 11.976922 2.7413637\n[341,] 9.236263 6.494645 11.977881 2.7416178\n[342,] 9.236963 6.495093 11.978832 2.7418694\n[343,] 9.237657 6.495538 11.979775 2.7421183\n[344,] 9.238345 6.495980 11.980709 2.7423646\n[345,] 9.239027 6.496419 11.981636 2.7426084\n[346,] 9.239704 6.496854 11.982554 2.7428497\n[347,] 9.240375 6.497287 11.983463 2.7430884\n[348,] 9.241040 6.497716 11.984365 2.7433246\n[349,] 9.241700 6.498142 11.985259 2.7435583\n[350,] 9.242354 6.498565 11.986144 2.7437896\n[351,] 9.243003 6.498984 11.987021 2.7440185\n[352,] 9.243645 6.499400 11.987890 2.7442449\n[353,] 9.244282 6.499813 11.988751 2.7444690\n[354,] 9.244914 6.500223 11.989605 2.7446907\n[355,] 9.245540 6.500630 11.990450 2.7449100\n[356,] 9.246160 6.501033 11.991287 2.7451270\n[357,] 9.246774 6.501433 11.992116 2.7453417\n[358,] 9.247383 6.501829 11.992938 2.7455541\n[359,] 9.247987 6.502223 11.993751 2.7457642\n[360,] 9.248585 6.502613 11.994557 2.7459721\n[361,] 9.249177 6.502999 11.995355 2.7461778\n[362,] 9.249764 6.503382 11.996145 2.7463812\n[363,] 9.250345 6.503762 11.996927 2.7465825\n[364,] 9.250920 6.504139 11.997702 2.7467815\n[365,] 9.251490 6.504512 11.998469 2.7469785\n\n$small_biotech\n           fcst     lower    upper        CI\n  [1,] 46.71585 42.089118 51.34258  4.626729\n  [2,] 46.57477 40.058207 53.09133  6.516562\n  [3,] 46.43671 38.487990 54.38542  7.948716\n  [4,] 46.30160 37.160417 55.44279  9.141184\n  [5,] 46.16940 35.990595 56.34821 10.178805\n  [6,] 46.04005 34.934758 57.14534 11.105291\n  [7,] 45.91349 33.966766 57.86022 11.946726\n  [8,] 45.78968 33.069495 58.50987 12.720186\n  [9,] 45.66856 32.230976 59.10615 13.437587\n [10,] 45.55009 31.442446 59.65774 14.107645\n [11,] 45.43422 30.697245 60.17119 14.736972\n [12,] 45.32089 29.990164 60.65162 15.330729\n [13,] 45.21007 29.317035 61.10311 15.893039\n [14,] 45.10171 28.674452 61.52898 16.427263\n [15,] 44.99577 28.059589 61.93196 16.936184\n [16,] 44.89220 27.470066 62.31434 17.422139\n [17,] 44.79097 26.903855 62.67808 17.887115\n [18,] 44.69203 26.359208 63.02485 18.332819\n [19,] 44.59534 25.834604 63.35607 18.760733\n [20,] 44.50086 25.328710 63.67301 19.172150\n [21,] 44.40856 24.840347 63.97677 19.568211\n [22,] 44.31839 24.368467 64.26832 19.949928\n [23,] 44.23033 23.912130 64.54854 20.318203\n [24,] 44.14434 23.470491 64.81818 20.673847\n [25,] 44.06037 23.042786 65.07796 21.017588\n [26,] 43.97841 22.628317 65.32850 21.350090\n [27,] 43.89840 22.226451 65.57036 21.671952\n [28,] 43.82033 21.836605 65.80406 21.983725\n [29,] 43.74416 21.458243 66.03007 22.285912\n [30,] 43.66985 21.090871 66.24882 22.578977\n [31,] 43.59738 20.734031 66.46072 22.863345\n [32,] 43.52671 20.387297 66.66612 23.139413\n [33,] 43.45782 20.050273 66.86537 23.407547\n [34,] 43.39068 19.722589 67.05876 23.668087\n [35,] 43.32525 19.403899 67.24660 23.921352\n [36,] 43.26152 19.093877 67.42915 24.167638\n [37,] 43.19944 18.792219 67.60667 24.407223\n [38,] 43.13900 18.498636 67.77937 24.640369\n [39,] 43.08018 18.212856 67.94749 24.867319\n [40,] 43.02293 17.934623 68.11123 25.088306\n [41,] 42.96724 17.663693 68.27078 25.303546\n [42,] 42.91308 17.399836 68.42633 25.513245\n [43,] 42.86043 17.142832 68.57803 25.717598\n [44,] 42.80926 16.892472 68.72605 25.916788\n [45,] 42.75955 16.648559 68.87054 26.110991\n [46,] 42.71128 16.410902 69.01165 26.300373\n [47,] 42.66441 16.179320 69.14950 26.485092\n [48,] 42.61894 15.953640 69.28424 26.665298\n [49,] 42.57483 15.733697 69.41596 26.841133\n [50,] 42.53207 15.519332 69.54480 27.012737\n [51,] 42.49063 15.310391 69.67087 27.180238\n [52,] 42.45049 15.106730 69.79425 27.343761\n [53,] 42.41164 14.908208 69.91506 27.503428\n [54,] 42.37404 14.714689 70.03339 27.659350\n [55,] 42.33768 14.526043 70.14932 27.811640\n [56,] 42.30255 14.342145 70.26295 27.960402\n [57,] 42.26861 14.162875 70.37435 28.105737\n [58,] 42.23586 13.988115 70.48360 28.247742\n [59,] 42.20426 13.817753 70.59078 28.386511\n [60,] 42.17381 13.651681 70.69595 28.522134\n [61,] 42.14449 13.489793 70.79919 28.654697\n [62,] 42.11627 13.331987 70.90056 28.784284\n [63,] 42.08914 13.178166 71.00012 28.910976\n [64,] 42.06308 13.028234 71.09793 29.034849\n [65,] 42.03808 12.882099 71.19406 29.155979\n [66,] 42.01411 12.739671 71.28855 29.274438\n [67,] 41.99116 12.600863 71.38145 29.390296\n [68,] 41.96921 12.465592 71.47283 29.503620\n [69,] 41.94825 12.333775 71.56273 29.614476\n [70,] 41.92826 12.205334 71.65119 29.722927\n [71,] 41.90923 12.080191 71.73826 29.829034\n [72,] 41.89113 11.958272 71.82398 29.932856\n [73,] 41.87395 11.839503 71.90841 30.034451\n [74,] 41.85769 11.723813 71.99156 30.133875\n [75,] 41.84231 11.611134 72.07349 30.231180\n [76,] 41.82782 11.501399 72.15424 30.326420\n [77,] 41.81419 11.394542 72.23383 30.419645\n [78,] 41.80140 11.290499 72.31231 30.510904\n [79,] 41.78945 11.189210 72.38970 30.600245\n [80,] 41.77833 11.090612 72.46604 30.687714\n [81,] 41.76801 10.994648 72.54136 30.773357\n [82,] 41.75848 10.901260 72.61570 30.857218\n [83,] 41.74973 10.810392 72.68907 30.939337\n [84,] 41.74175 10.721990 72.76151 31.019758\n [85,] 41.73452 10.636000 72.83304 31.098521\n [86,] 41.72803 10.552369 72.90370 31.175663\n [87,] 41.72227 10.471049 72.97350 31.251225\n [88,] 41.71723 10.391988 73.04247 31.325241\n [89,] 41.71289 10.315138 73.11064 31.397749\n [90,] 41.70924 10.240453 73.17802 31.468783\n [91,] 41.70626 10.167886 73.24464 31.538378\n [92,] 41.70396 10.097392 73.31053 31.606567\n [93,] 41.70231 10.028926 73.37569 31.673382\n [94,] 41.70130  9.962446 73.44016 31.738855\n [95,] 41.70093  9.897910 73.50394 31.803017\n [96,] 41.70117  9.835276 73.56707 31.865897\n [97,] 41.70203  9.774505 73.62955 31.927524\n [98,] 41.70348  9.715556 73.69141 31.987928\n [99,] 41.70553  9.658391 73.75266 32.047135\n[100,] 41.70815  9.602972 73.81332 32.105173\n[101,] 41.71133  9.549263 73.87340 32.162069\n[102,] 41.71508  9.497228 73.93292 32.217847\n[103,] 41.71936  9.446831 73.99190 32.272534\n[104,] 41.72419  9.398037 74.05034 32.326153\n[105,] 41.72954  9.350813 74.10827 32.378728\n[106,] 41.73541  9.305126 74.16569 32.430283\n[107,] 41.74178  9.260943 74.22262 32.480841\n[108,] 41.74866  9.218232 74.27908 32.530423\n[109,] 41.75601  9.176963 74.33507 32.579051\n[110,] 41.76385  9.137105 74.39060 32.626747\n[111,] 41.77216  9.098627 74.44569 32.673531\n[112,] 41.78092  9.061502 74.50035 32.719423\n[113,] 41.79014  9.025699 74.55459 32.764443\n[114,] 41.79980  8.991192 74.60841 32.808610\n[115,] 41.80990  8.957953 74.66184 32.851943\n[116,] 41.82041  8.925954 74.71487 32.894460\n[117,] 41.83135  8.895170 74.76753 32.936179\n[118,] 41.84269  8.865575 74.81981 32.977117\n[119,] 41.85444  8.837143 74.87173 33.017293\n[120,] 41.86657  8.809849 74.92329 33.056721\n[121,] 41.87909  8.783670 74.97451 33.095419\n[122,] 41.89198  8.758581 75.02539 33.133403\n[123,] 41.90525  8.734559 75.07593 33.170687\n[124,] 41.91887  8.711581 75.12616 33.207288\n[125,] 41.93285  8.689625 75.17606 33.243220\n[126,] 41.94717  8.668669 75.22566 33.278497\n[127,] 41.96182  8.648691 75.27496 33.313134\n[128,] 41.97681  8.629669 75.32396 33.347145\n[129,] 41.99213  8.611585 75.37267 33.380543\n[130,] 42.00776  8.594416 75.42110 33.413341\n[131,] 42.02369  8.578143 75.46925 33.445552\n[132,] 42.03994  8.562747 75.51713 33.477189\n[133,] 42.05647  8.548208 75.56474 33.508264\n[134,] 42.07330  8.534508 75.61209 33.538790\n[135,] 42.09041  8.521628 75.65918 33.568778\n[136,] 42.10779  8.509550 75.70603 33.598239\n[137,] 42.12544  8.498257 75.75263 33.627185\n[138,] 42.14336  8.487731 75.79899 33.655627\n[139,] 42.16153  8.477955 75.84511 33.683575\n[140,] 42.17995  8.468913 75.89100 33.711041\n[141,] 42.19862  8.460588 75.93666 33.738034\n[142,] 42.21753  8.452963 75.98209 33.764565\n[143,] 42.23667  8.446025 76.02731 33.790642\n[144,] 42.25603  8.439756 76.07231 33.816277\n[145,] 42.27562  8.434141 76.11710 33.841478\n[146,] 42.29542  8.429167 76.16167 33.866254\n[147,] 42.31543  8.424818 76.20605 33.890614\n[148,] 42.33565  8.421080 76.25022 33.914568\n[149,] 42.35606  8.417940 76.29419 33.938124\n[150,] 42.37667  8.415382 76.33796 33.961289\n[151,] 42.39747  8.413395 76.38154 33.984073\n[152,] 42.41845  8.411964 76.42493 34.006483\n[153,] 42.43960  8.411078 76.46813 34.028527\n[154,] 42.46093  8.410722 76.51115 34.050213\n[155,] 42.48243  8.410885 76.55398 34.071548\n[156,] 42.50409  8.411555 76.59663 34.092539\n[157,] 42.52591  8.412718 76.63911 34.113194\n[158,] 42.54789  8.414365 76.68141 34.133520\n[159,] 42.57001  8.416483 76.72353 34.153523\n[160,] 42.59227  8.419061 76.76548 34.173211\n[161,] 42.61468  8.422088 76.80727 34.192589\n[162,] 42.63722  8.425553 76.84888 34.211664\n[163,] 42.65989  8.429445 76.89033 34.230442\n[164,] 42.68268  8.433754 76.93161 34.248929\n[165,] 42.70560  8.438470 76.97273 34.267132\n[166,] 42.72864  8.443583 77.01369 34.285056\n[167,] 42.75179  8.449082 77.05449 34.302706\n[168,] 42.77505  8.454958 77.09514 34.320089\n[169,] 42.79841  8.461203 77.13562 34.337210\n[170,] 42.82188  8.467805 77.17595 34.354073\n[171,] 42.84544  8.474757 77.21613 34.370685\n[172,] 42.86910  8.482050 77.25615 34.387050\n[173,] 42.89285  8.489674 77.29602 34.403174\n[174,] 42.91668  8.497621 77.33574 34.419060\n[175,] 42.94060  8.505883 77.37531 34.434715\n[176,] 42.96459  8.514451 77.41473 34.450141\n[177,] 42.98866  8.523318 77.45401 34.465345\n[178,] 43.01281  8.532475 77.49314 34.480330\n[179,] 43.03702  8.541915 77.53212 34.495101\n[180,] 43.06129  8.551630 77.57095 34.509662\n[181,] 43.08563  8.561612 77.60964 34.524016\n[182,] 43.11002  8.571855 77.64819 34.538169\n[183,] 43.13447  8.582350 77.68660 34.552124\n[184,] 43.15898  8.593092 77.72486 34.565885\n[185,] 43.18353  8.604073 77.76298 34.579455\n[186,] 43.20812  8.615286 77.80096 34.592838\n[187,] 43.23276  8.626726 77.83880 34.606039\n[188,] 43.25744  8.638384 77.87650 34.619059\n[189,] 43.28216  8.650255 77.91406 34.631904\n[190,] 43.30691  8.662333 77.95148 34.644576\n[191,] 43.33169  8.674612 77.98877 34.657078\n[192,] 43.35650  8.687085 78.02591 34.669413\n[193,] 43.38133  8.699747 78.06292 34.681586\n[194,] 43.40619  8.712592 78.09979 34.693598\n[195,] 43.43107  8.725614 78.13652 34.705453\n[196,] 43.45596  8.738808 78.17312 34.717154\n[197,] 43.48087  8.752168 78.20957 34.728703\n[198,] 43.50579  8.765690 78.24590 34.740104\n[199,] 43.53073  8.779367 78.28208 34.751358\n[200,] 43.55566  8.793195 78.31813 34.762470\n[201,] 43.58061  8.807169 78.35405 34.773440\n[202,] 43.60556  8.821284 78.38983 34.784273\n[203,] 43.63050  8.835535 78.42547 34.794969\n[204,] 43.65545  8.849917 78.46098 34.805533\n[205,] 43.68039  8.864426 78.49636 34.815966\n[206,] 43.70533  8.879058 78.53160 34.826270\n[207,] 43.73025  8.893807 78.56670 34.836447\n[208,] 43.75517  8.908670 78.60167 34.846501\n[209,] 43.78007  8.923642 78.63651 34.856432\n[210,] 43.80496  8.938720 78.67121 34.866244\n[211,] 43.82984  8.953899 78.70577 34.875938\n[212,] 43.85469  8.969174 78.74021 34.885516\n[213,] 43.87952  8.984544 78.77451 34.894981\n[214,] 43.90434  9.000002 78.80867 34.904333\n[215,] 43.92912  9.015547 78.84270 34.913576\n[216,] 43.95388  9.031173 78.87659 34.922710\n[217,] 43.97862  9.046878 78.91035 34.931738\n[218,] 44.00332  9.062658 78.94398 34.940662\n[219,] 44.02799  9.078510 78.97747 34.949482\n[220,] 44.05263  9.094429 79.01083 34.958201\n[221,] 44.07723  9.110414 79.04406 34.966821\n[222,] 44.10180  9.126461 79.07715 34.975342\n[223,] 44.12633  9.142565 79.11010 34.983768\n[224,] 44.15082  9.158726 79.14292 34.992098\n[225,] 44.17527  9.174939 79.17561 35.000335\n[226,] 44.19968  9.191201 79.20816 35.008480\n[227,] 44.22404  9.207510 79.24058 35.016534\n[228,] 44.24836  9.223863 79.27286 35.024500\n[229,] 44.27263  9.240257 79.30501 35.032377\n[230,] 44.29686  9.256689 79.33703 35.040168\n[231,] 44.32103  9.273156 79.36891 35.047874\n[232,] 44.34515  9.289657 79.40065 35.055497\n[233,] 44.36922  9.306188 79.43226 35.063036\n[234,] 44.39324  9.322748 79.46374 35.070494\n[235,] 44.41721  9.339333 79.49508 35.077873\n[236,] 44.44111  9.355941 79.52628 35.085172\n[237,] 44.46496  9.372570 79.55736 35.092393\n[238,] 44.48875  9.389218 79.58829 35.099537\n[239,] 44.51249  9.405882 79.61909 35.106606\n[240,] 44.53616  9.422560 79.64976 35.113600\n[241,] 44.55977  9.439251 79.68029 35.120521\n[242,] 44.58332  9.455951 79.71069 35.127369\n[243,] 44.60681  9.472660 79.74095 35.134145\n[244,] 44.63023  9.489374 79.77108 35.140852\n[245,] 44.65358  9.506093 79.80107 35.147488\n[246,] 44.67687  9.522814 79.83093 35.154056\n[247,] 44.70009  9.539535 79.86065 35.160556\n[248,] 44.72324  9.556255 79.89023 35.166989\n[249,] 44.74633  9.572972 79.91968 35.173356\n[250,] 44.76934  9.589684 79.94900 35.179659\n[251,] 44.79229  9.606389 79.97818 35.185896\n[252,] 44.81516  9.623086 80.00723 35.192071\n[253,] 44.83796  9.639772 80.03614 35.198183\n[254,] 44.86068  9.656448 80.06491 35.204233\n[255,] 44.88333  9.673110 80.09355 35.210222\n[256,] 44.90591  9.689758 80.12206 35.216151\n[257,] 44.92841  9.706390 80.15043 35.222020\n[258,] 44.95084  9.723005 80.17867 35.227830\n[259,] 44.97318  9.739601 80.20677 35.233582\n[260,] 44.99545  9.756176 80.23473 35.239277\n[261,] 45.01765  9.772731 80.26256 35.244915\n[262,] 45.03976  9.789262 80.29026 35.250497\n[263,] 45.06179  9.805769 80.31782 35.256024\n[264,] 45.08375  9.822252 80.34524 35.261496\n[265,] 45.10562  9.838707 80.37254 35.266914\n[266,] 45.12741  9.855135 80.39969 35.272278\n[267,] 45.14912  9.871534 80.42671 35.277590\n[268,] 45.17075  9.887904 80.45360 35.282849\n[269,] 45.19230  9.904242 80.48036 35.288057\n[270,] 45.21376  9.920548 80.50697 35.293213\n[271,] 45.23514  9.936822 80.53346 35.298319\n[272,] 45.25644  9.953061 80.55981 35.303375\n[273,] 45.27765  9.969265 80.58603 35.308382\n[274,] 45.29877  9.985433 80.61211 35.313339\n[275,] 45.31981 10.001564 80.63806 35.318248\n[276,] 45.34077 10.017657 80.66388 35.323110\n[277,] 45.36164 10.033712 80.68956 35.327924\n[278,] 45.38242 10.049727 80.71511 35.332691\n[279,] 45.40311 10.065702 80.74053 35.337412\n[280,] 45.42372 10.081635 80.76581 35.342087\n[281,] 45.44424 10.097526 80.79096 35.346716\n[282,] 45.46468 10.113375 80.81598 35.351301\n[283,] 45.48502 10.129180 80.84086 35.355841\n[284,] 45.50528 10.144940 80.86561 35.360337\n[285,] 45.52545 10.160656 80.89023 35.364789\n[286,] 45.54552 10.176326 80.91472 35.369199\n[287,] 45.56552 10.191950 80.93908 35.373565\n[288,] 45.58542 10.207527 80.96331 35.377889\n[289,] 45.60523 10.223056 80.98740 35.382172\n[290,] 45.62495 10.238537 81.01136 35.386413\n[291,] 45.64458 10.253970 81.03519 35.390612\n[292,] 45.66412 10.269353 81.05890 35.394771\n[293,] 45.68358 10.284686 81.08247 35.398890\n[294,] 45.70294 10.299969 81.10591 35.402969\n[295,] 45.72221 10.315200 81.12922 35.407009\n[296,] 45.74139 10.330380 81.15240 35.411009\n[297,] 45.76048 10.345509 81.17545 35.414970\n[298,] 45.77948 10.360585 81.19837 35.418893\n[299,] 45.79839 10.375608 81.22116 35.422778\n[300,] 45.81720 10.390578 81.24383 35.426625\n[301,] 45.83593 10.405494 81.26636 35.430435\n[302,] 45.85456 10.420355 81.28877 35.434208\n[303,] 45.87311 10.435163 81.31105 35.437944\n[304,] 45.89156 10.449915 81.33320 35.441644\n[305,] 45.90992 10.464612 81.35523 35.445308\n[306,] 45.92819 10.479254 81.37713 35.448936\n[307,] 45.94637 10.493839 81.39890 35.452528\n[308,] 45.96445 10.508369 81.42054 35.456086\n[309,] 45.98245 10.522841 81.44206 35.459609\n[310,] 46.00035 10.537257 81.46345 35.463097\n[311,] 46.01817 10.551615 81.48472 35.466552\n[312,] 46.03589 10.565916 81.50586 35.469972\n[313,] 46.05352 10.580160 81.52688 35.473359\n[314,] 46.07106 10.594345 81.54777 35.476713\n[315,] 46.08851 10.608472 81.56854 35.480034\n[316,] 46.10586 10.622540 81.58918 35.483322\n[317,] 46.12313 10.636550 81.60971 35.486577\n[318,] 46.14030 10.650501 81.63010 35.489801\n[319,] 46.15739 10.664393 81.65038 35.492993\n[320,] 46.17438 10.678225 81.67053 35.496153\n[321,] 46.19128 10.691998 81.69056 35.499282\n[322,] 46.20809 10.705711 81.71047 35.502380\n[323,] 46.22481 10.719365 81.73026 35.505448\n[324,] 46.24144 10.732958 81.74993 35.508485\n[325,] 46.25798 10.746492 81.76947 35.511491\n[326,] 46.27443 10.759965 81.78890 35.514468\n[327,] 46.29079 10.773378 81.80821 35.517415\n[328,] 46.30706 10.786730 81.82740 35.520333\n[329,] 46.32324 10.800022 81.84646 35.523221\n[330,] 46.33933 10.813253 81.86541 35.526081\n[331,] 46.35534 10.826423 81.88425 35.528912\n[332,] 46.37125 10.839533 81.90296 35.531714\n[333,] 46.38707 10.852581 81.92156 35.534489\n[334,] 46.40280 10.865569 81.94004 35.537235\n[335,] 46.41845 10.878495 81.95840 35.539953\n[336,] 46.43400 10.891360 81.97665 35.542645\n[337,] 46.44947 10.904164 81.99478 35.545308\n[338,] 46.46485 10.916907 82.01280 35.547945\n[339,] 46.48014 10.929589 82.03070 35.550555\n[340,] 46.49535 10.942210 82.04849 35.553139\n[341,] 46.51046 10.954769 82.06616 35.555696\n[342,] 46.52549 10.967267 82.08372 35.558227\n[343,] 46.54044 10.979703 82.10117 35.560732\n[344,] 46.55529 10.992079 82.11850 35.563212\n[345,] 46.57006 11.004393 82.13572 35.565666\n[346,] 46.58474 11.016646 82.15284 35.568095\n[347,] 46.59934 11.028837 82.16983 35.570499\n[348,] 46.61385 11.040968 82.18672 35.572878\n[349,] 46.62827 11.053037 82.20350 35.575232\n[350,] 46.64261 11.065045 82.22017 35.577563\n[351,] 46.65686 11.076992 82.23673 35.579868\n[352,] 46.67103 11.088878 82.25318 35.582150\n[353,] 46.68511 11.100703 82.26952 35.584409\n[354,] 46.69911 11.112467 82.28575 35.586643\n[355,] 46.71302 11.124170 82.30188 35.588855\n[356,] 46.72685 11.135812 82.31790 35.591043\n[357,] 46.74060 11.147393 82.33381 35.593208\n[358,] 46.75426 11.158914 82.34962 35.595351\n[359,] 46.76784 11.170374 82.36532 35.597471\n[360,] 46.78134 11.181774 82.38091 35.599568\n[361,] 46.79476 11.193113 82.39640 35.601644\n[362,] 46.80809 11.204392 82.41179 35.603697\n[363,] 46.82134 11.215611 82.42707 35.605729\n[364,] 46.83451 11.226770 82.44225 35.607739\n[365,] 46.84760 11.237868 82.45732 35.609727\n\n\nCode\nfanchart(fit.pr, mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. How the lifestyle influences the mortality rates of major diseases?\nLifestyle factors such as smoking prevalence and alcohol consumption are significant determinants of mortality rates from major diseases like cancer, heart disease, and diabetes. Smoking is a well-known risk factor for many types of cancer and cardiovascular diseases, and it exacerbates the complications associated with diabetes. Similarly, excessive alcohol consumption is linked to a range of health issues, including liver diseases, cardiovascular problems, and certain cancers. A VAR model incorporating these lifestyle variables could reveal the extent to which changes in smoking and drinking behaviors might predict fluctuations in mortality rates from these diseases, reflecting the impact of public health interventions and societal shifts in behavior.\n(VAR) Mortality rates of major disease(Cancer, Heart Disease, Diabetes) ~ Smoking Prevalance + Alcohol Consumption\n\nData VisualizationVARselectInitial Model SelectionCross-ValidationModel Creation and DiagnosticsForecasting\n\n\nThe top panel shows a declining trend in total deaths of major diseases, suggesting improvements in healthcare or effective disease prevention and management strategies over time.The middle panel indicates a steady decline in adult smoking rates, which historically correlates with public health initiatives and smoking cessation programs. This trend is consistent with the falling mortality rates, supporting the view that reduced smoking prevalence can lead to lower mortality from smoking-related diseases. The bottom panel shows alcohol consumption, which seems relatively stable with slight fluctuations. The stability in alcohol consumption, juxtaposed against declining mortality rates, might imply that alcohol may not be as significant a factor in the total mortality rate from these major diseases as smoking, or it may suggest that the effects of alcohol consumption on mortality may be more complex and influenced by other factors such as drinking patterns or socioeconomic status.\n\n\nCode\nlifestyle_mortality_ts &lt;- ts(lifestyle_mortality[,c(5:7)], start=c(1970), end=c(2019), frequency=1)\n\nautoplot(lifestyle_mortality_ts[,c(1:3)], facets=TRUE, color=\"#27aeef\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Variables Influencing the Total Mortality Rates of Major Diseases\")\n\n\n\n\n\n\n\n\n\n\n\nBased on the VARselect function, the optimal p for VAR(p) is 6.\n\n\nCode\nVARselect(lifestyle_mortality_ts, lag.max=6, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      6      6      6 \n\n$criteria\n               1        2        3         4          5          6\nAIC(n)  4.021194 1.537451 1.349325 0.6786107 -0.4376741 -1.2718053\nHQ(n)   4.246761 1.898358 1.845573 1.3101986  0.3292540 -0.3695368\nSC(n)   4.629440 2.510645 2.687467 2.3817008  1.6303638  1.1611806\nFPE(n) 55.932829 4.710134 3.982071 2.1112004  0.7329008  0.3476392\n\n\n\n\nWe run the VAR() model with p = 6.\n\n\nCode\nsummary(fitvar3&lt;-vars::VAR(lifestyle_mortality_ts, p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Deaths, Num_Adult_Smokers, Alcohol_Consumption \nDeterministic variables: both \nSample size: 44 \nLog Likelihood: -99.32 \nRoots of the characteristic polynomial:\n0.9935 0.9935 0.9883 0.9883 0.9373 0.9373 0.9347 0.9347 0.8843 0.8843 0.8782 0.8755 0.8755 0.8695 0.8695 0.8461 0.8461  0.26\nCall:\nvars::VAR(y = lifestyle_mortality_ts, p = 6, type = \"both\")\n\n\nEstimation results for equation Total_Deaths: \n============================================= \nTotal_Deaths = Total_Deaths.l1 + Num_Adult_Smokers.l1 + Alcohol_Consumption.l1 + Total_Deaths.l2 + Num_Adult_Smokers.l2 + Alcohol_Consumption.l2 + Total_Deaths.l3 + Num_Adult_Smokers.l3 + Alcohol_Consumption.l3 + Total_Deaths.l4 + Num_Adult_Smokers.l4 + Alcohol_Consumption.l4 + Total_Deaths.l5 + Num_Adult_Smokers.l5 + Alcohol_Consumption.l5 + Total_Deaths.l6 + Num_Adult_Smokers.l6 + Alcohol_Consumption.l6 + const + trend \n\n                         Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Deaths.l1        -6.446e-01  5.319e-01  -1.212  0.23736    \nNum_Adult_Smokers.l1    2.409e+01  1.124e+01   2.143  0.04242 *  \nAlcohol_Consumption.l1  1.342e+03  8.250e+02   1.627  0.11678    \nTotal_Deaths.l2        -3.610e-01  4.218e-01  -0.856  0.40043    \nNum_Adult_Smokers.l2   -1.993e+01  8.816e+00  -2.260  0.03314 *  \nAlcohol_Consumption.l2 -2.566e+03  1.199e+03  -2.141  0.04266 *  \nTotal_Deaths.l3         2.425e+00  1.457e+00   1.664  0.10905    \nNum_Adult_Smokers.l3    7.741e+00  1.054e+01   0.734  0.46979    \nAlcohol_Consumption.l3  9.710e+02  8.584e+02   1.131  0.26919    \nTotal_Deaths.l4         5.088e-03  8.193e-01   0.006  0.99510    \nNum_Adult_Smokers.l4   -5.067e+01  1.188e+01  -4.264  0.00027 ***\nAlcohol_Consumption.l4  1.800e+02  6.809e+02   0.264  0.79374    \nTotal_Deaths.l5        -1.127e+00  1.106e+00  -1.019  0.31852    \nNum_Adult_Smokers.l5    2.687e+01  1.497e+01   1.794  0.08537 .  \nAlcohol_Consumption.l5 -5.069e+02  4.427e+02  -1.145  0.26351    \nTotal_Deaths.l6         3.191e-01  7.187e-01   0.444  0.66098    \nNum_Adult_Smokers.l6   -9.528e+00  1.061e+01  -0.898  0.37826    \nAlcohol_Consumption.l6  3.972e+02  2.094e+02   1.897  0.06997 .  \nconst                   1.532e+03  2.178e+03   0.703  0.48857    \ntrend                  -2.570e-01  5.959e-01  -0.431  0.67012    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 46.1 on 24 degrees of freedom\nMultiple R-Squared: 0.827,  Adjusted R-squared:  0.69 \nF-statistic: 6.037 on 19 and 24 DF,  p-value: 3.204e-05 \n\n\nEstimation results for equation Num_Adult_Smokers: \n================================================== \nNum_Adult_Smokers = Total_Deaths.l1 + Num_Adult_Smokers.l1 + Alcohol_Consumption.l1 + Total_Deaths.l2 + Num_Adult_Smokers.l2 + Alcohol_Consumption.l2 + Total_Deaths.l3 + Num_Adult_Smokers.l3 + Alcohol_Consumption.l3 + Total_Deaths.l4 + Num_Adult_Smokers.l4 + Alcohol_Consumption.l4 + Total_Deaths.l5 + Num_Adult_Smokers.l5 + Alcohol_Consumption.l5 + Total_Deaths.l6 + Num_Adult_Smokers.l6 + Alcohol_Consumption.l6 + const + trend \n\n                         Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Deaths.l1         -0.014574   0.018253  -0.798 0.432461    \nNum_Adult_Smokers.l1     0.821901   0.385720   2.131 0.043549 *  \nAlcohol_Consumption.l1  70.999042  28.311290   2.508 0.019316 *  \nTotal_Deaths.l2         -0.067179   0.014474  -4.641 0.000103 ***\nNum_Adult_Smokers.l2    -0.159797   0.302534  -0.528 0.602213    \nAlcohol_Consumption.l2 -62.389930  41.132719  -1.517 0.142381    \nTotal_Deaths.l3          0.047173   0.049994   0.944 0.354788    \nNum_Adult_Smokers.l3     1.248181   0.361712   3.451 0.002081 ** \nAlcohol_Consumption.l3  30.909512  29.459138   1.049 0.304522    \nTotal_Deaths.l4          0.037187   0.028116   1.323 0.198421    \nNum_Adult_Smokers.l4    -1.932315   0.407786  -4.739 8.07e-05 ***\nAlcohol_Consumption.l4 -22.573892  23.365545  -0.966 0.343620    \nTotal_Deaths.l5          0.016786   0.037953   0.442 0.662237    \nNum_Adult_Smokers.l5     0.524944   0.513849   1.022 0.317162    \nAlcohol_Consumption.l5  -8.083185  15.193102  -0.532 0.599596    \nTotal_Deaths.l6          0.019777   0.024663   0.802 0.430495    \nNum_Adult_Smokers.l6    -1.058650   0.364242  -2.906 0.007742 ** \nAlcohol_Consumption.l6   4.951450   7.187283   0.689 0.497481    \nconst                   19.545547  74.756063   0.261 0.795970    \ntrend                   -0.008017   0.020451  -0.392 0.698495    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.582 on 24 degrees of freedom\nMultiple R-Squared: 0.923,  Adjusted R-squared: 0.8621 \nF-statistic: 15.15 on 19 and 24 DF,  p-value: 4.496e-09 \n\n\nEstimation results for equation Alcohol_Consumption: \n==================================================== \nAlcohol_Consumption = Total_Deaths.l1 + Num_Adult_Smokers.l1 + Alcohol_Consumption.l1 + Total_Deaths.l2 + Num_Adult_Smokers.l2 + Alcohol_Consumption.l2 + Total_Deaths.l3 + Num_Adult_Smokers.l3 + Alcohol_Consumption.l3 + Total_Deaths.l4 + Num_Adult_Smokers.l4 + Alcohol_Consumption.l4 + Total_Deaths.l5 + Num_Adult_Smokers.l5 + Alcohol_Consumption.l5 + Total_Deaths.l6 + Num_Adult_Smokers.l6 + Alcohol_Consumption.l6 + const + trend \n\n                         Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Deaths.l1         2.695e-04  2.058e-04   1.309 0.202786    \nNum_Adult_Smokers.l1    4.560e-03  4.349e-03   1.048 0.304903    \nAlcohol_Consumption.l1  1.025e+00  3.192e-01   3.212 0.003727 ** \nTotal_Deaths.l2        -1.159e-03  1.632e-04  -7.103 2.42e-07 ***\nNum_Adult_Smokers.l2   -1.840e-03  3.411e-03  -0.539 0.594664    \nAlcohol_Consumption.l2 -8.813e-01  4.638e-01  -1.900 0.069477 .  \nTotal_Deaths.l3         4.231e-04  5.637e-04   0.750 0.460253    \nNum_Adult_Smokers.l3    3.165e-03  4.079e-03   0.776 0.445357    \nAlcohol_Consumption.l3 -3.112e-01  3.322e-01  -0.937 0.358160    \nTotal_Deaths.l4         6.835e-04  3.170e-04   2.156 0.041329 *  \nNum_Adult_Smokers.l4   -2.001e-02  4.598e-03  -4.352 0.000216 ***\nAlcohol_Consumption.l4  3.231e-01  2.635e-01   1.226 0.231972    \nTotal_Deaths.l5        -6.804e-04  4.280e-04  -1.590 0.124925    \nNum_Adult_Smokers.l5   -3.243e-04  5.794e-03  -0.056 0.955826    \nAlcohol_Consumption.l5 -1.505e-01  1.713e-01  -0.879 0.388369    \nTotal_Deaths.l6        -1.036e-04  2.781e-04  -0.373 0.712768    \nNum_Adult_Smokers.l6   -1.395e-03  4.107e-03  -0.340 0.737027    \nAlcohol_Consumption.l6  4.935e-02  8.104e-02   0.609 0.548273    \nconst                   3.121e+00  8.429e-01   3.703 0.001112 ** \ntrend                  -6.576e-05  2.306e-04  -0.285 0.777959    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.01784 on 24 degrees of freedom\nMultiple R-Squared: 0.984,  Adjusted R-squared: 0.9714 \nF-statistic: 77.82 on 19 and 24 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n                    Total_Deaths Num_Adult_Smokers Alcohol_Consumption\nTotal_Deaths           2124.9081          64.60675           0.5627289\nNum_Adult_Smokers        64.6067           2.50256           0.0109549\nAlcohol_Consumption       0.5627           0.01095           0.0003182\n\nCorrelation matrix of residuals:\n                    Total_Deaths Num_Adult_Smokers Alcohol_Consumption\nTotal_Deaths              1.0000            0.8860              0.6844\nNum_Adult_Smokers         0.8860            1.0000              0.3882\nAlcohol_Consumption       0.6844            0.3882              1.0000\n\n\n\n\nWe use cross-validation to compare two models: VAR(6) and VAR(5). And the best model which is selected by the cross-validation process is VAR(6).\n\n\nCode\n## Cross-validation\n\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(lifestyle_mortality_ts)/folds)\n\nyr = rep(c(1970:2019),each =1) #year\n\nrmse1 = data.frame()\nrmse2 = data.frame()\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- lifestyle_mortality_ts[-(start:end), ]\n  test_model &lt;- lifestyle_mortality_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 6, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  # Define the fit models\n  fit &lt;- vars::VAR(train_model, p=6, type= \"both\", season = NULL, exog = NULL)\n  fit2 &lt;- vars::VAR(train_model, p=5, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  \n  # Define the prediction\n  pred &lt;- predict(fit, n.ahead = h)\n  pred2 &lt;- predict(fit2, n.ahead = h)\n  \n  pred_deaths &lt;- pred$fcst$Total_Deaths[,1]\n  pred_deaths2 &lt;- pred2$fcst$Total_Deaths[,1]\n  \n  mse &lt;- mean((pred_deaths - test_model[, \"Total_Deaths\"])^2)\n  mse2 &lt;- mean((pred_deaths2 - test_model[, \"Total_Deaths\"])^2)\n  \n  rmse1 &lt;- rbind(rmse1, data.frame(Total_Deaths = sqrt(mse)))\n  rmse2 &lt;- rbind(rmse2, data.frame(Total_Deaths = sqrt(mse2)))\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  } else if(mse2 &lt; best_performance){\n    best_model &lt;- fit2\n    best_performance &lt;- mse2\n  }\n\n}\n\n# Plot the RMSE of two models\n\nplot(rmse1$Total_Deaths, type = \"l\", col = \"blue\", xlab = \"Fold\", ylab = \"RMSE\", main = \"RMSE of VAR(6) and VAR(5)\")\nlines(rmse2$Total_Deaths, col = \"red\")\nlegend(\"topright\", legend = c(\"VAR(6)\", \"VAR(5)\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\n\n\nThe models VAR(6) and VAR(5) are created. The serial correlation test is performed on both models. The results show that the VAR(6) model has a lower p-value, indicating that the residuals are not serially correlated. Based on the p-value and ACF plots, the VAR(6) model is selected as the final model.\n\n\nCode\nvar_model_3 &lt;- vars::VAR(lifestyle_mortality_ts, p=6, type= \"both\", season = NULL, exog = NULL)\nlm.serial &lt;- serial.test(var_model_3, lags.pt = 12, type = \"PT.asymptotic\") \nlm.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_3\nChi-squared = 181.05, df = 54, p-value = 1.221e-15\n\n\nCode\nplot(lm.serial, names = \"Total_Deaths\", mar = c(2,2,2,2)) \n\n\n\n\n\n\n\n\n\nCode\nplot(lm.serial, names = \"Num_Adult_Smokers\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(lm.serial, names = \"Alcohol_Consumption\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nvar_model_4 &lt;- vars::VAR(lifestyle_mortality_ts, p=5, type= \"both\", season = NULL, exog = NULL)\nlm.serial2 &lt;- serial.test(var_model_4, lags.pt = 12, type = \"PT.asymptotic\")\nlm.serial2\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_4\nChi-squared = 164.16, df = 63, p-value = 5.846e-11\n\n\nCode\nplot(lm.serial2, names = \"Total_Deaths\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(lm.serial2, names = \"Num_Adult_Smokers\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\nplot(lm.serial2, names = \"Alcohol_Consumption\", mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\n\n\nTotal Major Diseases Deaths show a first increasing then decreasing trend, indicating an expectation of decline but with some uncertainty. The Number of Adult Smokers shows an increase trend, then decrease. The Alcohol Consumption shows a small increase, and then a huge decrease.\n\n\nCode\n## Forecasting\n\n(fit.pr = predict(fitvar3, n.ahead = 5, ci = 0.95))\n\n\n$Total_Deaths\n         fcst    lower    upper        CI\n[1,] 584.4412 494.0932 674.7891  90.34792\n[2,] 570.3451 467.3448 673.3455 103.00035\n[3,] 591.2256 483.8303 698.6208 107.39527\n[4,] 595.1272 471.2067 719.0478 123.92054\n[5,] 504.2740 374.7316 633.8164 129.54244\n\n$Num_Adult_Smokers\n         fcst    lower    upper       CI\n[1,] 44.06137 40.96081 47.16193 3.100558\n[2,] 47.52614 43.22556 51.82671 4.300575\n[3,] 48.32198 43.45892 53.18505 4.863062\n[4,] 51.34189 45.42781 57.25597 5.914080\n[5,] 49.74122 43.66348 55.81896 6.077737\n\n$Alcohol_Consumption\n         fcst    lower    upper         CI\n[1,] 2.486055 2.451094 2.521015 0.03496093\n[2,] 2.658562 2.584408 2.732715 0.07415365\n[3,] 2.507023 2.418532 2.595514 0.08849080\n[4,] 2.335424 2.174391 2.496458 0.16103368\n[5,] 2.249375 2.072146 2.426603 0.17722881\n\n\nCode\nfanchart(fit.pr, mar = c(2,2,2,2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. How the average life expectancy of the US be influenced by the mortality rates of Cancer, Heart Diseases, Diabetes, and infant mortality rates?\nThe average life expectancy in the U.S. could be influenced by the mortality rates of major diseases like Cancer, Heart Diseases, and Diabetes, as well as infant mortality rates, because these are significant indicators of overall population health. If mortality rates from these diseases decrease due to better healthcare, prevention, and treatment, life expectancy is likely to increase. An ARIMAX model that incorporates these mortality rates as exogenous variables would allow for an assessment of how changes in these rates could impact average life expectancy. The inclusion of infant mortality rates accounts for early-life factors, which are crucial determinants of life expectancy trends.\n(ARIMAX) Average Life Expectancy ~ Mortality rates of major disease(Cancer, Heart Diseases, Diabetes) + Infant Mortality rates\n\nData VisualizationAuto ARIMAManual Model FittingBest Models ResidualsCross-ValidationForecasting\n\n\nAs illustrated by the plots, average life expectancy shows an upward trend, which is typically a sign of improving overall health and healthcare in a population. Concurrently, mortality rates from cancer, heart disease, and diabetes are declining, which could be a contributing factor to increasing life expectancy. Similarly, the declining trend in infant mortality rates is another positive indicator, as lower infant mortality is directly related to higher life expectancy.\n\n\nCode\nlife_expectancy_mortality_ts &lt;- ts(life_expectancy_mortality[,c(2:6)], start=c(2000), end=c(2019), frequency=1)\n\nautoplot(life_expectancy_mortality_ts, facets=TRUE, color=\"#27aeef\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Variables Influencing the Average Life Expectancy\")\n\n\n\n\n\n\n\n\n\n\n\nBased on the results we got, we can see the Auto ARIMA function select the model ARMA(2,0). And the residuals are not correlated.\n\n\nCode\nxreg &lt;- cbind(\n  life_expectancy_mortality_ts[, \"Death_Cancer\"],\n  life_expectancy_mortality_ts[, \"Death_Heart_Disease\"],\n  life_expectancy_mortality_ts[, \"Death_Diabetes\"],\n  life_expectancy_mortality_ts[, \"Infant_Mortality_Rates\"]\n)\n\nfit_expectancy_mortality &lt;- auto.arima(life_expectancy_mortality_ts[, \"Life_Expectancy\"], xreg = xreg)\nsummary(fit_expectancy_mortality)\n\n\nSeries: life_expectancy_mortality_ts[, \"Life_Expectancy\"] \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2  intercept\n      1.0801  -0.7378    84.9804\ns.e.  0.1820   0.1521     0.4888\n      life_expectancy_mortality_ts[, \"Death_Cancer\"]\n                                              0.0038\ns.e.                                          0.0060\n      life_expectancy_mortality_ts[, \"Death_Heart_Disease\"]\n                                                    -0.0174\ns.e.                                                 0.0026\n      life_expectancy_mortality_ts[, \"Death_Diabetes\"]\n                                               -0.1160\ns.e.                                            0.0244\n      life_expectancy_mortality_ts[, \"Infant_Mortality_Rates\"]\n                                                       -0.2179\ns.e.                                                    0.1484\n\nsigma^2 = 0.002603:  log likelihood = 34.41\nAIC=-52.82   AICc=-39.73   BIC=-44.85\n\nTraining set error measures:\n                       ME     RMSE        MAE          MPE       MAPE      MASE\nTraining set 0.0002198822 0.041134 0.02996728 0.0002854916 0.03825511 0.2085635\n                   ACF1\nTraining set -0.1195763\n\n\nCode\n# Check residuals\ncheckresiduals(fit_expectancy_mortality)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,0,0) errors\nQ* = 3.3216, df = 3, p-value = 0.3446\n\nModel df: 2.   Total lags used: 5\n\n\n\n\nFirstly, we fit the regression model of Life Expectancy on the mortality rates of major diseases and infant mortality rates. The model shows that the mortality rates of cancer, heart disease, and diabetes are significant predictors of life expectancy. Then we take the residuals of this model and fit several ARIMA models to them. We’ll choose the best ARIMA model based on the AIC and BIC values. As we can see from the results, the best ARIMA models are ARMA(2,2), and ARMA(2,1).\n\n\nCode\n# Change the dataset to time series\nlife_expectancy_mortality$Life_Expectancy &lt;- ts(life_expectancy_mortality$Life_Expectancy, start=c(2000), end=c(2019), frequency=1)\nlife_expectancy_mortality$Death_Cancer &lt;- ts(life_expectancy_mortality$Death_Cancer, start=c(2000), end=c(2019), frequency=1)\nlife_expectancy_mortality$Death_Heart_Disease &lt;- ts(life_expectancy_mortality$Death_Heart_Disease, start=c(2000), end=c(2019), frequency=1)\nlife_expectancy_mortality$Death_Diabetes &lt;- ts(life_expectancy_mortality$Death_Diabetes, start=c(2000), end=c(2019), frequency=1)\nlife_expectancy_mortality$Infant_Mortality_Rates &lt;- ts(life_expectancy_mortality$Infant_Mortality_Rates, start=c(2000), end=c(2019), frequency=1)\n\n############# First fit the linear model##########\nfit.reg_expectancy_mortality &lt;- lm(Life_Expectancy ~ Death_Cancer + Death_Heart_Disease + Death_Diabetes + Infant_Mortality_Rates, data = life_expectancy_mortality)\n\nsummary(fit.reg_expectancy_mortality)\n\n\n\nCall:\nlm(formula = Life_Expectancy ~ Death_Cancer + Death_Heart_Disease + \n    Death_Diabetes + Infant_Mortality_Rates, data = life_expectancy_mortality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16589 -0.03271  0.01128  0.03038  0.13291 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            84.663004   0.621863 136.144  &lt; 2e-16 ***\nDeath_Cancer            0.017860   0.009697   1.842   0.0853 .  \nDeath_Heart_Disease    -0.020004   0.003182  -6.286 1.46e-05 ***\nDeath_Diabetes         -0.072540   0.035171  -2.063   0.0569 .  \nInfant_Mortality_Rates -0.630844   0.270555  -2.332   0.0341 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07763 on 15 degrees of freedom\nMultiple R-squared:  0.9927,    Adjusted R-squared:  0.9908 \nF-statistic: 510.3 on 4 and 15 DF,  p-value: 7.929e-16\n\n\nCode\n## Residual Fit\nres.fit_em &lt;- ts(residuals(fit.reg_expectancy_mortality), star=decimal_date(as.Date(\"2000-01-01\",format = \"%Y-%m-%d\")),frequency = 1)\n\nggAcf(res.fit_em, lag.max = 30) # q=1,2,3\n\n\n\n\n\n\n\n\n\nCode\nggPacf(res.fit_em, lag.max = 30) # p=1,2\n\n\n\n\n\n\n\n\n\nCode\n# We don't need differencing as the data is already stationary\n\n# Manual Fitting the models\nd=0\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*25),nrow=25) # roughly nrow = 5x2 (see below)\n\n\nfor (p in 0:3)# p=0,1,2,3 : 4\n{\n  for(q in 0:3)# q=0,1,2,3 :4\n  {\n    model&lt;- Arima(res.fit_em, order=c(p,d,q),include.drift=TRUE) \n    ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n    i=i+1\n  }\n}\n\noutput= as.data.frame(ls)\nnames(output)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-45.22928\n-42.24209\n-43.72928\n\n\n0\n0\n1\n-50.83552\n-46.85259\n-48.16886\n\n\n0\n0\n2\n-49.30000\n-44.32134\n-45.01429\n\n\n0\n0\n3\n-52.44608\n-46.47169\n-45.98454\n\n\n1\n0\n0\n-47.11440\n-43.13147\n-44.44773\n\n\n1\n0\n1\n-49.05043\n-44.07177\n-44.76472\n\n\n1\n0\n2\n-50.98358\n-45.00918\n-44.52204\n\n\n1\n0\n3\n-50.94297\n-43.97284\n-41.60963\n\n\n2\n0\n0\n-53.40464\n-48.42598\n-49.11892\n\n\n2\n0\n1\n-58.69264\n-52.71825\n-52.23111\n\n\n2\n0\n2\n-59.06576\n-52.09564\n-49.73243\n\n\n2\n0\n3\n-58.88831\n-50.92245\n-45.79740\n\n\n3\n0\n0\n-52.83544\n-46.86104\n-46.37390\n\n\n3\n0\n1\n-56.77333\n-49.80321\n-47.44000\n\n\n3\n0\n2\n-55.37811\n-47.41225\n-42.28720\n\n\n3\n0\n3\n-57.28889\n-48.32730\n-39.28889\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\n#output\noutput[which.min(output$AIC),] \n\n\n   p d q       AIC       BIC      AICc\n11 2 0 2 -59.06576 -52.09564 -49.73243\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q       AIC       BIC      AICc\n10 2 0 1 -58.69264 -52.71825 -52.23111\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q       AIC       BIC      AICc\n10 2 0 1 -58.69264 -52.71825 -52.23111\n\n\n\n\nHere, we do the model diagnostics for the best models we got from the manual fitting. The best models are ARMA(2,2) and ARMA(2,1). From the results, we can say ARMA(2,1) is the better model. Let’s move to the cross-validation step to find which model is better to forecast.\n\n\nCode\ncapture.output(sarima(res.fit_em, 2,0,2)) \n\n\n\n\n\n\n\n\n\n  [1] \"initial  value -2.651140 \"                                 \n  [2] \"iter   2 value -2.892889\"                                  \n  [3] \"iter   3 value -2.906275\"                                  \n  [4] \"iter   4 value -2.937897\"                                  \n  [5] \"iter   5 value -3.007801\"                                  \n  [6] \"iter   6 value -3.044662\"                                  \n  [7] \"iter   7 value -3.141007\"                                  \n  [8] \"iter   8 value -3.157433\"                                  \n  [9] \"iter   9 value -3.184432\"                                  \n [10] \"iter  10 value -3.252894\"                                  \n [11] \"iter  11 value -3.310655\"                                  \n [12] \"iter  12 value -3.318604\"                                  \n [13] \"iter  13 value -3.362692\"                                  \n [14] \"iter  14 value -3.379019\"                                  \n [15] \"iter  15 value -3.383407\"                                  \n [16] \"iter  16 value -3.391760\"                                  \n [17] \"iter  17 value -3.405274\"                                  \n [18] \"iter  17 value -3.405274\"                                  \n [19] \"iter  18 value -3.408156\"                                  \n [20] \"iter  19 value -3.408261\"                                  \n [21] \"iter  20 value -3.409250\"                                  \n [22] \"iter  21 value -3.409495\"                                  \n [23] \"iter  22 value -3.409950\"                                  \n [24] \"iter  23 value -3.410259\"                                  \n [25] \"iter  24 value -3.410551\"                                  \n [26] \"iter  25 value -3.410884\"                                  \n [27] \"iter  26 value -3.411122\"                                  \n [28] \"iter  27 value -3.411460\"                                  \n [29] \"iter  28 value -3.411684\"                                  \n [30] \"iter  29 value -3.412018\"                                  \n [31] \"iter  30 value -3.412241\"                                  \n [32] \"iter  31 value -3.412567\"                                  \n [33] \"iter  32 value -3.412795\"                                  \n [34] \"iter  33 value -3.413112\"                                  \n [35] \"iter  34 value -3.413347\"                                  \n [36] \"iter  35 value -3.413654\"                                  \n [37] \"iter  36 value -3.413898\"                                  \n [38] \"iter  37 value -3.414194\"                                  \n [39] \"iter  38 value -3.414447\"                                  \n [40] \"iter  39 value -3.414733\"                                  \n [41] \"iter  40 value -3.414995\"                                  \n [42] \"iter  41 value -3.415269\"                                  \n [43] \"iter  42 value -3.415541\"                                  \n [44] \"iter  43 value -3.415804\"                                  \n [45] \"iter  44 value -3.416086\"                                  \n [46] \"iter  45 value -3.416337\"                                  \n [47] \"iter  46 value -3.416630\"                                  \n [48] \"iter  47 value -3.416869\"                                  \n [49] \"iter  48 value -3.417172\"                                  \n [50] \"iter  49 value -3.417398\"                                  \n [51] \"iter  50 value -3.417713\"                                  \n [52] \"iter  51 value -3.417926\"                                  \n [53] \"iter  52 value -3.418252\"                                  \n [54] \"iter  53 value -3.418453\"                                  \n [55] \"iter  54 value -3.418790\"                                  \n [56] \"iter  55 value -3.418977\"                                  \n [57] \"iter  56 value -3.419326\"                                  \n [58] \"iter  57 value -3.419500\"                                  \n [59] \"iter  58 value -3.419861\"                                  \n [60] \"iter  59 value -3.420021\"                                  \n [61] \"iter  60 value -3.420394\"                                  \n [62] \"iter  61 value -3.420540\"                                  \n [63] \"iter  62 value -3.420926\"                                  \n [64] \"iter  63 value -3.421057\"                                  \n [65] \"iter  64 value -3.421457\"                                  \n [66] \"iter  65 value -3.421573\"                                  \n [67] \"iter  66 value -3.421986\"                                  \n [68] \"iter  67 value -3.422087\"                                  \n [69] \"iter  68 value -3.422514\"                                  \n [70] \"iter  69 value -3.422599\"                                  \n [71] \"iter  70 value -3.423040\"                                  \n [72] \"iter  71 value -3.423109\"                                  \n [73] \"iter  72 value -3.423565\"                                  \n [74] \"iter  73 value -3.423618\"                                  \n [75] \"iter  74 value -3.424088\"                                  \n [76] \"iter  75 value -3.424124\"                                  \n [77] \"iter  76 value -3.424609\"                                  \n [78] \"iter  77 value -3.424629\"                                  \n [79] \"iter  78 value -3.425129\"                                  \n [80] \"iter  79 value -3.425132\"                                  \n [81] \"iter  80 value -3.425648\"                                  \n [82] \"iter  81 value -3.425718\"                                  \n [83] \"iter  82 value -3.425755\"                                  \n [84] \"iter  83 value -3.425771\"                                  \n [85] \"iter  84 value -3.426276\"                                  \n [86] \"iter  85 value -3.426347\"                                  \n [87] \"iter  86 value -3.426381\"                                  \n [88] \"iter  87 value -3.426462\"                                  \n [89] \"iter  88 value -3.426486\"                                  \n [90] \"iter  89 value -3.426568\"                                  \n [91] \"iter  90 value -3.426592\"                                  \n [92] \"iter  91 value -3.426673\"                                  \n [93] \"iter  92 value -3.426698\"                                  \n [94] \"iter  93 value -3.426779\"                                  \n [95] \"iter  94 value -3.426804\"                                  \n [96] \"iter  95 value -3.426884\"                                  \n [97] \"iter  96 value -3.426909\"                                  \n [98] \"iter  97 value -3.426990\"                                  \n [99] \"iter  98 value -3.427015\"                                  \n[100] \"iter  99 value -3.427095\"                                  \n[101] \"iter 100 value -3.427120\"                                  \n[102] \"final  value -3.427120 \"                                   \n[103] \"stopped after 100 iterations\"                              \n[104] \"initial  value -2.699617 \"                                 \n[105] \"iter   2 value -2.911025\"                                  \n[106] \"iter   3 value -2.942243\"                                  \n[107] \"iter   4 value -2.953901\"                                  \n[108] \"iter   5 value -3.013882\"                                  \n[109] \"iter   6 value -3.055169\"                                  \n[110] \"iter   7 value -3.062883\"                                  \n[111] \"iter   8 value -3.116825\"                                  \n[112] \"iter   9 value -3.124528\"                                  \n[113] \"iter  10 value -3.158764\"                                  \n[114] \"iter  11 value -3.169523\"                                  \n[115] \"iter  12 value -3.179768\"                                  \n[116] \"iter  13 value -3.179821\"                                  \n[117] \"iter  14 value -3.180894\"                                  \n[118] \"iter  15 value -3.183756\"                                  \n[119] \"iter  16 value -3.184007\"                                  \n[120] \"iter  17 value -3.185137\"                                  \n[121] \"iter  18 value -3.186624\"                                  \n[122] \"iter  19 value -3.187004\"                                  \n[123] \"iter  20 value -3.187024\"                                  \n[124] \"iter  21 value -3.187026\"                                  \n[125] \"iter  21 value -3.187026\"                                  \n[126] \"iter  21 value -3.187026\"                                  \n[127] \"final  value -3.187026 \"                                   \n[128] \"converged\"                                                 \n[129] \"&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\"                              \n[130] \" \"                                                         \n[131] \"Coefficients: \"                                            \n[132] \"      Estimate     SE t.value p.value\"                     \n[133] \"ar1     1.2162 0.2088  5.8237  0.0000\"                     \n[134] \"ar2    -0.8495 0.1514 -5.6129  0.0000\"                     \n[135] \"ma1    -1.1512 0.5411 -2.1277  0.0504\"                     \n[136] \"ma2     0.1512 0.5189  0.2915  0.7747\"                     \n[137] \"xmean   0.0007 0.0021  0.3500  0.7312\"                     \n[138] \"\"                                                          \n[139] \"sigma^2 estimated as 0.001317917 on 15 degrees of freedom \"\n[140] \" \"                                                         \n[141] \"AIC = -2.936175  AICc = -2.72189  BIC = -2.637456 \"        \n[142] \" \"                                                         \n\n\nCode\ncapture.output(sarima(res.fit_em, 2,0,1))\n\n\n\n\n\n\n\n\n\n  [1] \"initial  value -2.651140 \"                                 \n  [2] \"iter   2 value -2.905712\"                                  \n  [3] \"iter   3 value -2.915018\"                                  \n  [4] \"iter   4 value -2.932119\"                                  \n  [5] \"iter   5 value -2.958665\"                                  \n  [6] \"iter   6 value -2.986696\"                                  \n  [7] \"iter   7 value -3.018482\"                                  \n  [8] \"iter   8 value -3.083446\"                                  \n  [9] \"iter   9 value -3.094075\"                                  \n [10] \"iter  10 value -3.097043\"                                  \n [11] \"iter  11 value -3.097922\"                                  \n [12] \"iter  12 value -3.234201\"                                  \n [13] \"iter  13 value -3.299118\"                                  \n [14] \"iter  14 value -3.321504\"                                  \n [15] \"iter  15 value -3.350087\"                                  \n [16] \"iter  16 value -3.361102\"                                  \n [17] \"iter  17 value -3.374836\"                                  \n [18] \"iter  18 value -3.377059\"                                  \n [19] \"iter  19 value -3.380334\"                                  \n [20] \"iter  20 value -3.383157\"                                  \n [21] \"iter  21 value -3.383197\"                                  \n [22] \"iter  22 value -3.399364\"                                  \n [23] \"iter  23 value -3.402564\"                                  \n [24] \"iter  23 value -3.402564\"                                  \n [25] \"iter  24 value -3.414677\"                                  \n [26] \"iter  24 value -3.414677\"                                  \n [27] \"iter  25 value -3.415728\"                                  \n [28] \"iter  26 value -3.415788\"                                  \n [29] \"iter  27 value -3.416127\"                                  \n [30] \"iter  28 value -3.416258\"                                  \n [31] \"iter  29 value -3.416470\"                                  \n [32] \"iter  30 value -3.416618\"                                  \n [33] \"iter  31 value -3.416804\"                                  \n [34] \"iter  32 value -3.416953\"                                  \n [35] \"iter  33 value -3.417136\"                                  \n [36] \"iter  34 value -3.417282\"                                  \n [37] \"iter  35 value -3.417467\"                                  \n [38] \"iter  36 value -3.417610\"                                  \n [39] \"iter  37 value -3.417798\"                                  \n [40] \"iter  38 value -3.417936\"                                  \n [41] \"iter  39 value -3.418129\"                                  \n [42] \"iter  40 value -3.418263\"                                  \n [43] \"iter  41 value -3.418460\"                                  \n [44] \"iter  42 value -3.418589\"                                  \n [45] \"iter  43 value -3.418790\"                                  \n [46] \"iter  44 value -3.418915\"                                  \n [47] \"iter  45 value -3.419121\"                                  \n [48] \"iter  46 value -3.419241\"                                  \n [49] \"iter  47 value -3.419451\"                                  \n [50] \"iter  48 value -3.419566\"                                  \n [51] \"iter  49 value -3.419781\"                                  \n [52] \"iter  50 value -3.419891\"                                  \n [53] \"iter  51 value -3.420111\"                                  \n [54] \"iter  52 value -3.420216\"                                  \n [55] \"iter  53 value -3.420441\"                                  \n [56] \"iter  54 value -3.420541\"                                  \n [57] \"iter  55 value -3.420771\"                                  \n [58] \"iter  56 value -3.420865\"                                  \n [59] \"iter  57 value -3.421100\"                                  \n [60] \"iter  58 value -3.421189\"                                  \n [61] \"iter  59 value -3.421429\"                                  \n [62] \"iter  60 value -3.421513\"                                  \n [63] \"iter  61 value -3.421758\"                                  \n [64] \"iter  62 value -3.421836\"                                  \n [65] \"iter  63 value -3.422087\"                                  \n [66] \"iter  64 value -3.422160\"                                  \n [67] \"iter  65 value -3.422415\"                                  \n [68] \"iter  66 value -3.422483\"                                  \n [69] \"iter  67 value -3.422744\"                                  \n [70] \"iter  68 value -3.422805\"                                  \n [71] \"iter  69 value -3.423072\"                                  \n [72] \"iter  70 value -3.423128\"                                  \n [73] \"iter  71 value -3.423400\"                                  \n [74] \"iter  72 value -3.423449\"                                  \n [75] \"iter  73 value -3.423727\"                                  \n [76] \"iter  74 value -3.423771\"                                  \n [77] \"iter  75 value -3.424055\"                                  \n [78] \"iter  76 value -3.424092\"                                  \n [79] \"iter  77 value -3.424382\"                                  \n [80] \"iter  78 value -3.424413\"                                  \n [81] \"iter  79 value -3.424708\"                                  \n [82] \"iter  80 value -3.424734\"                                  \n [83] \"iter  81 value -3.425035\"                                  \n [84] \"iter  82 value -3.425054\"                                  \n [85] \"iter  83 value -3.425361\"                                  \n [86] \"iter  84 value -3.425374\"                                  \n [87] \"iter  85 value -3.425687\"                                  \n [88] \"iter  86 value -3.425694\"                                  \n [89] \"iter  87 value -3.426013\"                                  \n [90] \"iter  88 value -3.426062\"                                  \n [91] \"iter  89 value -3.426079\"                                  \n [92] \"iter  90 value -3.426079\"                                  \n [93] \"iter  91 value -3.426410\"                                  \n [94] \"iter  92 value -3.426460\"                                  \n [95] \"iter  93 value -3.426476\"                                  \n [96] \"iter  94 value -3.426528\"                                  \n [97] \"iter  95 value -3.426543\"                                  \n [98] \"iter  96 value -3.426595\"                                  \n [99] \"iter  97 value -3.426610\"                                  \n[100] \"iter  98 value -3.426662\"                                  \n[101] \"iter  99 value -3.426677\"                                  \n[102] \"iter 100 value -3.426729\"                                  \n[103] \"final  value -3.426729 \"                                   \n[104] \"stopped after 100 iterations\"                              \n[105] \"initial  value -2.699617 \"                                 \n[106] \"iter   2 value -2.935051\"                                  \n[107] \"iter   3 value -2.947145\"                                  \n[108] \"iter   4 value -2.958274\"                                  \n[109] \"iter   5 value -2.990007\"                                  \n[110] \"iter   6 value -3.044065\"                                  \n[111] \"iter   7 value -3.069038\"                                  \n[112] \"iter   8 value -3.086065\"                                  \n[113] \"iter   9 value -3.108245\"                                  \n[114] \"iter  10 value -3.117474\"                                  \n[115] \"iter  11 value -3.121246\"                                  \n[116] \"iter  12 value -3.160135\"                                  \n[117] \"iter  13 value -3.177161\"                                  \n[118] \"iter  14 value -3.182063\"                                  \n[119] \"iter  15 value -3.183726\"                                  \n[120] \"iter  16 value -3.184448\"                                  \n[121] \"iter  17 value -3.184590\"                                  \n[122] \"iter  18 value -3.184621\"                                  \n[123] \"iter  19 value -3.184621\"                                  \n[124] \"iter  19 value -3.184621\"                                  \n[125] \"final  value -3.184621 \"                                   \n[126] \"converged\"                                                 \n[127] \"&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\"                              \n[128] \" \"                                                         \n[129] \"Coefficients: \"                                            \n[130] \"      Estimate     SE t.value p.value\"                     \n[131] \"ar1     1.1617 0.1334  8.7077  0.0000\"                     \n[132] \"ar2    -0.8128 0.1240 -6.5545  0.0000\"                     \n[133] \"ma1    -1.0000 0.1476 -6.7744  0.0000\"                     \n[134] \"xmean   0.0006 0.0023  0.2703  0.7904\"                     \n[135] \"\"                                                          \n[136] \"sigma^2 estimated as 0.001347482 on 16 degrees of freedom \"\n[137] \" \"                                                         \n[138] \"AIC = -3.031366  AICc = -2.898033  BIC = -2.782433 \"       \n[139] \" \"                                                         \n\n\n\n\nBased on the results of cross-validation, we can see that the ARMA(2,1) model has a lower RMSE value compared to the ARMA(2,2) model in most of the time. Therefore, we can say that the ARMA(2,1) model is the better model to forecast.\n\n\nCode\nn &lt;- length(res.fit_em)\nk &lt;- 3  # Assuming 5 is the maximum number of observations for testing\n\nrmse1 &lt;- matrix(NA, 8)\nrmse2 &lt;- matrix(NA, 8)\n\nst &lt;- tsp(res.fit_em)[1] + (k - 1)\n\nfor (i in 1:8) {\n  # Define the training set\n  train_end &lt;- st + i - 1\n  xtrain &lt;- window(res.fit_em, end = train_end)\n\n  # Define the testing set\n  test_start &lt;- train_end + 1\n  test_end &lt;- min(st + i, tsp(res.fit_em)[2])\n  xtest &lt;- window(res.fit_em, start = test_start, end = test_end)\n\n  fit &lt;- Arima(xtrain, order = c(2, 0, 2), include.drift = TRUE, method = \"ML\")\n  fcast &lt;- forecast(fit, h = 4)\n\n  fit2 &lt;- Arima(xtrain, order = c(2, 0, 1), include.drift = TRUE, method = \"ML\")\n  fcast2 &lt;- forecast(fit2, h = 4)\n\n  rmse1[i] &lt;- sqrt((fcast$mean - xtest)^2)\n  rmse2[i] &lt;- sqrt((fcast2$mean - xtest)^2)\n}\n\nplot(1:8, rmse2, type = \"l\", col = 2, xlab = \"horizon\", ylab = \"RMSE\")\nlines(1:8, rmse1, type = \"l\", col = 3)\nlegend(\"topleft\", legend = c(\"fit2\", \"fit1\"), col = 2:3, lty = 1)\n\n\n\n\n\n\n\n\n\n\n\nThe forecast, with ARMA(2,1) error terms, indicates that average life expectancy in the US is expected to continue increasing, but with a degree of uncertainty, as represented by the widening confidence intervals. This suggests optimism about future health outcomes. As we can see, the death rates due to cancer, heart disease, and diabetes are decreasing, and the infant mortality rate is also decreasing, which could be reasons for the increase in life expectancy. What’s more, we can find from Question 2 that the number of smokers and the alcohol consumption rate are decreasing, which could also increase the life expectancy in the US. Meanwhile, we can see a decrease of average life expectancy around 2020, which could be due to the COVID-19 pandemic.\n\n\nCode\nfit_em &lt;- Arima(life_expectancy_mortality_ts[, \"Life_Expectancy\"], order=c(2,0,1), xreg = life_expectancy_mortality_ts[, c('Death_Cancer', 'Death_Heart_Disease', 'Death_Diabetes', 'Infant_Mortality_Rates')])\n\nsummary(fit_em)\n\n\nSeries: life_expectancy_mortality_ts[, \"Life_Expectancy\"] \nRegression with ARIMA(2,0,1) errors \n\nCoefficients:\n         ar1      ar2      ma1  intercept  Death_Cancer  Death_Heart_Disease\n      1.3451  -0.9196  -1.0000    85.5236        0.0017              -0.0146\ns.e.  0.0955   0.0669   0.1408     0.2952        0.0056               0.0016\n      Death_Diabetes  Infant_Mortality_Rates\n             -0.1386                 -0.2536\ns.e.          0.0220                  0.1511\n\nsigma^2 = 0.0014:  log likelihood = 39.31\nAIC=-60.63   AICc=-42.63   BIC=-51.66\n\nTraining set error measures:\n                       ME       RMSE        MAE         MPE       MAPE\nTraining set 0.0008543314 0.02897839 0.02014405 0.001093736 0.02571722\n                  MASE       ACF1\nTraining set 0.1401967 -0.1625128\n\n\nCode\ncancer_fit &lt;- auto.arima(life_expectancy_mortality_ts[, \"Death_Cancer\"]) \nft &lt;- forecast(cancer_fit)\n\nheart_disease_fit &lt;- auto.arima(life_expectancy_mortality_ts[, \"Death_Heart_Disease\"])\nft2 &lt;- forecast(heart_disease_fit)\n\ndiabetes_fit &lt;- auto.arima(life_expectancy_mortality_ts[, \"Death_Diabetes\"])\nft3 &lt;- forecast(diabetes_fit)\n\ninfant_mortality_fit &lt;- auto.arima(life_expectancy_mortality_ts[, \"Infant_Mortality_Rates\"])\nft4 &lt;- forecast(infant_mortality_fit)\n\nxreg = cbind(Cancer = ft$mean,\n             Heart_Disease = ft2$mean,\n             Diabetes = ft3$mean,\n             Infant_Mortality = ft4$mean)\n\nfcast &lt;- forecast(fit_em, xreg=xreg)\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Average Life Expectancy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Will the US Average Life Expectancy Increase with Prescription Drug Expenditure?\nThere were previous research studies that showed the more rapid adoption of new drugs has substantial benefits in the form of increased life expectancy, higher productivity and lower non-drug health care expenditure (Baker, & Fugh-Berman, 2009). In this section, we will investigate the relationship between the average life expectancy in the US and the prescription drug expenditure.\n\n(VAR) Average Life Expectancy ~ Prescription Drug Expenditure\n\n\nData VisualizationVARselectInitial Model SelectionCross-ValidationModel Creation and DiagnosticsForecasting\n\n\nThe graph presents the trends of average life expectancy alongside prescription drug expenditure over the years. Average Life expectancy shows an increase over time until a sharp drop in 2020, which could be due to the COVID-19 pandemic. The prescription drug expenditure appears to have a consistent upward trend, indicating growing costs or increased consumption of medications.\n\n\nCode\nhealthcare_ts &lt;- ts(healthcare_data[, c(2:3)], start = c(2005), end = 2023, frequency = 1)\n\nautoplot(healthcare_ts, facets = TRUE, color=\"#27aeef\") + xlab(\"Year\") +\n  ggtitle(\"Average Life Expectancy vs Health Insurance Coverage and Prescription Drug Expenditure\")\n\n\n\n\n\n\n\n\n\n\n\nThe VARselect function is used to select the optimal p for the VAR model. From the results, we can see that the optimal p are 3 and 4.\n\n\nCode\nVARselect(healthcare_ts, lag.max = 4, type = \"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     4      4      3      3 \n\n$criteria\n               1         2         3         4\nAIC(n)  3.417688  3.013839  2.712314  2.545620\nHQ(n)   3.413665  3.007805  2.704269  2.535564\nSC(n)   3.795314  3.580279  3.467567  3.489687\nFPE(n) 31.315351 22.385979 19.262274 22.149823\n\n\n\n\nBased on the VARselect results, we fit the VAR model with p=3 and p=4 to the dataset. And the VAR(3) model is better, based on the residual errors and number of significant variables.\n\n\nCode\nsummary(fitvar4&lt;-vars::VAR(healthcare_ts, p=3, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Life_Expectancy, Prescription_Drug_Expenditure \nDeterministic variables: both \nSample size: 16 \nLog Likelihood: -51.578 \nRoots of the characteristic polynomial:\n 1.12  1.12 1.096 0.8173 0.8173 0.0876\nCall:\nvars::VAR(y = healthcare_ts, p = 3, type = \"both\")\n\n\nEstimation results for equation Life_Expectancy: \n================================================ \nLife_Expectancy = Life_Expectancy.l1 + Prescription_Drug_Expenditure.l1 + Life_Expectancy.l2 + Prescription_Drug_Expenditure.l2 + Life_Expectancy.l3 + Prescription_Drug_Expenditure.l3 + const + trend \n\n                                  Estimate Std. Error t value Pr(&gt;|t|)  \nLife_Expectancy.l1                0.555793   0.299310   1.857   0.1004  \nPrescription_Drug_Expenditure.l1 -0.015476   0.017402  -0.889   0.3998  \nLife_Expectancy.l2               -0.717527   0.374989  -1.913   0.0920 .\nPrescription_Drug_Expenditure.l2  0.018246   0.019666   0.928   0.3807  \nLife_Expectancy.l3                1.316801   0.540176   2.438   0.0407 *\nPrescription_Drug_Expenditure.l3 -0.007517   0.008621  -0.872   0.4086  \nconst                            -9.771491  40.477954  -0.241   0.8153  \ntrend                            -0.084929   0.166056  -0.511   0.6229  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.432 on 8 degrees of freedom\nMultiple R-Squared: 0.7032, Adjusted R-squared: 0.4435 \nF-statistic: 2.707 on 7 and 8 DF,  p-value: 0.09318 \n\n\nEstimation results for equation Prescription_Drug_Expenditure: \n============================================================== \nPrescription_Drug_Expenditure = Life_Expectancy.l1 + Prescription_Drug_Expenditure.l1 + Life_Expectancy.l2 + Prescription_Drug_Expenditure.l2 + Life_Expectancy.l3 + Prescription_Drug_Expenditure.l3 + const + trend \n\n                                   Estimate Std. Error t value Pr(&gt;|t|)   \nLife_Expectancy.l1                 -9.23836    4.72385  -1.956  0.08623 . \nPrescription_Drug_Expenditure.l1    0.75815    0.27465   2.760  0.02466 * \nLife_Expectancy.l2                 -6.59986    5.91825  -1.115  0.29715   \nPrescription_Drug_Expenditure.l2   -0.76668    0.31038  -2.470  0.03870 * \nLife_Expectancy.l3                 -4.37751    8.52532  -0.513  0.62150   \nPrescription_Drug_Expenditure.l3    0.08615    0.13606   0.633  0.54428   \nconst                            1745.30910  638.84243   2.732  0.02577 * \ntrend                               9.84516    2.62077   3.757  0.00557 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 6.819 on 8 degrees of freedom\nMultiple R-Squared: 0.9903, Adjusted R-squared: 0.9817 \nF-statistic: 116.3 on 7 and 8 DF,  p-value: 2.065e-07 \n\n\n\nCovariance matrix of residuals:\n                              Life_Expectancy Prescription_Drug_Expenditure\nLife_Expectancy                        0.1867                       -0.1631\nPrescription_Drug_Expenditure         -0.1631                       46.4947\n\nCorrelation matrix of residuals:\n                              Life_Expectancy Prescription_Drug_Expenditure\nLife_Expectancy                       1.00000                      -0.05536\nPrescription_Drug_Expenditure        -0.05536                       1.00000\n\n\nCode\nsummary(fitvar5&lt;-vars::VAR(healthcare_ts, p=4, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Life_Expectancy, Prescription_Drug_Expenditure \nDeterministic variables: both \nSample size: 15 \nLog Likelihood: -41.66 \nRoots of the characteristic polynomial:\n1.318 1.318 1.054 1.016 1.016 0.8372 0.8372 0.06897\nCall:\nvars::VAR(y = healthcare_ts, p = 4, type = \"both\")\n\n\nEstimation results for equation Life_Expectancy: \n================================================ \nLife_Expectancy = Life_Expectancy.l1 + Prescription_Drug_Expenditure.l1 + Life_Expectancy.l2 + Prescription_Drug_Expenditure.l2 + Life_Expectancy.l3 + Prescription_Drug_Expenditure.l3 + Life_Expectancy.l4 + Prescription_Drug_Expenditure.l4 + const + trend \n\n                                   Estimate Std. Error t value Pr(&gt;|t|)\nLife_Expectancy.l1                2.893e-01  4.009e-01   0.722    0.503\nPrescription_Drug_Expenditure.l1  1.229e-04  2.408e-02   0.005    0.996\nLife_Expectancy.l2               -5.435e-01  4.431e-01  -1.227    0.275\nPrescription_Drug_Expenditure.l2 -3.475e-03  2.733e-02  -0.127    0.904\nLife_Expectancy.l3                2.897e-01  1.678e+00   0.173    0.870\nPrescription_Drug_Expenditure.l3  2.069e-02  2.863e-02   0.723    0.502\nLife_Expectancy.l4                1.450e+00  1.448e+00   1.001    0.363\nPrescription_Drug_Expenditure.l4 -7.628e-03  1.050e-02  -0.727    0.500\nconst                            -3.715e+01  7.590e+01  -0.489    0.645\ntrend                            -2.966e-01  3.118e-01  -0.951    0.385\n\n\nResidual standard error: 0.4636 on 5 degrees of freedom\nMultiple R-Squared: 0.7767, Adjusted R-squared: 0.3749 \nF-statistic: 1.933 on 9 and 5 DF,  p-value: 0.2422 \n\n\nEstimation results for equation Prescription_Drug_Expenditure: \n============================================================== \nPrescription_Drug_Expenditure = Life_Expectancy.l1 + Prescription_Drug_Expenditure.l1 + Life_Expectancy.l2 + Prescription_Drug_Expenditure.l2 + Life_Expectancy.l3 + Prescription_Drug_Expenditure.l3 + Life_Expectancy.l4 + Prescription_Drug_Expenditure.l4 + const + trend \n\n                                  Estimate Std. Error t value Pr(&gt;|t|)  \nLife_Expectancy.l1                -14.3214     5.3702  -2.667   0.0445 *\nPrescription_Drug_Expenditure.l1    0.5250     0.3225   1.628   0.1645  \nLife_Expectancy.l2                 -9.4113     5.9349  -1.586   0.1737  \nPrescription_Drug_Expenditure.l2   -0.7647     0.3660  -2.089   0.0910 .\nLife_Expectancy.l3                -47.2844    22.4822  -2.103   0.0894 .\nPrescription_Drug_Expenditure.l3   -0.1577     0.3835  -0.411   0.6980  \nLife_Expectancy.l4                 33.8285    19.4019   1.744   0.1417  \nPrescription_Drug_Expenditure.l4   -0.1149     0.1406  -0.817   0.4510  \nconst                            3187.3370  1016.6616   3.135   0.0258 *\ntrend                              14.9545     4.1767   3.580   0.0159 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 6.209 on 5 degrees of freedom\nMultiple R-Squared: 0.9944, Adjusted R-squared: 0.9843 \nF-statistic: 98.59 on 9 and 5 DF,  p-value: 4.317e-05 \n\n\n\nCovariance matrix of residuals:\n                              Life_Expectancy Prescription_Drug_Expenditure\nLife_Expectancy                        0.2149                       -0.5582\nPrescription_Drug_Expenditure         -0.5582                       38.5562\n\nCorrelation matrix of residuals:\n                              Life_Expectancy Prescription_Drug_Expenditure\nLife_Expectancy                        1.0000                       -0.1939\nPrescription_Drug_Expenditure         -0.1939                        1.0000\n\n\n\n\nThe cross-validation is used to find the best model for forecasting. It selects VAR(3) as the best model.\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(healthcare_ts)/folds)\n\nyr = rep(c(2005:2022),each =1) #year\n\nrmse1 = data.frame()\nrmse2 = data.frame()\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- healthcare_ts[-(start:end), ]\n  test_model &lt;- healthcare_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 4, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  # Define the fit models\n  fit &lt;- vars::VAR(train_model, p=4, type= \"both\", season = NULL, exog = NULL)\n  fit2 &lt;- vars::VAR(train_model, p=3, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  \n  # Define the prediction\n  pred &lt;- predict(fit, n.ahead = h)\n  pred2 &lt;- predict(fit2, n.ahead = h)\n  \n  pred_life &lt;- pred$fcst$Life_Expectancy[,1]\n  pred_life2 &lt;- pred2$fcst$Life_Expectancy[,1]\n  \n  mse &lt;- mean((pred_life - test_model[, \"Life_Expectancy\"])^2)\n  mse2 &lt;- mean((pred_life2 - test_model[, \"Life_Expectancy\"])^2)\n  \n  rmse1 &lt;- rbind(rmse1, data.frame(Life_Expectancy = sqrt(mse)))\n  rmse2 &lt;- rbind(rmse2, data.frame(Life_Expectancy = sqrt(mse2)))\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  } else if(mse2 &lt; best_performance){\n    best_model &lt;- fit2\n    best_performance &lt;- mse2\n  }\n\n}\n\n\n\n# Plot the RMSE of two models\n\nplot(rmse1$Life_Expectancy, type = \"l\", col = \"blue\", xlab = \"Fold\", ylab = \"RMSE\", main = \"RMSE of VAR(3) and VAR(4)\")\nlines(rmse2$Life_Expectancy, col = \"red\")\nlegend(\"topright\", legend = c(\"VAR(4)\", \"VAR(3)\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\n\n\nModel VAR(3) is a better fit without serial correlation, based on the low p-value and ACF plots. Model VAR(3) is then used to forecast the average life expectancy and prescription drug expenditure.\n\n\nCode\n# VAR(3)\nvar_model_5 &lt;- vars::VAR(healthcare_ts, p=3, type= \"both\", season = NULL, exog = NULL)\nlife.serial &lt;- serial.test(var_model_5, lags.pt = 4, type = \"PT.asymptotic\") \nlife.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_5\nChi-squared = 11.835, df = 4, p-value = 0.01862\n\n\nCode\nplot(life.serial, names = \"Life_Expectancy\", mar = c(4, 4, 2, 1)) \n\n\n\n\n\n\n\n\n\nCode\nplot(life.serial, names = \"Prescription_Drug_Expenditure\", mar = c(4, 4, 2, 1))\n\n\n\n\n\n\n\n\n\nCode\n# VAR(4)\nvar_model_6 &lt;- vars::VAR(healthcare_ts, p=4, type= \"both\", season = NULL, exog = NULL)\nlife_2.serial &lt;- serial.test(var_model_6, lags.pt = 4, type = \"PT.asymptotic\")\nlife_2.serial\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object var_model_6\nChi-squared = 30.728, df = 0, p-value &lt; 2.2e-16\n\n\nCode\nplot(life_2.serial, names = \"Life_Expectancy\", mar = c(4, 4, 2, 1))\n\n\n\n\n\n\n\n\n\nCode\nplot(life_2.serial, names = \"Prescription_Drug_Expenditure\", mar = c(4, 4, 2, 1))\n\n\n\n\n\n\n\n\n\n\n\nThe forecast plot shows the predicted values for Life Expectancy and Prescription Drug Expenditure for the next several years. The forecasted values are based on the VAR(3) model.\nThe fanchart for life expectancy suggests a first decrease and then an increase trend, while the prescription drug expenditure is expected to increase over the next several years. We can speculate that the decrease in life expectancy could be due to the COVID-19 pandemic, which has had a significant impact on public health. Then the prescription drug expenditure is expected to increase, which will cause a rise in average life expectancy. Our forecasting conclusion aligns perfectly with the previous research findings.\n\n\nCode\n## Forecasting\n(fit.pr = predict(fitvar4, n.ahead = 10, ci = 0.95))\n\n\n$Life_Expectancy\n          fcst    lower    upper        CI\n [1,] 76.44938 75.60260 77.29617 0.8467877\n [2,] 74.53820 73.54215 75.53424 0.9960452\n [3,] 75.86014 74.83897 76.88131 1.0211716\n [4,] 75.13810 73.85523 76.42097 1.2828725\n [5,] 71.14905 69.42541 72.87268 1.7236337\n [6,] 71.22022 69.45785 72.98259 1.7623740\n [7,] 72.61743 70.83283 74.40203 1.7846038\n [8,] 67.67221 65.10940 70.23502 2.5628092\n [9,] 64.19359 61.53640 66.85077 2.6571863\n[10,] 67.22979 64.51337 69.94621 2.7164196\n\n$Prescription_Drug_Expenditure\n          fcst    lower    upper       CI\n [1,] 422.2153 408.8509 435.5797 13.36441\n [2,] 432.9074 414.1659 451.6489 18.74149\n [3,] 465.8879 441.3460 490.4297 24.54183\n [4,] 502.0095 474.7731 529.2459 27.23641\n [5,] 521.1881 493.3974 548.9787 27.79063\n [6,] 556.5518 526.6893 586.4144 29.86255\n [7,] 610.4465 576.4960 644.3971 33.95058\n [8,] 639.7759 604.3627 675.1890 35.41315\n [9,] 669.7366 631.6595 707.8136 38.07708\n[10,] 743.1114 698.6160 787.6068 44.49539\n\n\nCode\nfanchart(fit.pr, mar = c(4, 4, 2, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. How Public Health Crisis(Covid-19) influences the financial market of Healthcare\n\nData VisualizationAuto ARIMAModel FittingBest Models ResidualsCross-ValidationFit the Best ModelForecasting\n\n\n\n\nCode\ncovid_stock_ts &lt;- ts(covid_stock[,c(2:6)], star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")), frequency = 365) # Daily data\n\nautoplot(covid_stock_ts[,c(1:5)], facets=TRUE, color=\"#27aeef\") +\n  xlab(\"Date\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Variables Influencing the Healthcare Financial Market\")\n\n\n\n\n\n\n\n\n\n\n\nThe Auto ARIMA function suggest the best ARIMA model for the given data is ARIMA(0,1,0).\n\n\nCode\nxreg &lt;- cbind(TEVA = covid_stock_ts[, \"TEVA\"],\n              VRTX = covid_stock_ts[, \"VRTX\"],\n              SAGE = covid_stock_ts[, \"SAGE\"],\n              new_cases = covid_stock_ts[, \"new_cases\"])\n\nfit_health_finance &lt;- auto.arima(covid_stock_ts[, \"PFE\"], xreg = xreg)\nsummary(fit_health_finance)\n\n\nSeries: covid_stock_ts[, \"PFE\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        drift    TEVA    VRTX     SAGE  new_cases\n      -0.0041  0.3077  0.0424  -0.0192          0\ns.e.   0.0049  0.1119  0.0063   0.0165          0\n\nsigma^2 = 0.4624:  log likelihood = -619.49\nAIC=1250.98   AICc=1251.12   BIC=1277.38\n\nTraining set error measures:\n                      ME      RMSE       MAE          MPE     MAPE       MASE\nTraining set 3.78327e-05 0.6765729 0.4705595 -0.005928339 1.084673 0.05370965\n                    ACF1\nTraining set -0.02671866\n\n\nCode\n# Check residuals\ncheckresiduals(fit_health_finance)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 239.51, df = 121, p-value = 8.155e-10\n\nModel df: 0.   Total lags used: 121\n\n\n\n\nI first created a regression of the stock price of Pfizer(PFE) with the stock prices of Vertex Pharmaceuticals(VRTX), Teva Pharmaceutical Industries(TEVA), Sage Therapeutics(SAGE), and the new cases of Covid-19. Then I examined the residuals and fitted the models manually to find the best model based on AIC, BIC, and AICc. Next, I checked the models by doing cross-validation.\nAfter fitting the models manually, I found that the best model is SARIMA(2,0,1)(0,1,0)[365] and SARIMA(1,0,0)(0,1,0)[365] based on AIC, BIC, and AICc.\n\n\nCode\n# Change the data to time series\ncovid_stock$SPFE&lt;-ts(covid_stock$PFE,star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")),frequency = 365.25)\ncovid_stock$VRTX&lt;-ts(covid_stock$VRTX,star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")),frequency = 365.25)\ncovid_stock$TEVA&lt;-ts(covid_stock$TEVA,star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")),frequency = 365.25)\ncovid_stock$SAGE&lt;-ts(covid_stock$SAGE,star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")),frequency = 365.25)\ncovid_stock$new_cases&lt;-ts(covid_stock$new_cases,star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\n\n############# First fit the linear model##########\nfit.reg_health_finance &lt;- lm(PFE ~ VRTX + TEVA + SAGE + new_cases, data=covid_stock)\nsummary(fit.reg_health_finance)\n\nres.fit&lt;-ts(residuals(fit.reg_health_finance), star=decimal_date(as.Date(\"2020-12-14\",format = \"%Y-%m-%d\")),frequency = 365.25)\n\nggAcf(res.fit, lag.max = 30)\nggPacf(res.fit, lag.max = 30)\n\n# We have to difference the data\nres.fit %&gt;%\n  diff() %&gt;%\n  diff(365) %&gt;%\n  ggtsdisplay()\n\n# Manual Fitting the models\n#p=8, q=8, d=2, P=1, Q=1, D=1, s=365\n\nSARIMA.c=function(p1,p2,q1,q2,P1,Q1,d1,d2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=365\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*42),nrow=42)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P1)\n      {\n        for(Q in Q1:Q1)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n##q=1,3 Q=1 , p=1,2, P=1,2 d=0,1 \n\noutput=SARIMA.c(p1=1,p2=8,q1=1,q2=8,P1=1,Q1=1,d1=0,d2=1,data=res.fit)\n\n#output\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),]\noutput[which.min(output$AICc),]\n\n\n\n\n\n\nCode\nprint('----------------------------BEST MODEL----------------------------------------------')\n# Adjust the order and include the seasonal components for the best model\nmodel_output_health_finance_best &lt;- Arima(res.fit, order=c(2,0,1), \n                                  seasonal=list(order=c(0,1,0), period=365),\n                                  include.drift = TRUE)\nsummary(model_output_health_finance_best)\n\nprint('--------------------------SECOND BEST MODEL------------------------------------------')\n# Adjust the order and include the seasonal components for the second best model\nmodel_output_health_finance_second_best &lt;- Arima(res.fit, order=c(1,0,0),\n                                         seasonal=list(order=c(0,1,0), period=365),\n                                         include.drift = TRUE)\nsummary(model_output_health_finance_second_best)\n\n\n\n\n\n\nCode\nlen &lt;- length(res.fit)\nx &lt;- 30\ntrain_size &lt;- len - x\nst &lt;- tsp(res.fit)[1]+(292-1)/365\n\n# Initialize vectors to store the forecast errors\nerrors1 &lt;- numeric(x)\nerrors2 &lt;- numeric(x)\n\n# Loop over the forecast horizon\nfor (i in 1:x) {\n  train_ts &lt;- window(res.fit, end=c(st + i - 1))\n  \n                                                                              model1 &lt;- Arima(res.fit, order=c(2,0,1),\n                                                                                              seasonal=list(order=c(0,1,0), period=365),\n                                                                                              include.drift = TRUE)\n                                                                                                \n  model2 &lt;- Arima(res.fit, order=c(1,0,0),\n                  seasonal=list(order=c(0,1,0), period=365),\n                  include.drift = TRUE)\n\n  forecast1 &lt;- forecast(model1, h=2)\n  forecast2 &lt;- forecast(model2, h=2)\n\n  errors1[i] &lt;- res.fit[train_size + i] - forecast1$mean\n  errors2[i] &lt;- res.fit[train_size + i] - forecast2$mean\n}\n\n# Calculate RMSE for each model\nrmse1 &lt;- sqrt(mean(errors1^2))\nrmse2 &lt;- sqrt(mean(errors2^2))\n\n# Create a data frame for plotting\nrmse_data &lt;- data.frame(\n  Time = 1:x,\n  RMSE_Model1 = sqrt(cumsum(errors1^2) / 1:x),\n  RMSE_Model2 = sqrt(cumsum(errors2^2) / 1:x)\n)\n\n#print(paste(\"MAE Model 1:\", mae_B))\nprint(paste(\"RMSE Model 1:\", rmse1))\n\n#print(paste(\"MAE Model 2:\", mae_sB))\nprint(paste(\"RMSE Model 2:\", rmse2))\n\n# Function to compare models based on MAE and RMSE\ncompare_models &lt;- function(rmse1, rmse2) {\n  if (rmse1 &lt; rmse2) {\n    return(\"Model 1 is better based on RMSE\")\n  } else if (rmse2 &lt; rmse1) {\n    return(\"Model 2 is better based on RMSE\")\n  } else {\n    return(\"The models perform differently across metrics. Further analysis is required.\")\n  }\n}\n  \n# Comparing the models\nresult &lt;- compare_models(rmse1,rmse2)\nprint(result)\n\n# Melt the data into long format\nrmse_long &lt;- reshape2::melt(rmse_data, id.vars = 'Time', variable.name = 'Model', value.name = 'RMSE')\n\n# Plot the RMSE for both models\nggplot(rmse_long, aes(x = Time, y = RMSE, colour = Model)) +\n  geom_line() +\n  labs(title = \"RMSE Comparison Over Time\", x = \"Time\", y = \"RMSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n[1] Yousaf, I., Pham, L., & Goodell, J. W. (2023). Interconnectedness between healthcare tokens and healthcare stocks: Evidence from a quantile VAR approach. International Review of Economics & Finance, 86, 271-283. Available here\n[2] Haydier, E. A., Albarwari, N. H. S., & Ali, T. H. (2023). The Comparison Between VAR and ARIMAX Time Series Models in Forecasting. Iraqi Journal of Statistical Sciences, 20(2), 249-262. Available here\n[3] Adu, W. K., Appiahene, P., & Afrifa, S. (2023). VAR, ARIMAX and ARIMA models for nowcasting unemployment rate in Ghana using Google trends. Journal of Electrical Systems and Information Technology, 10(1), 12. Available here\n[4] Ulyah, S. M. (2019, July). Forecasting index and stock returns by considering the effect of Indonesia pre-presidential election 2019 using ARIMAX and VARX approches. In Journal of Physics: Conference Series (Vol. 1277, No. 1, p. 012053). IOP Publishing. Available here\n[5] Baker, D., & Fugh-Berman, A. (2009). Do new drugs increase life expectancy? A critique of a Manhattan Institute paper. Journal of general internal medicine, 24(5), 678–682. Available here"
  },
  {
    "objectID": "multi_TS_model.html#data-visualization",
    "href": "multi_TS_model.html#data-visualization",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Data Visualization",
    "text": "Data Visualization\n(VAR)Pfizer stock ~ Vertex stock + Teva stock + SAGE stock\nWe chose these stocks because they can each represent different pharmaceutical companies. Pfizer is a large pharmaceutical company, Vertex is a biotech company, Teva is a drug company, and SAGE is a small biotech company. They play different roles in the pharmaceutical industry, and Pfizer was chosen as our primary research subject because it made the COVID-19 vaccines. It has substantial market cap and prominent role in pharmaceuticals.\n\n\nCode\nstock_ts &lt;- cbind(pharma_ts, biotech_ts, drug_ts, small_biotech_ts)\ncolnames(stock_ts) &lt;- c(\"pharma\", \"biotech\", \"drug\", \"small_biotech\")\n\nautoplot(stock_ts)"
  },
  {
    "objectID": "multi_TS_model.html#varselect",
    "href": "multi_TS_model.html#varselect",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "VARselect",
    "text": "VARselect\nAs we can see from the results, the p recommended by VARselect() are 10 and 1.\n\n\nCode\nVARselect(stock_ts, lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      1      1     10 \n\n$criteria\n               1         2         3         4         5         6         7\nAIC(n)  2.357832  2.352112  2.358823  2.355605  2.356543  2.354853  2.358775\nHQ(n)   2.390402  2.406394  2.434819  2.453314  2.475964  2.495988  2.521623\nSC(n)   2.445117  2.497586  2.562486  2.617458  2.676585  2.733085  2.795197\nFPE(n) 10.568022 10.507746 10.578520 10.544564 10.554498 10.536746 10.578240\n               8         9        10\nAIC(n)  2.365997  2.371767  2.349877\nHQ(n)   2.550557  2.578041  2.577863\nSC(n)   2.860607  2.924568  2.960867\nFPE(n) 10.655016 10.716828 10.484956"
  },
  {
    "objectID": "multi_TS_model.html#initial-model-selection",
    "href": "multi_TS_model.html#initial-model-selection",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Initial Model Selection",
    "text": "Initial Model Selection\nAs we can see from the results, VAR(10) model is better than VAR(1) model, based on the residual errors and significance of the coefficients. What’s more, except small-biotech stock, all the other stocks are correlated with each other.\n\n\nCode\nsummary(fitvar1&lt;-vars::VAR(stock_ts, p=10, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: pharma, biotech, drug, small_biotech \nDeterministic variables: both \nSample size: 1452 \nLog Likelihood: -9779.206 \nRoots of the characteristic polynomial:\n0.9925 0.9925 0.9897 0.8963 0.8963 0.8844 0.8844 0.8304 0.8304 0.8153 0.8153 0.8109 0.8109 0.7968 0.7968 0.7622 0.7622 0.7443 0.7443 0.7417 0.7417 0.7309 0.7309 0.7126 0.7126 0.7122 0.7122 0.7091 0.7091 0.6601 0.6292 0.6292 0.6236 0.6236 0.6151 0.6151 0.5956 0.5956 0.5617 0.5617\nCall:\nvars::VAR(y = stock_ts, p = 10, type = \"both\")\n\n\nEstimation results for equation pharma: \n======================================= \npharma = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1          9.655e-01  2.714e-02  35.574  &lt; 2e-16 ***\nbiotech.l1        -5.299e-03  2.463e-03  -2.151 0.031615 *  \ndrug.l1           -6.690e-02  5.996e-02  -1.116 0.264716    \nsmall_biotech.l1  -6.720e-03  7.606e-03  -0.884 0.377115    \npharma.l2          3.606e-03  3.796e-02   0.095 0.924326    \nbiotech.l2         4.074e-03  3.393e-03   1.201 0.230079    \ndrug.l2            1.020e-01  8.545e-02   1.194 0.232599    \nsmall_biotech.l2   1.078e-02  1.089e-02   0.989 0.322667    \npharma.l3          7.828e-02  3.799e-02   2.060 0.039557 *  \nbiotech.l3         5.923e-04  3.393e-03   0.175 0.861450    \ndrug.l3            4.240e-02  8.557e-02   0.495 0.620342    \nsmall_biotech.l3   7.559e-03  1.090e-02   0.693 0.488191    \npharma.l4         -5.744e-02  3.813e-02  -1.506 0.132199    \nbiotech.l4         3.194e-03  3.394e-03   0.941 0.346821    \ndrug.l4           -2.166e-01  8.554e-02  -2.532 0.011449 *  \nsmall_biotech.l4  -1.360e-02  1.089e-02  -1.249 0.211981    \npharma.l5          5.466e-02  3.812e-02   1.434 0.151809    \nbiotech.l5        -6.002e-03  3.404e-03  -1.763 0.078051 .  \ndrug.l5            3.085e-01  8.568e-02   3.601 0.000328 ***\nsmall_biotech.l5  -4.774e-03  1.092e-02  -0.437 0.662148    \npharma.l6         -8.343e-02  3.800e-02  -2.195 0.028307 *  \nbiotech.l6         2.726e-03  3.406e-03   0.800 0.423632    \ndrug.l6           -3.008e-01  8.586e-02  -3.503 0.000474 ***\nsmall_biotech.l6   1.137e-02  1.093e-02   1.041 0.298125    \npharma.l7          1.758e-02  3.793e-02   0.463 0.643165    \nbiotech.l7        -1.808e-03  3.398e-03  -0.532 0.594836    \ndrug.l7            6.673e-02  8.569e-02   0.779 0.436269    \nsmall_biotech.l7  -1.537e-03  1.093e-02  -0.141 0.888200    \npharma.l8         -1.153e-02  3.788e-02  -0.304 0.760984    \nbiotech.l8         2.563e-03  3.404e-03   0.753 0.451756    \ndrug.l8           -6.666e-02  8.564e-02  -0.778 0.436491    \nsmall_biotech.l8  -6.568e-03  1.093e-02  -0.601 0.548056    \npharma.l9          1.085e-01  3.782e-02   2.867 0.004201 ** \nbiotech.l9         5.616e-03  3.406e-03   1.649 0.099471 .  \ndrug.l9            1.352e-01  8.545e-02   1.582 0.113813    \nsmall_biotech.l9   2.195e-02  1.094e-02   2.007 0.044953 *  \npharma.l10        -8.871e-02  2.695e-02  -3.292 0.001021 ** \nbiotech.l10       -7.045e-03  2.461e-03  -2.862 0.004270 ** \ndrug.l10          -5.664e-02  6.006e-02  -0.943 0.345748    \nsmall_biotech.l10 -2.088e-02  7.663e-03  -2.725 0.006512 ** \nconst              1.468e+00  2.893e-01   5.075 4.39e-07 ***\ntrend              8.407e-06  4.015e-05   0.209 0.834193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6332 on 1410 degrees of freedom\nMultiple R-Squared: 0.9917, Adjusted R-squared: 0.9915 \nF-statistic:  4119 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation biotech: \n======================================== \nbiotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         -0.5394694  0.3307112  -1.631  0.10306    \nbiotech.l1         0.9374168  0.0300145  31.232  &lt; 2e-16 ***\ndrug.l1           -1.4363831  0.7305535  -1.966  0.04948 *  \nsmall_biotech.l1  -0.1375215  0.0926745  -1.484  0.13805    \npharma.l2          0.2732680  0.4625104   0.591  0.55472    \nbiotech.l2         0.0799657  0.0413446   1.934  0.05330 .  \ndrug.l2            0.6845242  1.0411832   0.657  0.51100    \nsmall_biotech.l2   0.2115732  0.1327460   1.594  0.11120    \npharma.l3          0.6956512  0.4629649   1.503  0.13317    \nbiotech.l3        -0.1185443  0.0413455  -2.867  0.00420 ** \ndrug.l3            2.5388033  1.0426828   2.435  0.01502 *  \nsmall_biotech.l3  -0.1735610  0.1328426  -1.307  0.19159    \npharma.l4         -0.8436573  0.4646580  -1.816  0.06964 .  \nbiotech.l4         0.0852963  0.0413570   2.062  0.03935 *  \ndrug.l4           -1.5481418  1.0423289  -1.485  0.13770    \nsmall_biotech.l4   0.1004230  0.1327192   0.757  0.44938    \npharma.l5          0.5638756  0.4644399   1.214  0.22491    \nbiotech.l5        -0.0008106  0.0414726  -0.020  0.98441    \ndrug.l5            0.2396946  1.0440282   0.230  0.81845    \nsmall_biotech.l5  -0.0933255  0.1330991  -0.701  0.48331    \npharma.l6         -0.7427928  0.4630729  -1.604  0.10893    \nbiotech.l6        -0.0503807  0.0415044  -1.214  0.22500    \ndrug.l6           -0.7064403  1.0462163  -0.675  0.49964    \nsmall_biotech.l6   0.0652107  0.1331520   0.490  0.62439    \npharma.l7          0.3714194  0.4622257   0.804  0.42179    \nbiotech.l7         0.1121951  0.0414046   2.710  0.00682 ** \ndrug.l7           -0.7947308  1.0440871  -0.761  0.44668    \nsmall_biotech.l7   0.1869698  0.1332014   1.404  0.16064    \npharma.l8         -0.2217743  0.4616053  -0.480  0.63099    \nbiotech.l8        -0.1051239  0.0414831  -2.534  0.01138 *  \ndrug.l8           -0.3286737  1.0434655  -0.315  0.75282    \nsmall_biotech.l8  -0.3386215  0.1331950  -2.542  0.01112 *  \npharma.l9          0.8171663  0.4608941   1.773  0.07644 .  \nbiotech.l9         0.1072026  0.0415069   2.583  0.00990 ** \ndrug.l9            1.1643963  1.0411697   1.118  0.26361    \nsmall_biotech.l9   0.3889288  0.1332582   2.919  0.00357 ** \npharma.l10        -0.4393511  0.3283823  -1.338  0.18114    \nbiotech.l10       -0.0658917  0.0299908  -2.197  0.02818 *  \ndrug.l10          -0.3742131  0.7317882  -0.511  0.60917    \nsmall_biotech.l10 -0.2463088  0.0933743  -2.638  0.00843 ** \nconst             14.5976417  3.5245187   4.142 3.65e-05 ***\ntrend             -0.0000865  0.0004893  -0.177  0.85970    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 7.716 on 1410 degrees of freedom\nMultiple R-Squared: 0.9808, Adjusted R-squared: 0.9802 \nF-statistic:  1756 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation drug: \n===================================== \ndrug = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         -1.128e-02  1.321e-02  -0.854  0.39311    \nbiotech.l1        -8.603e-04  1.199e-03  -0.718  0.47305    \ndrug.l1            9.889e-01  2.917e-02  33.897  &lt; 2e-16 ***\nsmall_biotech.l1   3.841e-03  3.701e-03   1.038  0.29952    \npharma.l2          1.334e-02  1.847e-02   0.722  0.47039    \nbiotech.l2         4.731e-03  1.651e-03   2.865  0.00423 ** \ndrug.l2           -6.208e-02  4.158e-02  -1.493  0.13566    \nsmall_biotech.l2   3.736e-03  5.301e-03   0.705  0.48107    \npharma.l3         -2.261e-02  1.849e-02  -1.223  0.22167    \nbiotech.l3        -3.639e-03  1.651e-03  -2.204  0.02768 *  \ndrug.l3            7.669e-02  4.164e-02   1.842  0.06571 .  \nsmall_biotech.l3  -1.811e-03  5.305e-03  -0.341  0.73284    \npharma.l4          3.595e-02  1.856e-02   1.937  0.05291 .  \nbiotech.l4        -2.728e-03  1.652e-03  -1.652  0.09878 .  \ndrug.l4            5.328e-02  4.163e-02   1.280  0.20075    \nsmall_biotech.l4  -1.595e-02  5.300e-03  -3.009  0.00267 ** \npharma.l5         -1.959e-02  1.855e-02  -1.056  0.29095    \nbiotech.l5         1.852e-03  1.656e-03   1.118  0.26366    \ndrug.l5           -1.293e-02  4.169e-02  -0.310  0.75659    \nsmall_biotech.l5   7.352e-03  5.315e-03   1.383  0.16683    \npharma.l6          1.189e-02  1.849e-02   0.643  0.52028    \nbiotech.l6        -1.978e-03  1.657e-03  -1.193  0.23301    \ndrug.l6           -8.702e-02  4.178e-02  -2.083  0.03745 *  \nsmall_biotech.l6   4.756e-03  5.317e-03   0.894  0.37127    \npharma.l7         -2.629e-02  1.846e-02  -1.424  0.15464    \nbiotech.l7         2.646e-03  1.653e-03   1.600  0.10980    \ndrug.l7            5.727e-02  4.170e-02   1.374  0.16980    \nsmall_biotech.l7   6.565e-03  5.319e-03   1.234  0.21737    \npharma.l8          1.040e-02  1.843e-02   0.564  0.57280    \nbiotech.l8        -1.959e-03  1.657e-03  -1.183  0.23712    \ndrug.l8           -4.526e-02  4.167e-02  -1.086  0.27766    \nsmall_biotech.l8  -1.129e-02  5.319e-03  -2.123  0.03394 *  \npharma.l9          3.580e-02  1.841e-02   1.945  0.05199 .  \nbiotech.l9         4.124e-03  1.658e-03   2.488  0.01296 *  \ndrug.l9           -4.726e-02  4.158e-02  -1.137  0.25592    \nsmall_biotech.l9   6.399e-03  5.322e-03   1.203  0.22937    \npharma.l10        -3.207e-02  1.311e-02  -2.446  0.01458 *  \nbiotech.l10       -2.331e-03  1.198e-03  -1.946  0.05184 .  \ndrug.l10           3.304e-02  2.922e-02   1.130  0.25849    \nsmall_biotech.l10 -3.268e-03  3.729e-03  -0.876  0.38101    \nconst              6.302e-01  1.408e-01   4.477 8.17e-06 ***\ntrend             -3.409e-06  1.954e-05  -0.174  0.86153    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3081 on 1410 degrees of freedom\nMultiple R-Squared: 0.9525, Adjusted R-squared: 0.9511 \nF-statistic: 689.9 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation small_biotech: \n============================================== \nsmall_biotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         -1.711e-01  1.015e-01  -1.686   0.0921 .  \nbiotech.l1         7.012e-03  9.214e-03   0.761   0.4468    \ndrug.l1            7.975e-03  2.243e-01   0.036   0.9716    \nsmall_biotech.l1   1.039e+00  2.845e-02  36.509   &lt;2e-16 ***\npharma.l2          2.631e-01  1.420e-01   1.853   0.0641 .  \nbiotech.l2        -1.640e-02  1.269e-02  -1.293   0.1964    \ndrug.l2            9.605e-02  3.196e-01   0.301   0.7638    \nsmall_biotech.l2  -6.604e-02  4.075e-02  -1.621   0.1053    \npharma.l3         -1.736e-01  1.421e-01  -1.221   0.2222    \nbiotech.l3         2.317e-02  1.269e-02   1.826   0.0681 .  \ndrug.l3           -3.106e-01  3.201e-01  -0.970   0.3320    \nsmall_biotech.l3   3.491e-02  4.078e-02   0.856   0.3921    \npharma.l4          1.145e-01  1.426e-01   0.803   0.4221    \nbiotech.l4        -1.092e-02  1.270e-02  -0.860   0.3900    \ndrug.l4            2.837e-01  3.200e-01   0.886   0.3755    \nsmall_biotech.l4  -2.376e-02  4.074e-02  -0.583   0.5599    \npharma.l5         -1.511e-01  1.426e-01  -1.060   0.2895    \nbiotech.l5        -8.980e-03  1.273e-02  -0.705   0.4807    \ndrug.l5            2.016e-01  3.205e-01   0.629   0.5294    \nsmall_biotech.l5  -3.130e-02  4.086e-02  -0.766   0.4438    \npharma.l6          7.105e-02  1.422e-01   0.500   0.6173    \nbiotech.l6        -5.298e-03  1.274e-02  -0.416   0.6776    \ndrug.l6           -4.770e-01  3.212e-01  -1.485   0.1377    \nsmall_biotech.l6   2.295e-02  4.088e-02   0.561   0.5746    \npharma.l7          5.261e-02  1.419e-01   0.371   0.7108    \nbiotech.l7         1.037e-02  1.271e-02   0.816   0.4147    \ndrug.l7            2.947e-01  3.205e-01   0.919   0.3580    \nsmall_biotech.l7   7.281e-03  4.089e-02   0.178   0.8587    \npharma.l8          5.686e-02  1.417e-01   0.401   0.6883    \nbiotech.l8        -1.173e-02  1.273e-02  -0.921   0.3571    \ndrug.l8            2.023e-01  3.203e-01   0.632   0.5278    \nsmall_biotech.l8   1.783e-03  4.089e-02   0.044   0.9652    \npharma.l9          5.435e-02  1.415e-01   0.384   0.7009    \nbiotech.l9         1.833e-02  1.274e-02   1.438   0.1506    \ndrug.l9           -1.789e-01  3.196e-01  -0.560   0.5757    \nsmall_biotech.l9   5.691e-02  4.091e-02   1.391   0.1644    \npharma.l10        -1.250e-01  1.008e-01  -1.240   0.2153    \nbiotech.l10       -3.745e-03  9.207e-03  -0.407   0.6843    \ndrug.l10          -1.382e-01  2.246e-01  -0.615   0.5386    \nsmall_biotech.l10 -4.822e-02  2.866e-02  -1.682   0.0928 .  \nconst              3.010e-01  1.082e+00   0.278   0.7809    \ntrend              3.827e-05  1.502e-04   0.255   0.7989    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2.369 on 1410 degrees of freedom\nMultiple R-Squared: 0.9842, Adjusted R-squared: 0.9838 \nF-statistic:  2149 on 41 and 1410 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n               pharma biotech    drug small_biotech\npharma        0.40096  0.2439 0.02854       0.27136\nbiotech       0.24387 59.5311 0.87782      -4.91190\ndrug          0.02854  0.8778 0.09494       0.03908\nsmall_biotech 0.27136 -4.9119 0.03908       5.61017\n\nCorrelation matrix of residuals:\n               pharma  biotech    drug small_biotech\npharma        1.00000  0.04991 0.14629       0.18093\nbiotech       0.04991  1.00000 0.36924      -0.26878\ndrug          0.14629  0.36924 1.00000       0.05355\nsmall_biotech 0.18093 -0.26878 0.05355       1.00000\n\n\nCode\nsummary(fitvar2&lt;-vars::VAR(stock_ts, p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: pharma, biotech, drug, small_biotech \nDeterministic variables: both \nSample size: 1461 \nLog Likelihood: -9983.058 \nRoots of the characteristic polynomial:\n0.9909 0.9906 0.9906 0.9577\nCall:\nvars::VAR(y = stock_ts, p = 1, type = \"both\")\n\n\nEstimation results for equation pharma: \n======================================= \npharma = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1         9.893e-01  2.904e-03 340.707  &lt; 2e-16 ***\nbiotech.l1       -1.155e-03  3.870e-04  -2.985  0.00288 ** \ndrug.l1          -4.275e-02  1.430e-02  -2.989  0.00285 ** \nsmall_biotech.l1 -1.824e-03  1.195e-03  -1.526  0.12722    \nconst             1.200e+00  2.582e-01   4.647 3.68e-06 ***\ntrend             4.345e-06  4.038e-05   0.108  0.91432    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6451 on 1455 degrees of freedom\nMultiple R-Squared: 0.9912, Adjusted R-squared: 0.9911 \nF-statistic: 3.259e+04 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation biotech: \n======================================== \nbiotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1        -0.0618969  0.0352756  -1.755  0.07953 .  \nbiotech.l1        0.9826416  0.0047016 209.002  &lt; 2e-16 ***\ndrug.l1          -0.5104846  0.1737440  -2.938  0.00335 ** \nsmall_biotech.l1 -0.0319371  0.0145236  -2.199  0.02804 *  \nconst            13.4257475  3.1367749   4.280 1.99e-05 ***\ntrend            -0.0001188  0.0004906  -0.242  0.80864    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 7.837 on 1455 degrees of freedom\nMultiple R-Squared: 0.9796, Adjusted R-squared: 0.9795 \nF-statistic: 1.396e+04 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation drug: \n===================================== \ndrug = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1        -2.885e-03  1.416e-03  -2.037 0.041826 *  \nbiotech.l1        2.219e-05  1.888e-04   0.118 0.906420    \ndrug.l1           9.642e-01  6.976e-03 138.222  &lt; 2e-16 ***\nsmall_biotech.l1  6.788e-04  5.831e-04   1.164 0.244606    \nconst             4.220e-01  1.259e-01   3.351 0.000826 ***\ntrend            -6.794e-06  1.970e-05  -0.345 0.730207    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3147 on 1455 degrees of freedom\nMultiple R-Squared: 0.949,  Adjusted R-squared: 0.9488 \nF-statistic:  5413 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation small_biotech: \n============================================== \nsmall_biotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \npharma.l1        -6.191e-03  1.062e-02  -0.583    0.560    \nbiotech.l1        1.889e-03  1.416e-03   1.334    0.183    \ndrug.l1           5.867e-03  5.233e-02   0.112    0.911    \nsmall_biotech.l1  9.937e-01  4.374e-03 227.149   &lt;2e-16 ***\nconst            -5.266e-02  9.448e-01  -0.056    0.956    \ntrend             3.358e-05  1.478e-04   0.227    0.820    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2.361 on 1455 degrees of freedom\nMultiple R-Squared: 0.984,  Adjusted R-squared: 0.984 \nF-statistic: 1.794e+04 on 5 and 1455 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n               pharma biotech    drug small_biotech\npharma        0.41618  0.4514 0.03365       0.27877\nbiotech       0.45144 61.4255 0.92748      -4.64132\ndrug          0.03365  0.9275 0.09902       0.04706\nsmall_biotech 0.27877 -4.6413 0.04706       5.57253\n\nCorrelation matrix of residuals:\n               pharma  biotech    drug small_biotech\npharma        1.00000  0.08929 0.16577       0.18305\nbiotech       0.08929  1.00000 0.37608      -0.25087\ndrug          0.16577  0.37608 1.00000       0.06336\nsmall_biotech 0.18305 -0.25087 0.06336       1.00000"
  },
  {
    "objectID": "uni_TS_model.html#log-transformation-of-the-data",
    "href": "uni_TS_model.html#log-transformation-of-the-data",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "12.1 Log Transformation of the data",
    "text": "12.1 Log Transformation of the data\n\n\nCode\n# Create log transformed time series for GDP Contribution of Healthcare Industry\ngdp_healthcare$log.GDP&lt;-log(gdp_healthcare$GDP)\n\nlg.gdp.ts&lt;-ts(gdp_healthcare$log.GDP,star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nautoplot(lg.gdp.ts, color = \"#27aeef\") +\n  xlab(\"Year\") + ylab(\"\") + theme_bw() +\n  ggtitle(\"Log Transformed GDP of Healthcare Industry\")"
  },
  {
    "objectID": "uni_TS_model.html#fit-sarima-on-large-scale-pharmaceutical-stock",
    "href": "uni_TS_model.html#fit-sarima-on-large-scale-pharmaceutical-stock",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "12.2 Fit SARIMA on Large-scale Pharmaceutical Stock",
    "text": "12.2 Fit SARIMA on Large-scale Pharmaceutical Stock\n\nACF and PACF PlotsBest SARIMA Model\n\n\n\n\nCode\n# ACF and PACF plots for Large-scale Pharmaceutical Stock\n# Do the first differencing and seasonal differencing\n\nggAcf(lg.pharma.ts %&gt;% \n        diff() %&gt;% \n        diff(12), lag.max = 30, main=\"Seasonal Differenced + First Differenced ACF\")\n\n\n\n\n\nCode\nggPacf(lg.pharma.ts %&gt;% \n         diff() %&gt;% \n         diff(12), lag.max = 30, main=\"Seasonal Differenced + First Differenced PACF\")\n\n\n\n\n\nAs we can see from the ACF and PACF plots, the seasonal differenced and first differenced time series of the log-transformed Pfizer stock prices exhibit significant autocorrelation at lag 1 and lag 12. As a result, let’s say: d=1, D=1, p=1,2, q=1,2, P=1,2 , and Q=1, 2.\n\n\n\n\nCode\n#write a function\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*100),nrow=100)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,lg.pharma.ts)\n\nknitr::kable(output)\noutput[which.min(output$AIC),]\noutput[which.min(output$BIC),]\noutput[which.min(output$AICc),]"
  },
  {
    "objectID": "uni_TS_model.html#fit-sarima-on-biotechnology-stock",
    "href": "uni_TS_model.html#fit-sarima-on-biotechnology-stock",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "12.3 Fit SARIMA on Biotechnology Stock",
    "text": "12.3 Fit SARIMA on Biotechnology Stock"
  },
  {
    "objectID": "uni_TS_model.html#fit-sarima-on-generic-and-specialty-drug-stock",
    "href": "uni_TS_model.html#fit-sarima-on-generic-and-specialty-drug-stock",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "12.4 Fit SARIMA on Generic and Specialty Drug Stock",
    "text": "12.4 Fit SARIMA on Generic and Specialty Drug Stock"
  },
  {
    "objectID": "uni_TS_model.html#fit-sarima-on-small-and-medium-sized-biotech-stock",
    "href": "uni_TS_model.html#fit-sarima-on-small-and-medium-sized-biotech-stock",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "12.5 Fit SARIMA on Small and Medium-sized Biotech Stock",
    "text": "12.5 Fit SARIMA on Small and Medium-sized Biotech Stock"
  },
  {
    "objectID": "uni_TS_model.html#fit-sarima-on-gdp-contribution-of-healthcare-industry",
    "href": "uni_TS_model.html#fit-sarima-on-gdp-contribution-of-healthcare-industry",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "12.2 Fit SARIMA on GDP Contribution of Healthcare Industry",
    "text": "12.2 Fit SARIMA on GDP Contribution of Healthcare Industry\n\nACF and PACF PlotsBest SARIMA ModelModel DiagnosticsModel FittingForecastingCompare SARIMA with BenchmarkCross-validation\n\n\n\n\nCode\n# ACF and PACF plots for GDP Contribution of Healthcare Industry\n# Do the first differencing and seasonal differencing\n\nggAcf(lg.gdp.ts %&gt;% \n        diff() %&gt;% \n        diff(4), lag.max = 30, main=\"Seasonal Differenced + First Differenced ACF\")\n\n\n\n\n\nCode\nggPacf(lg.gdp.ts %&gt;% \n         diff() %&gt;% \n         diff(4), lag.max = 30, main=\"Seasonal Differenced + First Differenced PACF\")\n\n\n\n\n\nAs we can see from the ACF and PACF plots of the seasonal differenced and first differenced time series of the log-transformed GDP contribution of the healthcare industry, let’s say: d=1, D=1, p=0,1,2, q=0,1,2, P=1,2 , and Q=1, 2.\n\n\nBased on the AIC, BIC, and AICc values, the best model for the log-transformed GDP contribution of the healthcare industry is ARIMA(0,1,1)(0,1,1)[4].\n\n\nCode\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n          }\n        }\n      }\n    }\n  }\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  temp\n}\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=3,P1=1,P2=3,Q1=1,Q2=3,lg.gdp.ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-43.33926\n-42.44889\n-43.08926\n\n\n0\n1\n0\n0\n1\n1\n-50.01086\n-48.23012\n-49.21086\n\n\n0\n1\n0\n0\n1\n2\n-48.09121\n-45.42010\n-46.37693\n\n\n0\n1\n0\n1\n1\n0\n-45.69993\n-43.91919\n-44.89993\n\n\n0\n1\n0\n1\n1\n1\n-48.06329\n-45.39217\n-46.34900\n\n\n0\n1\n0\n1\n1\n2\n-46.80345\n-43.24197\n-43.72653\n\n\n0\n1\n0\n2\n1\n0\n-45.81635\n-43.14523\n-44.10206\n\n\n0\n1\n0\n2\n1\n1\n-46.56998\n-43.00849\n-43.49306\n\n\n0\n1\n1\n0\n1\n0\n-48.46271\n-46.68197\n-47.66271\n\n\n0\n1\n1\n0\n1\n1\n-52.96747\n-50.29635\n-51.25318\n\n\n0\n1\n1\n0\n1\n2\n-50.96760\n-47.40611\n-47.89067\n\n\n0\n1\n1\n1\n1\n0\n-50.45283\n-47.78171\n-48.73854\n\n\n0\n1\n1\n1\n1\n1\n-50.96758\n-47.40609\n-47.89066\n\n\n0\n1\n1\n2\n1\n0\n-49.14983\n-45.58834\n-46.07291\n\n\n0\n1\n2\n0\n1\n0\n-46.67521\n-44.00410\n-44.96093\n\n\n0\n1\n2\n0\n1\n1\n-50.98485\n-47.42336\n-47.90793\n\n\n0\n1\n2\n1\n1\n0\n-48.45448\n-44.89300\n-45.37756\n\n\n1\n1\n0\n0\n1\n0\n-45.19830\n-43.41756\n-44.39830\n\n\n1\n1\n0\n0\n1\n1\n-51.27823\n-48.60711\n-49.56394\n\n\n1\n1\n0\n0\n1\n2\n-49.30550\n-45.74402\n-46.22858\n\n\n1\n1\n0\n1\n1\n0\n-47.54018\n-44.86906\n-45.82589\n\n\n1\n1\n0\n1\n1\n1\n-49.29786\n-45.73637\n-46.22094\n\n\n1\n1\n0\n2\n1\n0\n-47.23768\n-43.67620\n-44.16076\n\n\n1\n1\n1\n0\n1\n0\n-46.69119\n-44.02008\n-44.97690\n\n\n1\n1\n1\n0\n1\n1\n-50.98041\n-47.41892\n-47.90348\n\n\n1\n1\n1\n1\n1\n0\n-48.45413\n-44.89264\n-45.37720\n\n\n1\n1\n2\n0\n1\n0\n-46.77492\n-43.21344\n-43.69800\n\n\n2\n1\n0\n0\n1\n0\n-44.76842\n-42.09731\n-43.05414\n\n\n2\n1\n0\n0\n1\n1\n-50.55993\n-46.99844\n-47.48301\n\n\n2\n1\n0\n1\n1\n0\n-47.30943\n-43.74795\n-44.23251\n\n\n2\n1\n1\n0\n1\n0\n-44.71262\n-41.15113\n-41.63570\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q P D Q       AIC       BIC      AICc\n10 0 1 1 0 1 1 -52.96747 -50.29635 -51.25318\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q       AIC       BIC      AICc\n10 0 1 1 0 1 1 -52.96747 -50.29635 -51.25318\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q       AIC       BIC      AICc\n10 0 1 1 0 1 1 -52.96747 -50.29635 -51.25318\n\n\n\n\nThe ACF plot of the residuals of the ARIMA(0,1,1)(0,1,1)[4] model for the log-transformed GDP contribution of the healthcare industry shows that the residuals are not autocorrelated. The Ljung-Box test also confirms that the residuals are not autocorrelated, with p-values greater than 0.05.\n\n\nCode\ncapture.output(sarima(lg.gdp.ts, 0,1,1,0,1,1,4)) \n\n\n\n\n\n [1] \"initial  value -2.678362 \"                                 \n [2] \"iter   2 value -3.024123\"                                  \n [3] \"iter   3 value -3.081410\"                                  \n [4] \"iter   4 value -3.128822\"                                  \n [5] \"iter   5 value -3.210619\"                                  \n [6] \"iter   6 value -3.239200\"                                  \n [7] \"iter   7 value -3.239824\"                                  \n [8] \"iter   8 value -3.251571\"                                  \n [9] \"iter   9 value -3.256599\"                                  \n[10] \"iter  10 value -3.257339\"                                  \n[11] \"iter  11 value -3.259562\"                                  \n[12] \"iter  12 value -3.259569\"                                  \n[13] \"iter  13 value -3.259753\"                                  \n[14] \"iter  14 value -3.259754\"                                  \n[15] \"iter  14 value -3.259754\"                                  \n[16] \"iter  14 value -3.259754\"                                  \n[17] \"final  value -3.259754 \"                                   \n[18] \"converged\"                                                 \n[19] \"initial  value -2.999789 \"                                 \n[20] \"iter   2 value -3.053504\"                                  \n[21] \"iter   3 value -3.056557\"                                  \n[22] \"iter   4 value -3.056768\"                                  \n[23] \"iter   5 value -3.056902\"                                  \n[24] \"iter   6 value -3.056924\"                                  \n[25] \"iter   7 value -3.056924\"                                  \n[26] \"iter   7 value -3.056924\"                                  \n[27] \"iter   7 value -3.056924\"                                  \n[28] \"final  value -3.056924 \"                                   \n[29] \"converged\"                                                 \n[30] \"&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\"                              \n[31] \" \"                                                         \n[32] \"Coefficients: \"                                            \n[33] \"     Estimate    SE t.value p.value\"                       \n[34] \"ma1   -0.5889 0.209 -2.8180  0.0124\"                       \n[35] \"sma1  -0.9998 0.463 -2.1595  0.0463\"                       \n[36] \"\"                                                          \n[37] \"sigma^2 estimated as 0.001464451 on 16 degrees of freedom \"\n[38] \" \"                                                         \n[39] \"AIC = -2.942637  AICc = -2.898193  BIC = -2.794242 \"       \n[40] \" \"                                                         \n\n\n\n\nThe Equation for the best SARIMA model is:\n\\[\n(1-B)(1-B^4)(Y_t - \\mu)=(1 - 0.5889B-0.9998B^4)\\varepsilon_t\n\\]\n\n\nCode\nfit &lt;- Arima(lg.gdp.ts, order=c(0,1,1), seasonal=c(0,1,1))\nsummary(fit)\n\n\nSeries: lg.gdp.ts \nARIMA(0,1,1)(0,1,1)[4] \n\nCoefficients:\n          ma1     sma1\n      -0.5889  -0.9998\ns.e.   0.2090   0.4630\n\nsigma^2 = 0.001664:  log likelihood = 29.48\nAIC=-52.97   AICc=-51.25   BIC=-50.3\n\nTraining set error measures:\n                      ME       RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.006565141 0.03402407 0.0203669 0.08520986 0.2742404 0.3072039\n                    ACF1\nTraining set -0.02228731\n\n\n\n\nAs we can see from the forecast plot, the GDP Contribution of the Healthcare Industry is expected to increase in the next several years.\n\n\nCode\nsarima.for(lg.gdp.ts, 60, 0,1,1,0,1,1,4)\n\n\n\n\n\n$pred\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023                            7.641472\n2024 7.652795 7.638895 7.678722 7.696242\n2025 7.707565 7.693665 7.733491 7.751012\n2026 7.762335 7.748435 7.788261 7.805782\n2027 7.817105 7.803205 7.843031 7.860552\n2028 7.871875 7.857975 7.897801 7.915322\n2029 7.926645 7.912745 7.952571 7.970092\n2030 7.981414 7.967515 8.007341 8.024862\n2031 8.036184 8.022285 8.062111 8.079631\n2032 8.090954 8.077055 8.116881 8.134401\n2033 8.145724 8.131824 8.171651 8.189171\n2034 8.200494 8.186594 8.226421 8.243941\n2035 8.255264 8.241364 8.281190 8.298711\n2036 8.310034 8.296134 8.335960 8.353481\n2037 8.364804 8.350904 8.390730 8.408251\n2038 8.419574 8.405674 8.445500         \n\n$se\n           Qtr1       Qtr2       Qtr3       Qtr4\n2023                                  0.04194819\n2024 0.04536532 0.04844252 0.05133560 0.05672365\n2025 0.06034676 0.06334725 0.06621190 0.07135855\n2026 0.07511325 0.07807444 0.08092735 0.08592475\n2027 0.08977092 0.09270875 0.09555630 0.10045213\n2028 0.10436563 0.10728819 0.11013321 0.11495542\n2029 0.11892057 0.12183245 0.12467635 0.12944271\n2030 0.13344874 0.13635281 0.13919630 0.14391883\n2031 0.14795804 0.15085615 0.15369963 0.15838685\n2032 0.16245351 0.16534696 0.16819064 0.17284880\n2033 0.17693857 0.17982827 0.18267226 0.18730609\n2034 0.19141556 0.19430220 0.19714657 0.20175972\n2035 0.20588621 0.20877028 0.21161506 0.21621043\n2036 0.22035174 0.22323366 0.22607885 0.23065875\n2037 0.23481312 0.23769320 0.24053879 0.24510512\n2038 0.24927106 0.25214954 0.25499553           \n\n\n\n\n\n\nCode\nfit_gdp_bench &lt;- Arima(lg.gdp.ts, order=c(0,1,1), seasonal=c(0,1,1), include.drift=FALSE) \nautoplot(lg.gdp.ts) +\n  autolayer(meanf(lg.gdp.ts, h=12), series=\"Mean\", PI=FALSE) +\n  autolayer(naive(lg.gdp.ts, h=12), series=\"Naïve\", PI=FALSE) +\n  autolayer(rwf(lg.gdp.ts, drift=TRUE, h=12), series=\"Drift\", PI=FALSE) +\n  autolayer(forecast(fit_gdp_bench, 12), series=\"Arima\", PI=FALSE) +\n  theme_bw() +\n  ggtitle(\"Benchmark Methods Comparison of Healthcare Industry GDP Contribution\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAs we can see from the above plot, the ARIMA model outperforms the benchmark methods in forecasting the log-transformed GDP contribution of the healthcare industry. It not only shows the increasing trend but also captures the fluctuations. The Drift method only captures the increasing trend, while the Naïve and Mean methods show flat lines.\n\n\n\n\nCode\ns &lt;- 4\n\n# The length of the time series\nlen &lt;- length(lg.gdp.ts)\nst &lt;- tsp(lg.gdp.ts)[1]+(s-1)/s\n\n# Preallocate vectors to store forecasts and actual values\none_step_forecasts &lt;- vector(\"numeric\", len-s)\ns_step_forecasts &lt;- vector(\"numeric\", len-s)\nactuals &lt;- vector(\"numeric\", len-s)\n\n# Loop over the time series\nfor (i in 1:(len - s)) {\n  # Fit the model on the data up to the current point\n  model &lt;- auto.arima(window(lg.gdp.ts, end=st + i +s-1))\n  \n  # Forecast 1 step ahead\n  one_step_forecasts[i] &lt;- forecast(model, h=1)$mean\n  \n  # Forecast s steps ahead\n  s_step_forecasts[i] &lt;- forecast(model, h=s)$mean[s]\n  \n  # Store the actual value for the current point\n  actuals[i] &lt;- lg.gdp.ts[i+s]\n}\n\n# Calculate errors for 1-step forecasts\none_step_errors &lt;- actuals - one_step_forecasts\none_step_mae &lt;- mean(abs(one_step_errors))\none_step_rmse &lt;- sqrt(mean(one_step_errors^2))\n\n# Calculate errors for s-step forecasts\ns_step_errors &lt;- actuals - s_step_forecasts\ns_step_mae &lt;- mean(abs(s_step_errors))\ns_step_rmse &lt;- sqrt(mean(s_step_errors^2))\n\n# Print the error metrics\ncat(\"1-Step Forecast MAE of Healthcare Industry GDP Contribution:\", one_step_mae, \"\\n\")\n\n\n1-Step Forecast MAE of Healthcare Industry GDP Contribution: 0.1530439 \n\n\nCode\ncat(\"1-Step Forecast RMSE of Healthcare Industry GDP Contribution::\", one_step_rmse, \"\\n\")\n\n\n1-Step Forecast RMSE of Healthcare Industry GDP Contribution:: 0.1753251 \n\n\nCode\ncat(\"S-Step Forecast MAE of Healthcare Industry GDP Contribution:\", s_step_mae, \"\\n\")\n\n\nS-Step Forecast MAE of Healthcare Industry GDP Contribution: 0.1911874 \n\n\nCode\ncat(\"S-Step Forecast RMSE of Healthcare Industry GDP Contribution:\", s_step_rmse, \"\\n\")\n\n\nS-Step Forecast RMSE of Healthcare Industry GDP Contribution: 0.2094848 \n\n\n\n\n\nBased on the AIC, BIC, and AICc values, I found the best model for the log-transformed GDP contribution of the healthcare industry is ARIMA(0,1,1)(0,1,1)[4]. Then we can see from the forecasting plot above, the GDP contribution of the healthcare industry is expected to increase in the next several years, although there was a drop in 2020 due to the COVID-19 pandemic. What’s more, we can see the increase trend of the GDP contribution of the healthcare industry is becoming more dramatic after COVID-19. To conclude, the healthcare industry in the US is expected to keep growing in the future. And the Healthcare is becoming more and more important in the US economy after the COVID-19 pandemic."
  },
  {
    "objectID": "multi_TS_model.html#cross-validation",
    "href": "multi_TS_model.html#cross-validation",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Cross-validation",
    "text": "Cross-validation\n\n\nCode\nfolds = 5 \nbest_model &lt;- NULL\nbest_performance &lt;- Inf \n\nfold_s &lt;- floor(nrow(stock_ts)/folds)\n\nfor(fold in 1:folds){\n  start &lt;- (fold-1)*fold_s+1\n  end &lt;- fold*fold_s\n  \n  train_model &lt;- stock_ts[-(start:end), ]\n  test_model &lt;- stock_ts[start:end, ]\n  \n  sel &lt;- VARselect(train_model, lag.max = 10, type = \"both\")\n  best_lag &lt;- sel$selection[1]\n  \n  fit &lt;- vars::VAR(train_model, p=best_lag, type= \"both\", season = NULL, exog = NULL)\n  \n  h &lt;- nrow(test_model)\n  pred &lt;- predict(fit, n.ahead = h)\n  \n  pred_pharma &lt;- pred$fcst$pharma[,1]\n  mse &lt;- mean((pred_pharma - test_model[, \"pharma\"])^2)\n  \n  if(mse &lt; best_performance){\n    best_model &lt;- fit\n    best_performance &lt;- mse\n  }\n}\n\nprint(best_model)\n\n\n\nVAR Estimation Results:\n======================= \n\nEstimated coefficients for equation pharma: \n=========================================== \nCall:\npharma = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n        pharma.l1        biotech.l1           drug.l1  small_biotech.l1 \n     9.117665e-01     -6.174730e-03     -4.620247e-02     -9.379827e-03 \n        pharma.l2        biotech.l2           drug.l2  small_biotech.l2 \n     5.891521e-02      4.858987e-03      6.824120e-02      1.122680e-02 \n        pharma.l3        biotech.l3           drug.l3  small_biotech.l3 \n     7.405557e-02      3.985338e-03      8.983826e-02      2.223404e-02 \n        pharma.l4        biotech.l4           drug.l4  small_biotech.l4 \n    -5.564870e-02     -1.584910e-03     -2.196194e-01     -2.726915e-02 \n        pharma.l5        biotech.l5           drug.l5  small_biotech.l5 \n     5.043006e-02     -3.636957e-03      2.846027e-01     -1.061046e-03 \n        pharma.l6        biotech.l6           drug.l6  small_biotech.l6 \n    -8.096492e-02      1.181713e-03     -2.548818e-01      1.030590e-02 \n        pharma.l7        biotech.l7           drug.l7  small_biotech.l7 \n     1.083596e-02     -1.098131e-03      5.333776e-02      5.193421e-04 \n        pharma.l8        biotech.l8           drug.l8  small_biotech.l8 \n     3.870538e-02      2.794761e-03     -1.324374e-01     -1.364559e-02 \n        pharma.l9        biotech.l9           drug.l9  small_biotech.l9 \n     9.382633e-02      7.693035e-03      1.943561e-01      3.164221e-02 \n       pharma.l10       biotech.l10          drug.l10 small_biotech.l10 \n    -1.214848e-01     -9.891632e-03     -8.693206e-02     -2.710208e-02 \n            const             trend \n     1.753132e+00      4.014688e-05 \n\n\nEstimated coefficients for equation biotech: \n============================================ \nCall:\nbiotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n        pharma.l1        biotech.l1           drug.l1  small_biotech.l1 \n    -0.5603306645      0.9384455210     -1.5732517374     -0.1563497391 \n        pharma.l2        biotech.l2           drug.l2  small_biotech.l2 \n     0.6877250945      0.0759534054      0.7936897048      0.1992331832 \n        pharma.l3        biotech.l3           drug.l3  small_biotech.l3 \n     0.2794836234     -0.0926747450      2.5931096752     -0.1165575651 \n        pharma.l4        biotech.l4           drug.l4  small_biotech.l4 \n    -0.4791585614      0.0564415325     -1.4548768296      0.0666772231 \n        pharma.l5        biotech.l5           drug.l5  small_biotech.l5 \n     0.2757940834     -0.0118177078      0.0830405481     -0.1204064763 \n        pharma.l6        biotech.l6           drug.l6  small_biotech.l6 \n    -1.1191541172     -0.0459469315     -0.7444416771      0.0962860084 \n        pharma.l7        biotech.l7           drug.l7  small_biotech.l7 \n     1.0805980363      0.1380717168     -1.4345872167      0.2062132356 \n        pharma.l8        biotech.l8           drug.l8  small_biotech.l8 \n    -0.9367496270     -0.1308079380      0.3848334547     -0.3876458109 \n        pharma.l9        biotech.l9           drug.l9  small_biotech.l9 \n     1.0299311841      0.1154363053      1.2134220914      0.4162263673 \n       pharma.l10       biotech.l10          drug.l10 small_biotech.l10 \n    -0.3124133610     -0.0640046027     -0.2559666933     -0.2458310725 \n            const             trend \n    13.4087899738     -0.0002802438 \n\n\nEstimated coefficients for equation drug: \n========================================= \nCall:\ndrug = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n        pharma.l1        biotech.l1           drug.l1  small_biotech.l1 \n    -1.333564e-02     -5.425330e-04      9.746523e-01      3.567499e-03 \n        pharma.l2        biotech.l2           drug.l2  small_biotech.l2 \n     2.515844e-02      4.449199e-03     -6.770875e-02      4.465739e-03 \n        pharma.l3        biotech.l3           drug.l3  small_biotech.l3 \n    -2.309067e-02     -3.152273e-03      8.745803e-02     -1.515696e-03 \n        pharma.l4        biotech.l4           drug.l4  small_biotech.l4 \n     1.849273e-03     -1.774644e-03      4.380933e-02     -1.210272e-02 \n        pharma.l5        biotech.l5           drug.l5  small_biotech.l5 \n     7.899723e-03      7.705716e-04      8.757133e-03      3.833708e-03 \n        pharma.l6        biotech.l6           drug.l6  small_biotech.l6 \n     4.121565e-03     -1.895377e-03     -9.936169e-02      4.205722e-03 \n        pharma.l7        biotech.l7           drug.l7  small_biotech.l7 \n    -1.449116e-02      3.050708e-03      4.631025e-02      7.569039e-03 \n        pharma.l8        biotech.l8           drug.l8  small_biotech.l8 \n     8.258391e-03     -2.783682e-03     -2.754795e-02     -1.270100e-02 \n        pharma.l9        biotech.l9           drug.l9  small_biotech.l9 \n     4.974873e-02      5.404291e-03     -6.666864e-02      7.925408e-03 \n       pharma.l10       biotech.l10          drug.l10 small_biotech.l10 \n    -5.130292e-02     -3.461710e-03      4.851342e-02     -4.298687e-03 \n            const             trend \n     6.389161e-01     -5.949131e-06 \n\n\nEstimated coefficients for equation small_biotech: \n================================================== \nCall:\nsmall_biotech = pharma.l1 + biotech.l1 + drug.l1 + small_biotech.l1 + pharma.l2 + biotech.l2 + drug.l2 + small_biotech.l2 + pharma.l3 + biotech.l3 + drug.l3 + small_biotech.l3 + pharma.l4 + biotech.l4 + drug.l4 + small_biotech.l4 + pharma.l5 + biotech.l5 + drug.l5 + small_biotech.l5 + pharma.l6 + biotech.l6 + drug.l6 + small_biotech.l6 + pharma.l7 + biotech.l7 + drug.l7 + small_biotech.l7 + pharma.l8 + biotech.l8 + drug.l8 + small_biotech.l8 + pharma.l9 + biotech.l9 + drug.l9 + small_biotech.l9 + pharma.l10 + biotech.l10 + drug.l10 + small_biotech.l10 + const + trend \n\n        pharma.l1        biotech.l1           drug.l1  small_biotech.l1 \n    -1.255719e-01      4.784693e-03      6.101546e-02      1.031793e+00 \n        pharma.l2        biotech.l2           drug.l2  small_biotech.l2 \n     2.010068e-01     -9.191466e-03      3.143891e-03     -4.606482e-02 \n        pharma.l3        biotech.l3           drug.l3  small_biotech.l3 \n    -4.455854e-02      1.709482e-02     -3.311572e-01      6.507303e-03 \n        pharma.l4        biotech.l4           drug.l4  small_biotech.l4 \n     4.939498e-02     -6.469650e-03      2.843153e-01     -2.186570e-03 \n        pharma.l5        biotech.l5           drug.l5  small_biotech.l5 \n    -2.453230e-01     -1.537032e-02      2.435175e-01     -4.715411e-02 \n        pharma.l6        biotech.l6           drug.l6  small_biotech.l6 \n     7.318229e-02      5.841038e-04     -6.137202e-01      3.949530e-02 \n        pharma.l7        biotech.l7           drug.l7  small_biotech.l7 \n     4.739950e-02      1.180873e-02      3.161175e-01      1.067488e-02 \n        pharma.l8        biotech.l8           drug.l8  small_biotech.l8 \n     6.944550e-02     -1.424331e-02      4.064763e-01     -5.700921e-03 \n        pharma.l9        biotech.l9           drug.l9  small_biotech.l9 \n     8.944801e-02      1.629836e-02     -2.317581e-01      5.973843e-02 \n       pharma.l10       biotech.l10          drug.l10 small_biotech.l10 \n    -1.143154e-01     -2.190694e-03     -1.635641e-01     -5.157195e-02 \n            const             trend \n    -3.698878e-01      9.931225e-05"
  },
  {
    "objectID": "multi_TS_model.html#model-fitting",
    "href": "multi_TS_model.html#model-fitting",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Model Fitting",
    "text": "Model Fitting"
  },
  {
    "objectID": "multi_TS_model.html#forecasting",
    "href": "multi_TS_model.html#forecasting",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "Forecasting",
    "text": "Forecasting\n\n\nCode\n(fit.pr = predict(fitvar1, n.ahead = 365, ci = 0.95))\n\n\n$pharma\n           fcst    lower    upper        CI\n  [1,] 39.62720 38.38612 40.86827  1.241073\n  [2,] 39.69115 37.97387 41.40844  1.717284\n  [3,] 39.89844 37.82447 41.97240  2.073967\n  [4,] 39.89315 37.47575 42.31055  2.417400\n  [5,] 40.00419 37.30101 42.70736  2.703174\n  [6,] 40.07167 37.08495 43.05839  2.986720\n  [7,] 40.24652 37.02115 43.47189  3.225372\n  [8,] 40.26563 36.82269 43.70857  3.442942\n  [9,] 40.33798 36.70399 43.97197  3.633990\n [10,] 40.47721 36.62486 44.32956  3.852347\n [11,] 40.53096 36.48525 44.57667  4.045712\n [12,] 40.64180 36.41486 44.86874  4.226943\n [13,] 40.69543 36.29380 45.09705  4.401629\n [14,] 40.77900 36.21562 45.34239  4.563387\n [15,] 40.84365 36.12278 45.56452  4.720870\n [16,] 40.92756 36.05940 45.79571  4.868155\n [17,] 40.98579 35.97659 45.99499  5.009202\n [18,] 41.04157 35.89862 46.18453  5.142957\n [19,] 41.11505 35.83923 46.39087  5.275822\n [20,] 41.16301 35.75978 46.56624  5.403233\n [21,] 41.23003 35.70374 46.75632  5.526289\n [22,] 41.27612 35.62983 46.92242  5.646297\n [23,] 41.33227 35.56981 47.09473  5.762462\n [24,] 41.38186 35.50574 47.25797  5.876118\n [25,] 41.43330 35.44678 47.41982  5.986516\n [26,] 41.48133 35.38701 47.57565  6.094318\n [27,] 41.52421 35.32471 47.72371  6.199501\n [28,] 41.57407 35.27133 47.87680  6.302737\n [29,] 41.61436 35.21070 48.01803  6.403663\n [30,] 41.66029 35.15781 48.16277  6.502481\n [31,] 41.69966 35.10028 48.29904  6.599379\n [32,] 41.74108 35.04676 48.43540  6.694318\n [33,] 41.78070 34.99321 48.56818  6.787488\n [34,] 41.81887 34.94000 48.69773  6.878868\n [35,] 41.85732 34.88876 48.82588  6.968559\n [36,] 41.89250 34.83591 48.94910  7.056596\n [37,] 41.92977 34.78667 49.07287  7.143099\n [38,] 41.96348 34.73542 49.19154  7.228059\n [39,] 41.99836 34.68681 49.30990  7.311548\n [40,] 42.03107 34.63747 49.42468  7.393605\n [41,] 42.06348 34.58921 49.53776  7.474277\n [42,] 42.09533 34.54173 49.64894  7.553609\n [43,] 42.12577 34.49414 49.75741  7.631636\n [44,] 42.15629 34.44789 49.86469  7.708400\n [45,] 42.18505 34.40111 49.96898  7.783934\n [46,] 42.21407 34.35578 50.07236  7.858288\n [47,] 42.24158 34.31010 50.17306  7.931481\n [48,] 42.26887 34.26531 50.27243  8.003559\n [49,] 42.29523 34.22068 50.36977  8.074545\n [50,] 42.32085 34.17637 50.46532  8.144477\n [51,] 42.34602 34.13264 50.55940  8.213383\n [52,] 42.37017 34.08888 50.65146  8.281291\n [53,] 42.39401 34.04577 50.74224  8.348232\n [54,] 42.41682 34.00259 50.83105  8.414228\n [55,] 42.43927 33.95996 50.91858  8.479311\n [56,] 42.46088 33.91738 51.00438  8.543500\n [57,] 42.48192 33.87510 51.08874  8.606823\n [58,] 42.50233 33.83303 51.17163  8.669300\n [59,] 42.52202 33.79106 51.25297  8.730955\n [60,] 42.54121 33.74940 51.33302  8.791808\n [61,] 42.55964 33.70776 51.41152  8.851879\n [62,] 42.57758 33.66639 51.48877  8.911189\n [63,] 42.59481 33.62506 51.56456  8.969753\n [64,] 42.61150 33.58391 51.63910  9.027592\n [65,] 42.62757 33.54285 51.71229  9.084721\n [66,] 42.64304 33.50188 51.78419  9.141158\n [67,] 42.65795 33.46103 51.85487  9.196917\n [68,] 42.67223 33.42022 51.92425  9.252014\n [69,] 42.68600 33.37954 51.99246  9.306463\n [70,] 42.69914 33.33887 52.05942  9.360276\n [71,] 42.71177 33.29830 52.12524  9.413469\n [72,] 42.72381 33.25775 52.18986  9.466052\n [73,] 42.73531 33.21727 52.25335  9.518039\n [74,] 42.74627 33.17683 52.31571  9.569441\n [75,] 42.75667 33.13641 52.37694  9.620268\n [76,] 42.76657 33.09603 52.43710  9.670533\n [77,] 42.77591 33.05567 52.49616  9.720244\n [78,] 42.78475 33.01534 52.55417  9.769411\n [79,] 42.79307 32.97502 52.61111  9.818045\n [80,] 42.80088 32.93472 52.66703  9.866155\n [81,] 42.80818 32.89444 52.72193  9.913748\n [82,] 42.81499 32.85416 52.77582  9.960834\n [83,] 42.82131 32.81389 52.82873 10.007420\n [84,] 42.82714 32.77362 52.88065 10.053516\n [85,] 42.83249 32.73336 52.93162 10.099127\n [86,] 42.83736 32.69310 52.98162 10.144261\n [87,] 42.84177 32.65285 53.03070 10.188927\n [88,] 42.84572 32.61259 53.07885 10.233129\n [89,] 42.84920 32.57233 53.12608 10.276876\n [90,] 42.85224 32.53207 53.17241 10.320172\n [91,] 42.85483 32.49181 53.21786 10.363026\n [92,] 42.85698 32.45154 53.26243 10.405441\n [93,] 42.85870 32.41128 53.30613 10.447425\n [94,] 42.85999 32.37101 53.34898 10.488983\n [95,] 42.86086 32.33074 53.39098 10.530120\n [96,] 42.86131 32.29047 53.43215 10.570842\n [97,] 42.86135 32.25019 53.47250 10.611154\n [98,] 42.86098 32.20992 53.51204 10.651061\n [99,] 42.86021 32.16965 53.55078 10.690567\n[100,] 42.85905 32.12937 53.58873 10.729678\n[101,] 42.85750 32.08910 53.62590 10.768398\n[102,] 42.85556 32.04883 53.66230 10.806731\n[103,] 42.85325 32.00857 53.69793 10.844683\n[104,] 42.85057 31.96831 53.73282 10.882256\n[105,] 42.84751 31.92806 53.76697 10.919456\n[106,] 42.84410 31.88781 53.80038 10.956287\n[107,] 42.84033 31.84757 53.83308 10.992752\n[108,] 42.83620 31.80735 53.86506 11.028855\n[109,] 42.83173 31.76713 53.89633 11.064601\n[110,] 42.82692 31.72693 53.92692 11.099993\n[111,] 42.82178 31.68674 53.95681 11.135034\n[112,] 42.81630 31.64657 53.98603 11.169728\n[113,] 42.81050 31.60642 54.01458 11.204079\n[114,] 42.80438 31.56629 54.04247 11.238090\n[115,] 42.79794 31.52617 54.06970 11.271764\n[116,] 42.79119 31.48608 54.09630 11.305105\n[117,] 42.78414 31.44602 54.12225 11.338117\n[118,] 42.77678 31.40598 54.14758 11.370801\n[119,] 42.76913 31.36597 54.17230 11.403161\n[120,] 42.76120 31.32599 54.19640 11.435201\n[121,] 42.75297 31.28605 54.21990 11.466924\n[122,] 42.74447 31.24614 54.24280 11.498331\n[123,] 42.73569 31.20626 54.26511 11.529428\n[124,] 42.72663 31.16642 54.28685 11.560215\n[125,] 42.71732 31.12662 54.30801 11.590696\n[126,] 42.70774 31.08686 54.32861 11.620874\n[127,] 42.69790 31.04715 54.34865 11.650752\n[128,] 42.68781 31.00748 54.36814 11.680332\n[129,] 42.67747 30.96786 54.38709 11.709616\n[130,] 42.66689 30.92828 54.40550 11.738609\n[131,] 42.65607 30.88876 54.42339 11.767312\n[132,] 42.64502 30.84929 54.44075 11.795727\n[133,] 42.63374 30.80988 54.45760 11.823858\n[134,] 42.62223 30.77052 54.47394 11.851707\n[135,] 42.61050 30.73123 54.48978 11.879275\n[136,] 42.59856 30.69199 54.50512 11.906567\n[137,] 42.58640 30.65281 54.51998 11.933584\n[138,] 42.57403 30.61370 54.53436 11.960329\n[139,] 42.56146 30.57466 54.54827 11.986803\n[140,] 42.54869 30.53568 54.56170 12.013010\n[141,] 42.53573 30.49678 54.57468 12.038951\n[142,] 42.52258 30.45795 54.58720 12.064629\n[143,] 42.50923 30.41919 54.59928 12.090046\n[144,] 42.49571 30.38050 54.61091 12.115205\n[145,] 42.48200 30.34190 54.62211 12.140107\n[146,] 42.46812 30.30337 54.63288 12.164755\n[147,] 42.45407 30.26492 54.64322 12.189151\n[148,] 42.43986 30.22656 54.65315 12.213297\n[149,] 42.42548 30.18828 54.66267 12.237196\n[150,] 42.41094 30.15009 54.67178 12.260849\n[151,] 42.39624 30.11198 54.68050 12.284258\n[152,] 42.38139 30.07397 54.68882 12.307426\n[153,] 42.36640 30.03604 54.69675 12.330354\n[154,] 42.35126 29.99821 54.70430 12.353046\n[155,] 42.33598 29.96048 54.71148 12.375501\n[156,] 42.32056 29.92284 54.71829 12.397724\n[157,] 42.30501 29.88530 54.72473 12.419715\n[158,] 42.28933 29.84786 54.73081 12.441477\n[159,] 42.27353 29.81051 54.73654 12.463012\n[160,] 42.25760 29.77328 54.74192 12.484321\n[161,] 42.24155 29.73614 54.74696 12.505407\n[162,] 42.22539 29.69912 54.75166 12.526271\n[163,] 42.20911 29.66220 54.75603 12.546915\n[164,] 42.19273 29.62539 54.76007 12.567342\n[165,] 42.17624 29.58868 54.76379 12.587553\n[166,] 42.15964 29.55210 54.76719 12.607549\n[167,] 42.14295 29.51562 54.77029 12.627334\n[168,] 42.12617 29.47926 54.77307 12.646908\n[169,] 42.10929 29.44301 54.77556 12.666273\n[170,] 42.09232 29.40689 54.77775 12.685432\n[171,] 42.07526 29.37088 54.77965 12.704385\n[172,] 42.05812 29.33499 54.78126 12.723136\n[173,] 42.04090 29.29922 54.78259 12.741685\n[174,] 42.02361 29.26357 54.78364 12.760034\n[175,] 42.00624 29.22805 54.78442 12.778185\n[176,] 41.98880 29.19266 54.78494 12.796141\n[177,] 41.97129 29.15739 54.78519 12.813901\n[178,] 41.95371 29.12225 54.78518 12.831469\n[179,] 41.93608 29.08723 54.78492 12.848846\n[180,] 41.91838 29.05235 54.78441 12.866033\n[181,] 41.90063 29.01760 54.78366 12.883032\n[182,] 41.88282 28.98298 54.78267 12.899846\n[183,] 41.86496 28.94849 54.78144 12.916474\n[184,] 41.84706 28.91414 54.77998 12.932921\n[185,] 41.82911 28.87992 54.77829 12.949185\n[186,] 41.81111 28.84584 54.77638 12.965271\n[187,] 41.79308 28.81190 54.77425 12.981178\n[188,] 41.77500 28.77809 54.77191 12.996909\n[189,] 41.75689 28.74443 54.76936 13.012465\n[190,] 41.73875 28.71090 54.76660 13.027847\n[191,] 41.72058 28.67752 54.76364 13.043058\n[192,] 41.70238 28.64428 54.76048 13.058099\n[193,] 41.68415 28.61118 54.75712 13.072972\n[194,] 41.66590 28.57823 54.75358 13.087677\n[195,] 41.64764 28.54542 54.74985 13.102217\n[196,] 41.62935 28.51276 54.74594 13.116592\n[197,] 41.61104 28.48024 54.74185 13.130805\n[198,] 41.59273 28.44787 54.73758 13.144857\n[199,] 41.57440 28.41565 54.73315 13.158750\n[200,] 41.55606 28.38358 54.72854 13.172484\n[201,] 41.53771 28.35165 54.72378 13.186062\n[202,] 41.51936 28.31988 54.71885 13.199484\n[203,] 41.50101 28.28826 54.71376 13.212753\n[204,] 41.48266 28.25679 54.70853 13.225869\n[205,] 41.46431 28.22547 54.70314 13.238834\n[206,] 41.44596 28.19431 54.69761 13.251650\n[207,] 41.42761 28.16330 54.69193 13.264318\n[208,] 41.40928 28.13244 54.68612 13.276839\n[209,] 41.39095 28.10174 54.68017 13.289214\n[210,] 41.37264 28.07119 54.67408 13.301446\n[211,] 41.35434 28.04080 54.66787 13.313535\n[212,] 41.33605 28.01057 54.66153 13.325483\n[213,] 41.31778 27.98049 54.65507 13.337291\n[214,] 41.29953 27.95057 54.64849 13.348960\n[215,] 41.28130 27.92081 54.64179 13.360492\n[216,] 41.26309 27.89120 54.63498 13.371888\n[217,] 41.24491 27.86176 54.62806 13.383149\n[218,] 41.22675 27.83247 54.62103 13.394277\n[219,] 41.20862 27.80335 54.61389 13.405273\n[220,] 41.19052 27.77438 54.60666 13.416139\n[221,] 41.17245 27.74558 54.59932 13.426874\n[222,] 41.15441 27.71693 54.59189 13.437482\n[223,] 41.13641 27.68845 54.58437 13.447962\n[224,] 41.11844 27.66012 54.57676 13.458317\n[225,] 41.10051 27.63196 54.56906 13.468547\n[226,] 41.08262 27.60396 54.56127 13.478654\n[227,] 41.06477 27.57613 54.55341 13.488639\n[228,] 41.04696 27.54845 54.54546 13.498503\n[229,] 41.02919 27.52094 54.53744 13.508248\n[230,] 41.01147 27.49359 54.52934 13.517873\n[231,] 40.99379 27.46641 54.52117 13.527382\n[232,] 40.97616 27.43939 54.51294 13.536775\n[233,] 40.95858 27.41253 54.50463 13.546052\n[234,] 40.94105 27.38583 54.49627 13.555216\n[235,] 40.92357 27.35930 54.48784 13.564267\n[236,] 40.90614 27.33294 54.47935 13.573207\n[237,] 40.88877 27.30673 54.47081 13.582036\n[238,] 40.87145 27.28070 54.46221 13.590756\n[239,] 40.85419 27.25482 54.45356 13.599368\n[240,] 40.83699 27.22911 54.44486 13.607873\n[241,] 40.81984 27.20357 54.43611 13.616271\n[242,] 40.80275 27.17819 54.42732 13.624566\n[243,] 40.78573 27.15297 54.41848 13.632756\n[244,] 40.76876 27.12792 54.40961 13.640843\n[245,] 40.75186 27.10303 54.40069 13.648829\n[246,] 40.73502 27.07831 54.39174 13.656715\n[247,] 40.71825 27.05375 54.38275 13.664501\n[248,] 40.70155 27.02936 54.37373 13.672188\n[249,] 40.68491 27.00513 54.36468 13.679778\n[250,] 40.66834 26.98106 54.35561 13.687272\n[251,] 40.65183 26.95716 54.34650 13.694670\n[252,] 40.63540 26.93343 54.33737 13.701974\n[253,] 40.61904 26.90986 54.32822 13.709184\n[254,] 40.60275 26.88645 54.31905 13.716302\n[255,] 40.58653 26.86320 54.30986 13.723328\n[256,] 40.57039 26.84012 54.30065 13.730265\n[257,] 40.55432 26.81721 54.29143 13.737111\n[258,] 40.53832 26.79445 54.28219 13.743869\n[259,] 40.52240 26.77186 54.27294 13.750540\n[260,] 40.50656 26.74944 54.26369 13.757124\n[261,] 40.49080 26.72717 54.25442 13.763622\n[262,] 40.47511 26.70507 54.24514 13.770036\n[263,] 40.45950 26.68314 54.23587 13.776365\n[264,] 40.44397 26.66136 54.22658 13.782612\n[265,] 40.42852 26.63975 54.21730 13.788777\n[266,] 40.41315 26.61829 54.20802 13.794860\n[267,] 40.39787 26.59700 54.19873 13.800864\n[268,] 40.38266 26.57588 54.18945 13.806788\n[269,] 40.36754 26.55491 54.18018 13.812634\n[270,] 40.35250 26.53410 54.17090 13.818402\n[271,] 40.33755 26.51345 54.16164 13.824093\n[272,] 40.32268 26.49297 54.15239 13.829709\n[273,] 40.30789 26.47264 54.14314 13.835250\n[274,] 40.29319 26.45248 54.13391 13.840716\n[275,] 40.27858 26.43247 54.12469 13.846109\n[276,] 40.26405 26.41262 54.11548 13.851430\n[277,] 40.24961 26.39293 54.10629 13.856679\n[278,] 40.23526 26.37340 54.09711 13.861857\n[279,] 40.22099 26.35403 54.08796 13.866965\n[280,] 40.20681 26.33481 54.07882 13.872004\n[281,] 40.19272 26.31575 54.06970 13.876974\n[282,] 40.17873 26.29685 54.06060 13.881876\n[283,] 40.16482 26.27810 54.05153 13.886712\n[284,] 40.15100 26.25951 54.04248 13.891482\n[285,] 40.13727 26.24108 54.03345 13.896186\n[286,] 40.12363 26.22280 54.02445 13.900825\n[287,] 40.11008 26.20468 54.01548 13.905401\n[288,] 40.09662 26.18671 54.00653 13.909914\n[289,] 40.08325 26.16889 53.99762 13.914364\n[290,] 40.06998 26.15123 53.98873 13.918752\n[291,] 40.05680 26.13372 53.97988 13.923080\n[292,] 40.04371 26.11636 53.97106 13.927347\n[293,] 40.03071 26.09916 53.96227 13.931555\n[294,] 40.01781 26.08210 53.95351 13.935704\n[295,] 40.00500 26.06520 53.94479 13.939795\n[296,] 39.99228 26.04845 53.93611 13.943829\n[297,] 39.97965 26.03185 53.92746 13.947806\n[298,] 39.96712 26.01539 53.91885 13.951727\n[299,] 39.95468 25.99909 53.91028 13.955592\n[300,] 39.94234 25.98294 53.90174 13.959403\n[301,] 39.93009 25.96693 53.89325 13.963160\n[302,] 39.91794 25.95107 53.88480 13.966863\n[303,] 39.90587 25.93536 53.87639 13.970514\n[304,] 39.89391 25.91980 53.86802 13.974112\n[305,] 39.88204 25.90438 53.85969 13.977659\n[306,] 39.87026 25.88910 53.85141 13.981156\n[307,] 39.85858 25.87397 53.84318 13.984601\n[308,] 39.84699 25.85899 53.83499 13.987998\n[309,] 39.83549 25.84415 53.82684 13.991345\n[310,] 39.82410 25.82945 53.81874 13.994644\n[311,] 39.81279 25.81490 53.81069 13.997896\n[312,] 39.80158 25.80048 53.80268 14.001100\n[313,] 39.79047 25.78621 53.79473 14.004257\n[314,] 39.77945 25.77208 53.78682 14.007368\n[315,] 39.76853 25.75809 53.77896 14.010434\n[316,] 39.75770 25.74424 53.77115 14.013456\n[317,] 39.74696 25.73053 53.76340 14.016432\n[318,] 39.73633 25.71696 53.75569 14.019365\n[319,] 39.72578 25.70353 53.74804 14.022255\n[320,] 39.71533 25.69023 53.74043 14.025102\n[321,] 39.70498 25.67707 53.73288 14.027907\n[322,] 39.69472 25.66404 53.72539 14.030671\n[323,] 39.68455 25.65116 53.71794 14.033393\n[324,] 39.67448 25.63840 53.71055 14.036075\n[325,] 39.66450 25.62578 53.70322 14.038717\n[326,] 39.65462 25.61330 53.69594 14.041320\n[327,] 39.64483 25.60095 53.68871 14.043883\n[328,] 39.63513 25.58873 53.68154 14.046408\n[329,] 39.62553 25.57664 53.67443 14.048896\n[330,] 39.61603 25.56468 53.66737 14.051345\n[331,] 39.60661 25.55286 53.66037 14.053758\n[332,] 39.59729 25.54116 53.65343 14.056134\n[333,] 39.58807 25.52959 53.64654 14.058474\n[334,] 39.57893 25.51815 53.63971 14.060779\n[335,] 39.56989 25.50684 53.63294 14.063048\n[336,] 39.56094 25.49566 53.62623 14.065283\n[337,] 39.55209 25.48460 53.61957 14.067484\n[338,] 39.54332 25.47367 53.61297 14.069651\n[339,] 39.53465 25.46287 53.60644 14.071784\n[340,] 39.52607 25.45219 53.59996 14.073885\n[341,] 39.51759 25.44163 53.59354 14.075954\n[342,] 39.50919 25.43120 53.58718 14.077991\n[343,] 39.50089 25.42089 53.58088 14.079996\n[344,] 39.49267 25.41070 53.57464 14.081970\n[345,] 39.48455 25.40064 53.56846 14.083913\n[346,] 39.47652 25.39069 53.56234 14.085826\n[347,] 39.46857 25.38086 53.55628 14.087709\n[348,] 39.46072 25.37116 53.55028 14.089563\n[349,] 39.45296 25.36157 53.54435 14.091388\n[350,] 39.44529 25.35210 53.53847 14.093184\n[351,] 39.43770 25.34275 53.53265 14.094952\n[352,] 39.43021 25.33351 53.52690 14.096692\n[353,] 39.42280 25.32440 53.52121 14.098405\n[354,] 39.41548 25.31539 53.51557 14.100091\n[355,] 39.40825 25.30650 53.51000 14.101749\n[356,] 39.40111 25.29773 53.50449 14.103382\n[357,] 39.39406 25.28907 53.49904 14.104988\n[358,] 39.38709 25.28052 53.49366 14.106569\n[359,] 39.38021 25.27208 53.48833 14.108125\n[360,] 39.37341 25.26376 53.48307 14.109656\n[361,] 39.36670 25.25554 53.47787 14.111162\n[362,] 39.36008 25.24744 53.47273 14.112644\n[363,] 39.35354 25.23944 53.46765 14.114102\n[364,] 39.34709 25.23156 53.46263 14.115537\n[365,] 39.34073 25.22378 53.45767 14.116948\n\n$biotech\n           fcst    lower    upper        CI\n  [1,] 186.0090 170.8866 201.1313  15.12238\n  [2,] 187.0992 166.4480 207.7505  20.65123\n  [3,] 189.8876 164.8162 214.9589  25.07139\n  [4,] 190.7168 162.4041 219.0294  28.31263\n  [5,] 192.5425 161.3769 223.7081  31.16560\n  [6,] 192.9315 159.1158 226.7471  33.81564\n  [7,] 195.2313 159.2801 231.1825  35.95118\n  [8,] 195.6505 157.4980 233.8031  38.15252\n  [9,] 196.7070 156.7150 236.6989  39.99197\n [10,] 198.2096 156.3016 240.1176  41.90801\n [11,] 198.8862 155.3061 242.4663  43.58012\n [12,] 200.2213 155.0600 245.3825  45.16127\n [13,] 200.7082 154.0621 247.3543  46.64611\n [14,] 201.9550 153.9128 249.9973  48.04223\n [15,] 202.6127 153.2012 252.0241  49.41147\n [16,] 203.7031 153.0406 254.3656  50.66252\n [17,] 204.5133 152.6190 256.4076  51.89430\n [18,] 205.2589 152.2109 258.3069  53.04803\n [19,] 206.2554 152.0776 260.4331  54.17775\n [20,] 206.8989 151.6367 262.1612  55.26224\n [21,] 207.8728 151.5587 264.1869  56.31410\n [22,] 208.5350 151.1972 265.8728  57.33776\n [23,] 209.4086 151.0812 267.7360  58.32741\n [24,] 210.1356 150.8371 269.4342  59.29855\n [25,] 210.9238 150.6882 271.1594  60.23563\n [26,] 211.7045 150.5512 272.8579  61.15335\n [27,] 212.3963 150.3513 274.4414  62.04509\n [28,] 213.2089 150.2920 276.1259  62.91694\n [29,] 213.8838 150.1165 277.6511  63.76734\n [30,] 214.6657 150.0686 279.2629  64.59714\n [31,] 215.3495 149.9410 280.7580  65.40847\n [32,] 216.0829 149.8827 282.2832  66.20024\n [33,] 216.7893 149.8140 283.7646  66.97529\n [34,] 217.4829 149.7512 285.2146  67.73172\n [35,] 218.1970 149.7251 286.6689  68.47190\n [36,] 218.8622 149.6669 288.0575  69.19528\n [37,] 219.5705 149.6678 289.4733  69.90275\n [38,] 220.2283 149.6337 290.8230  70.59464\n [39,] 220.9144 149.6431 292.1857  71.27128\n [40,] 221.5728 149.6394 293.5061  71.93332\n [41,] 222.2341 149.6532 294.8150  72.58090\n [42,] 222.8926 149.6779 296.1073  73.21469\n [43,] 223.5346 149.6998 297.3694  73.83478\n [44,] 224.1865 149.7448 298.6282  74.44172\n [45,] 224.8151 149.7793 299.8508  75.03575\n [46,] 225.4548 149.8376 301.0721  75.61727\n [47,] 226.0760 149.8894 302.2625  76.18657\n [48,] 226.6999 149.9559 303.4439  76.74399\n [49,] 227.3147 150.0249 304.6045  77.28983\n [50,] 227.9233 150.0989 305.7477  77.82441\n [51,] 228.5304 150.1824 306.8784  78.34802\n [52,] 229.1262 150.2653 307.9871  78.86093\n [53,] 229.7230 150.3595 309.0864  79.36344\n [54,] 230.3082 150.4524 310.1640  79.85581\n [55,] 230.8929 150.5546 311.2312  80.33830\n [56,] 231.4690 150.6578 312.2802  80.81116\n [57,] 232.0411 150.7664 313.3157  81.27465\n [58,] 232.6079 150.8789 314.3369  81.72898\n [59,] 233.1679 150.9935 315.3423  82.17441\n [60,] 233.7248 151.1136 316.3359  82.61115\n [61,] 234.2738 151.2344 317.3132  83.03943\n [62,] 234.8197 151.3603 318.2792  83.45944\n [63,] 235.3585 151.4871 319.2299  83.87139\n [64,] 235.8931 151.6177 320.1686  84.27550\n [65,] 236.4219 151.7500 321.0938  84.67192\n [66,] 236.9453 151.8845 322.0062  85.06087\n [67,] 237.4640 152.0215 322.9065  85.44251\n [68,] 237.9765 152.1595 323.7936  85.81703\n [69,] 238.4847 152.3002 324.6693  86.18457\n [70,] 238.9869 152.4415 325.5322  86.54532\n [71,] 239.4844 152.5850 326.3838  86.89943\n [72,] 239.9763 152.7293 327.2234  87.24705\n [73,] 240.4632 152.8749 328.0515  87.58832\n [74,] 240.9450 153.0216 328.8684  87.92340\n [75,] 241.4214 153.1690 329.6738  88.25242\n [76,] 241.8929 153.3174 330.4685  88.57551\n [77,] 242.3590 153.4662 331.2519  88.89282\n [78,] 242.8204 153.6159 332.0248  89.20445\n [79,] 243.2764 153.7658 332.7869  89.51055\n [80,] 243.7275 153.9163 333.5387  89.81122\n [81,] 244.1735 154.0669 334.2800  90.10658\n [82,] 244.6144 154.2177 335.0112  90.39676\n [83,] 245.0505 154.3686 335.7323  90.68184\n [84,] 245.4814 154.5195 336.4434  90.96195\n [85,] 245.9076 154.6704 337.1448  91.23718\n [86,] 246.3287 154.8210 337.8363  91.50764\n [87,] 246.7450 154.9715 338.5184  91.77343\n [88,] 247.1563 155.1217 339.1909  92.03463\n [89,] 247.5628 155.2715 339.8542  92.29135\n [90,] 247.9645 155.4208 340.5082  92.54367\n [91,] 248.3614 155.5697 341.1530  92.79168\n [92,] 248.7535 155.7180 341.7889  93.03546\n [93,] 249.1408 155.8657 342.4159  93.27510\n [94,] 249.5234 156.0127 343.0341  93.51069\n [95,] 249.9012 156.1590 343.6435  93.74229\n [96,] 250.2745 156.3045 344.2444  93.96999\n [97,] 250.6430 156.4491 344.8369  94.19386\n [98,] 251.0069 156.5929 345.4209  94.41397\n [99,] 251.3662 156.7358 345.9966  94.63040\n[100,] 251.7209 156.8777 346.5641  94.84322\n[101,] 252.0711 157.0186 347.1236  95.05250\n[102,] 252.4167 157.1584 347.6750  95.25829\n[103,] 252.7579 157.2972 348.2185  95.46067\n[104,] 253.0945 157.4348 348.7542  95.65970\n[105,] 253.4267 157.5713 349.2822  95.85544\n[106,] 253.7545 157.7065 349.8025  96.04796\n[107,] 254.0779 157.8406 350.3152  96.23730\n[108,] 254.3969 157.9734 350.8205  96.42354\n[109,] 254.7116 158.1049 351.3183  96.60672\n[110,] 255.0220 158.2350 351.8089  96.78691\n[111,] 255.3280 158.3639 352.2922  96.96415\n[112,] 255.6298 158.4914 352.7683  97.13850\n[113,] 255.9274 158.6174 353.2374  97.31001\n[114,] 256.2208 158.7421 353.6995  97.47873\n[115,] 256.5100 158.8653 354.1547  97.64472\n[116,] 256.7951 158.9871 354.6031  97.80801\n[117,] 257.0760 159.1073 355.0447  97.96866\n[118,] 257.3528 159.2261 355.4796  98.12672\n[119,] 257.6256 159.3434 355.9078  98.28222\n[120,] 257.8944 159.4591 356.3296  98.43523\n[121,] 258.1591 159.5733 356.7449  98.58577\n[122,] 258.4198 159.6859 357.1537  98.73389\n[123,] 258.6766 159.7970 357.5563  98.87963\n[124,] 258.9295 159.9064 357.9525  99.02305\n[125,] 259.1785 160.0143 358.3426  99.16416\n[126,] 259.4236 160.1205 358.7266  99.30303\n[127,] 259.6648 160.2252 359.1045  99.43967\n[128,] 259.9023 160.3281 359.4764  99.57414\n[129,] 260.1359 160.4295 359.8424  99.70647\n[130,] 260.3658 160.5291 360.2025  99.83670\n[131,] 260.5920 160.6272 360.5569  99.96486\n[132,] 260.8145 160.7235 360.9055 100.09098\n[133,] 261.0333 160.8182 361.2484 100.21511\n[134,] 261.2485 160.9112 361.5858 100.33728\n[135,] 261.4600 161.0025 361.9176 100.45751\n[136,] 261.6680 161.0922 362.2439 100.57585\n[137,] 261.8724 161.1801 362.5648 100.69232\n[138,] 262.0733 161.2664 362.8803 100.80696\n[139,] 262.2707 161.3509 363.1905 100.91980\n[140,] 262.4646 161.4337 363.4955 101.03086\n[141,] 262.6551 161.5149 363.7953 101.14018\n[142,] 262.8421 161.5943 364.0899 101.24779\n[143,] 263.0258 161.6721 364.3795 101.35372\n[144,] 263.2061 161.7481 364.6641 101.45798\n[145,] 263.3831 161.8224 364.9437 101.56062\n[146,] 263.5567 161.8951 365.2184 101.66166\n[147,] 263.7271 161.9660 365.4882 101.76112\n[148,] 263.8943 162.0352 365.7533 101.85904\n[149,] 264.0582 162.1028 366.0136 101.95543\n[150,] 264.2189 162.1686 366.2693 102.05033\n[151,] 264.3765 162.2327 366.5202 102.14375\n[152,] 264.5309 162.2952 366.7667 102.23573\n[153,] 264.6823 162.3560 367.0085 102.32628\n[154,] 264.8305 162.4151 367.2459 102.41543\n[155,] 264.9757 162.4725 367.4789 102.50321\n[156,] 265.1179 162.5282 367.7075 102.58963\n[157,] 265.2570 162.5823 367.9318 102.67472\n[158,] 265.3932 162.6347 368.1517 102.75850\n[159,] 265.5265 162.6855 368.3675 102.84099\n[160,] 265.6568 162.7346 368.5791 102.92222\n[161,] 265.7843 162.7821 368.7865 103.00220\n[162,] 265.9089 162.8279 368.9899 103.08095\n[163,] 266.0307 162.8722 369.1892 103.15850\n[164,] 266.1496 162.9148 369.3845 103.23487\n[165,] 266.2658 162.9557 369.5759 103.31007\n[166,] 266.3792 162.9951 369.7633 103.38412\n[167,] 266.4899 163.0329 369.9470 103.45704\n[168,] 266.5979 163.0691 370.1268 103.52886\n[169,] 266.7032 163.1037 370.3028 103.59959\n[170,] 266.8059 163.1367 370.4752 103.66924\n[171,] 266.9060 163.1682 370.6438 103.73783\n[172,] 267.0035 163.1981 370.8089 103.80539\n[173,] 267.0984 163.2264 370.9703 103.87193\n[174,] 267.1907 163.2533 371.1282 103.93747\n[175,] 267.2806 163.2786 371.2826 104.00201\n[176,] 267.3679 163.3023 371.4335 104.06559\n[177,] 267.4528 163.3246 371.5810 104.12820\n[178,] 267.5353 163.3454 371.7252 104.18988\n[179,] 267.6153 163.3647 371.8660 104.25063\n[180,] 267.6930 163.3825 372.0035 104.31048\n[181,] 267.7683 163.3989 372.1377 104.36943\n[182,] 267.8413 163.4138 372.2688 104.42749\n[183,] 267.9119 163.4272 372.3966 104.48470\n[184,] 267.9803 163.4392 372.5213 104.54105\n[185,] 268.0464 163.4498 372.6430 104.59656\n[186,] 268.1103 163.4590 372.7615 104.65125\n[187,] 268.1719 163.4668 372.8771 104.70513\n[188,] 268.2314 163.4732 372.9896 104.75820\n[189,] 268.2887 163.4782 373.0992 104.81050\n[190,] 268.3439 163.4819 373.2060 104.86202\n[191,] 268.3970 163.4842 373.3098 104.91279\n[192,] 268.4480 163.4852 373.4108 104.96280\n[193,] 268.4969 163.4848 373.5090 105.01208\n[194,] 268.5438 163.4832 373.6044 105.06064\n[195,] 268.5887 163.4802 373.6972 105.10849\n[196,] 268.6315 163.4759 373.7872 105.15563\n[197,] 268.6724 163.4704 373.8745 105.20209\n[198,] 268.7114 163.4635 373.9593 105.24787\n[199,] 268.7484 163.4555 374.0414 105.29299\n[200,] 268.7836 163.4461 374.1210 105.33744\n[201,] 268.8169 163.4356 374.1981 105.38126\n[202,] 268.8483 163.4238 374.2727 105.42444\n[203,] 268.8779 163.4109 374.3448 105.46699\n[204,] 268.9056 163.3967 374.4146 105.50893\n[205,] 268.9316 163.3814 374.4819 105.55026\n[206,] 268.9559 163.3649 374.5469 105.59100\n[207,] 268.9783 163.3472 374.6095 105.63116\n[208,] 268.9991 163.3284 374.6699 105.67074\n[209,] 269.0182 163.3084 374.7279 105.70975\n[210,] 269.0356 163.2874 374.7838 105.74820\n[211,] 269.0514 163.2652 374.8375 105.78611\n[212,] 269.0655 163.2420 374.8890 105.82348\n[213,] 269.0780 163.2177 374.9383 105.86031\n[214,] 269.0889 163.1923 374.9855 105.89662\n[215,] 269.0983 163.1659 375.0307 105.93242\n[216,] 269.1061 163.1384 375.0738 105.96771\n[217,] 269.1124 163.1099 375.1149 106.00251\n[218,] 269.1172 163.0804 375.1540 106.03681\n[219,] 269.1205 163.0499 375.1911 106.07063\n[220,] 269.1223 163.0184 375.2263 106.10397\n[221,] 269.1227 162.9859 375.2596 106.13685\n[222,] 269.1217 162.9525 375.2910 106.16927\n[223,] 269.1193 162.9181 375.3205 106.20124\n[224,] 269.1155 162.8827 375.3483 106.23276\n[225,] 269.1103 162.8465 375.3742 106.26384\n[226,] 269.1038 162.8094 375.3983 106.29449\n[227,] 269.0960 162.7713 375.4207 106.32471\n[228,] 269.0869 162.7324 375.4414 106.35452\n[229,] 269.0765 162.6926 375.4604 106.38392\n[230,] 269.0648 162.6519 375.4777 106.41291\n[231,] 269.0519 162.6104 375.4934 106.44150\n[232,] 269.0377 162.5680 375.5074 106.46970\n[233,] 269.0224 162.5249 375.5199 106.49751\n[234,] 269.0059 162.4809 375.5308 106.52495\n[235,] 268.9881 162.4361 375.5401 106.55201\n[236,] 268.9693 162.3906 375.5480 106.57870\n[237,] 268.9493 162.3443 375.5543 106.60502\n[238,] 268.9282 162.2972 375.5592 106.63099\n[239,] 268.9060 162.2493 375.5626 106.65661\n[240,] 268.8827 162.2008 375.5645 106.68188\n[241,] 268.8583 162.1515 375.5651 106.70681\n[242,] 268.8329 162.1015 375.5643 106.73141\n[243,] 268.8065 162.0508 375.5622 106.75567\n[244,] 268.7791 161.9994 375.5587 106.77961\n[245,] 268.7506 161.9474 375.5539 106.80323\n[246,] 268.7212 161.8947 375.5477 106.82653\n[247,] 268.6908 161.8413 375.5404 106.84953\n[248,] 268.6595 161.7873 375.5317 106.87221\n[249,] 268.6273 161.7327 375.5219 106.89460\n[250,] 268.5942 161.6775 375.5108 106.91668\n[251,] 268.5601 161.6216 375.4986 106.93848\n[252,] 268.5252 161.5652 375.4852 106.95998\n[253,] 268.4894 161.5082 375.4706 106.98121\n[254,] 268.4528 161.4506 375.4549 107.00215\n[255,] 268.4153 161.3925 375.4381 107.02282\n[256,] 268.3771 161.3338 375.4203 107.04322\n[257,] 268.3380 161.2746 375.4013 107.06335\n[258,] 268.2981 161.2149 375.3813 107.08322\n[259,] 268.2575 161.1547 375.3603 107.10282\n[260,] 268.2161 161.0939 375.3383 107.12218\n[261,] 268.1740 161.0327 375.3153 107.14128\n[262,] 268.1312 160.9710 375.2913 107.16013\n[263,] 268.0876 160.9089 375.2663 107.17874\n[264,] 268.0433 160.8462 375.2405 107.19711\n[265,] 267.9984 160.7832 375.2137 107.21524\n[266,] 267.9528 160.7197 375.1860 107.23314\n[267,] 267.9066 160.6558 375.1574 107.25081\n[268,] 267.8597 160.5914 375.1279 107.26825\n[269,] 267.8121 160.5267 375.0976 107.28546\n[270,] 267.7640 160.4616 375.0665 107.30246\n[271,] 267.7153 160.3961 375.0345 107.31924\n[272,] 267.6660 160.3302 375.0018 107.33580\n[273,] 267.6161 160.2639 374.9683 107.35216\n[274,] 267.5656 160.1973 374.9340 107.36831\n[275,] 267.5147 160.1304 374.8989 107.38425\n[276,] 267.4631 160.0631 374.8631 107.39999\n[277,] 267.4111 159.9956 374.8266 107.41553\n[278,] 267.3585 159.9277 374.7894 107.43087\n[279,] 267.3055 159.8595 374.7515 107.44602\n[280,] 267.2520 159.7910 374.7129 107.46098\n[281,] 267.1980 159.7222 374.6737 107.47575\n[282,] 267.1435 159.6532 374.6338 107.49033\n[283,] 267.0886 159.5839 374.5933 107.50473\n[284,] 267.0333 159.5143 374.5522 107.51895\n[285,] 266.9775 159.4445 374.5105 107.53300\n[286,] 266.9213 159.3745 374.4682 107.54687\n[287,] 266.8648 159.3042 374.4253 107.56056\n[288,] 266.8078 159.2337 374.3819 107.57408\n[289,] 266.7505 159.1630 374.3379 107.58744\n[290,] 266.6927 159.0921 374.2934 107.60063\n[291,] 266.6347 159.0210 374.2483 107.61365\n[292,] 266.5763 158.9498 374.2028 107.62652\n[293,] 266.5175 158.8783 374.1568 107.63922\n[294,] 266.4585 158.8067 374.1102 107.65177\n[295,] 266.3991 158.7349 374.0633 107.66416\n[296,] 266.3394 158.6630 374.0158 107.67640\n[297,] 266.2794 158.5910 373.9679 107.68849\n[298,] 266.2192 158.5188 373.9196 107.70042\n[299,] 266.1587 158.4464 373.8709 107.71221\n[300,] 266.0979 158.3740 373.8217 107.72386\n[301,] 266.0368 158.3015 373.7722 107.73536\n[302,] 265.9755 158.2288 373.7223 107.74673\n[303,] 265.9140 158.1561 373.6720 107.75795\n[304,] 265.8523 158.0833 373.6213 107.76903\n[305,] 265.7903 158.0104 373.5703 107.77998\n[306,] 265.7282 157.9374 373.5190 107.79079\n[307,] 265.6658 157.8643 373.4673 107.80148\n[308,] 265.6033 157.7912 373.4153 107.81203\n[309,] 265.5405 157.7181 373.3630 107.82245\n[310,] 265.4776 157.6449 373.3104 107.83275\n[311,] 265.4146 157.5717 373.2575 107.84291\n[312,] 265.3514 157.4984 373.2043 107.85296\n[313,] 265.2880 157.4251 373.1509 107.86288\n[314,] 265.2245 157.3519 373.0972 107.87268\n[315,] 265.1609 157.2786 373.0433 107.88237\n[316,] 265.0972 157.2053 372.9891 107.89193\n[317,] 265.0333 157.1320 372.9347 107.90138\n[318,] 264.9694 157.0587 372.8801 107.91071\n[319,] 264.9053 156.9854 372.8253 107.91993\n[320,] 264.8412 156.9122 372.7702 107.92904\n[321,] 264.7770 156.8390 372.7150 107.93804\n[322,] 264.7127 156.7658 372.6596 107.94692\n[323,] 264.6483 156.6926 372.6040 107.95570\n[324,] 264.5839 156.6196 372.5483 107.96437\n[325,] 264.5195 156.5465 372.4924 107.97294\n[326,] 264.4550 156.4736 372.4364 107.98140\n[327,] 264.3904 156.4006 372.3802 107.98976\n[328,] 264.3258 156.3278 372.3239 107.99802\n[329,] 264.2612 156.2550 372.2674 108.00618\n[330,] 264.1966 156.1824 372.2108 108.01424\n[331,] 264.1320 156.1098 372.1542 108.02220\n[332,] 264.0673 156.0373 372.0974 108.03007\n[333,] 264.0027 155.9649 372.0405 108.03784\n[334,] 263.9381 155.8926 371.9836 108.04551\n[335,] 263.8735 155.8204 371.9266 108.05309\n[336,] 263.8089 155.7483 371.8695 108.06058\n[337,] 263.7443 155.6763 371.8123 108.06798\n[338,] 263.6798 155.6045 371.7551 108.07529\n[339,] 263.6153 155.5328 371.6978 108.08251\n[340,] 263.5509 155.4612 371.6405 108.08964\n[341,] 263.4865 155.3898 371.5831 108.09668\n[342,] 263.4221 155.3185 371.5258 108.10364\n[343,] 263.3578 155.2473 371.4684 108.11051\n[344,] 263.2936 155.1763 371.4109 108.11730\n[345,] 263.2295 155.1055 371.3535 108.12401\n[346,] 263.1654 155.0348 371.2961 108.13064\n[347,] 263.1014 154.9643 371.2386 108.13718\n[348,] 263.0375 154.8939 371.1812 108.14364\n[349,] 262.9737 154.8237 371.1238 108.15003\n[350,] 262.9100 154.7537 371.0664 108.15634\n[351,] 262.8464 154.6839 371.0090 108.16257\n[352,] 262.7829 154.6142 370.9516 108.16872\n[353,] 262.7195 154.5447 370.8943 108.17480\n[354,] 262.6563 154.4755 370.8371 108.18080\n[355,] 262.5931 154.4064 370.7798 108.18673\n[356,] 262.5301 154.3375 370.7227 108.19259\n[357,] 262.4672 154.2688 370.6656 108.19837\n[358,] 262.4044 154.2003 370.6085 108.20409\n[359,] 262.3418 154.1320 370.5515 108.20973\n[360,] 262.2793 154.0640 370.4946 108.21531\n[361,] 262.2169 153.9961 370.4378 108.22081\n[362,] 262.1547 153.9285 370.3810 108.22625\n[363,] 262.0927 153.8611 370.3243 108.23162\n[364,] 262.0308 153.7939 370.2677 108.23692\n[365,] 261.9691 153.7269 370.2113 108.24216\n\n$drug\n           fcst    lower     upper        CI\n  [1,] 9.346158 8.742243  9.950073 0.6039150\n  [2,] 9.328053 8.482422 10.173684 0.8456309\n  [3,] 9.393182 8.369902 10.416462 1.0232799\n  [4,] 9.420064 8.245866 10.594261 1.1741974\n  [5,] 9.492541 8.177326 10.807756 1.3152148\n  [6,] 9.483060 8.030154 10.935967 1.4529068\n  [7,] 9.535627 7.972427 11.098827 1.5632002\n  [8,] 9.541957 7.869665 11.214249 1.6722920\n  [9,] 9.527271 7.762065 11.292477 1.7652062\n [10,] 9.537662 7.692082 11.383241 1.8455792\n [11,] 9.538475 7.625343 11.451606 1.9131318\n [12,] 9.558643 7.584803 11.532483 1.9738398\n [13,] 9.544395 7.516373 11.572417 2.0280219\n [14,] 9.547463 7.474630 11.620295 2.0728326\n [15,] 9.540602 7.427275 11.653929 2.1133269\n [16,] 9.544588 7.396689 11.692487 2.1478989\n [17,] 9.536746 7.358499 11.714993 2.1782471\n [18,] 9.529908 7.326432 11.733384 2.2034763\n [19,] 9.530406 7.304233 11.756579 2.2261734\n [20,] 9.520115 7.273996 11.766235 2.2461193\n [21,] 9.517934 7.254703 11.781165 2.2632311\n [22,] 9.507705 7.229348 11.786062 2.2783567\n [23,] 9.503354 7.211908 11.794800 2.2914462\n [24,] 9.494630 7.191383 11.797878 2.3032479\n [25,] 9.487774 7.174278 11.801270 2.3134964\n [26,] 9.480812 7.158110 11.803515 2.3227028\n [27,] 9.471541 7.140598 11.802484 2.3309430\n [28,] 9.465216 7.126835 11.803597 2.3383809\n [29,] 9.455509 7.110361 11.800657 2.3451480\n [30,] 9.448953 7.097659 11.800247 2.3512942\n [31,] 9.439787 7.082787 11.796787 2.3570000\n [32,] 9.432021 7.069770 11.794272 2.3622511\n [33,] 9.423767 7.056598 11.790936 2.3671689\n [34,] 9.415490 7.043712 11.787268 2.3717779\n [35,] 9.407807 7.031670 11.783944 2.3761369\n [36,] 9.399070 7.018792 11.779348 2.3802783\n [37,] 9.391709 7.007483 11.775934 2.3842258\n [38,] 9.383259 6.995236 11.771281 2.3880225\n [39,] 9.375785 6.984110 11.767460 2.3916747\n [40,] 9.367785 6.972573 11.762997 2.3952119\n [41,] 9.360144 6.961502 11.758787 2.3986423\n [42,] 9.352652 6.950669 11.754636 2.4019839\n [43,] 9.344999 6.939753 11.750245 2.4052459\n [44,] 9.337841 6.929404 11.746278 2.4084368\n [45,] 9.330337 6.918770 11.741904 2.4115673\n [46,] 9.323379 6.908738 11.738020 2.4146408\n [47,] 9.316184 6.898518 11.733850 2.4176660\n [48,] 9.309333 6.888688 11.729979 2.4206454\n [49,] 9.302489 6.878904 11.726074 2.4235851\n [50,] 9.295747 6.869259 11.722235 2.4264876\n [51,] 9.289218 6.859862 11.718574 2.4293561\n [52,] 9.282656 6.850463 11.714850 2.4321936\n [53,] 9.276374 6.841372 11.711376 2.4350021\n [54,] 9.270044 6.832259 11.707828 2.4377841\n [55,] 9.263958 6.823417 11.704498 2.4405405\n [56,] 9.257897 6.814623 11.701170 2.4432737\n [57,] 9.251987 6.806003 11.697972 2.4459843\n [58,] 9.246189 6.797515 11.694863 2.4486740\n [59,] 9.240465 6.789122 11.691809 2.4513436\n [60,] 9.234906 6.780912 11.688900 2.4539939\n [61,] 9.229390 6.772764 11.686016 2.4566259\n [62,] 9.224040 6.764800 11.683281 2.4592401\n [63,] 9.218747 6.756909 11.680584 2.4618373\n [64,] 9.213591 6.749173 11.678009 2.4644179\n [65,] 9.208522 6.741539 11.675504 2.4669826\n [66,] 9.203554 6.734023 11.673086 2.4695315\n [67,] 9.198700 6.726635 11.670765 2.4720653\n [68,] 9.193925 6.719341 11.668510 2.4745842\n [69,] 9.189273 6.712184 11.666362 2.4770886\n [70,] 9.184696 6.705117 11.664274 2.4795788\n [71,] 9.180233 6.698179 11.662288 2.4820549\n [72,] 9.175854 6.691337 11.660371 2.4845173\n [73,] 9.171576 6.684609 11.658542 2.4869661\n [74,] 9.167390 6.677989 11.656792 2.4894016\n [75,] 9.163293 6.671469 11.655117 2.4918239\n [76,] 9.159295 6.665062 11.653528 2.4942332\n [77,] 9.155379 6.658750 11.652009 2.4966296\n [78,] 9.151561 6.652548 11.650575 2.4990133\n [79,] 9.147825 6.646441 11.649210 2.5013844\n [80,] 9.144182 6.640439 11.647925 2.5037430\n [81,] 9.140623 6.634534 11.646712 2.5060892\n [82,] 9.137150 6.628727 11.645574 2.5084232\n [83,] 9.133765 6.623020 11.644510 2.5107449\n [84,] 9.130461 6.617406 11.643515 2.5130546\n [85,] 9.127243 6.611891 11.642595 2.5153522\n [86,] 9.124105 6.606467 11.641743 2.5176379\n [87,] 9.121051 6.601140 11.640963 2.5199117\n [88,] 9.118077 6.595903 11.640250 2.5221737\n [89,] 9.115183 6.590759 11.639607 2.5244239\n [90,] 9.112369 6.585706 11.639031 2.5266624\n [91,] 9.109632 6.580743 11.638521 2.5288892\n [92,] 9.106974 6.575870 11.638079 2.5311044\n [93,] 9.104392 6.571084 11.637700 2.5333080\n [94,] 9.101887 6.566387 11.637387 2.5355000\n [95,] 9.099456 6.561775 11.637136 2.5376805\n [96,] 9.097100 6.557250 11.636950 2.5398496\n [97,] 9.094818 6.552810 11.636825 2.5420072\n [98,] 9.092608 6.548455 11.636761 2.5441533\n [99,] 9.090471 6.544182 11.636759 2.5462881\n[100,] 9.088404 6.539993 11.636816 2.5484114\n[101,] 9.086409 6.535885 11.636932 2.5505234\n[102,] 9.084482 6.531858 11.637106 2.5526240\n[103,] 9.082626 6.527912 11.637339 2.5547133\n[104,] 9.080837 6.524046 11.637628 2.5567912\n[105,] 9.079116 6.520258 11.637973 2.5588578\n[106,] 9.077461 6.516548 11.638374 2.5609130\n[107,] 9.075873 6.512916 11.638830 2.5629570\n[108,] 9.074349 6.509360 11.639339 2.5649896\n[109,] 9.072890 6.505879 11.639901 2.5670109\n[110,] 9.071495 6.502474 11.640516 2.5690210\n[111,] 9.070163 6.499144 11.641183 2.5710197\n[112,] 9.068894 6.495886 11.641901 2.5730071\n[113,] 9.067685 6.492702 11.642668 2.5749832\n[114,] 9.066538 6.489590 11.643486 2.5769479\n[115,] 9.065451 6.486549 11.644352 2.5789014\n[116,] 9.064423 6.483579 11.645266 2.5808436\n[117,] 9.063453 6.480679 11.646228 2.5827744\n[118,] 9.062542 6.477848 11.647236 2.5846939\n[119,] 9.061688 6.475086 11.648290 2.5866021\n[120,] 9.060890 6.472391 11.649389 2.5884990\n[121,] 9.060149 6.469764 11.650533 2.5903845\n[122,] 9.059462 6.467203 11.651721 2.5922588\n[123,] 9.058830 6.464708 11.652951 2.5941216\n[124,] 9.058251 6.462278 11.654224 2.5959732\n[125,] 9.057726 6.459913 11.655539 2.5978133\n[126,] 9.057253 6.457611 11.656895 2.5996422\n[127,] 9.056832 6.455372 11.658291 2.6014597\n[128,] 9.056461 6.453196 11.659727 2.6032658\n[129,] 9.056141 6.451081 11.661202 2.6050606\n[130,] 9.055871 6.449027 11.662715 2.6068440\n[131,] 9.055650 6.447034 11.664266 2.6086160\n[132,] 9.055477 6.445100 11.665853 2.6103767\n[133,] 9.055351 6.443225 11.667477 2.6121261\n[134,] 9.055273 6.441409 11.669137 2.6138641\n[135,] 9.055241 6.439650 11.670832 2.6155907\n[136,] 9.055255 6.437949 11.672561 2.6173060\n[137,] 9.055313 6.436304 11.674323 2.6190099\n[138,] 9.055417 6.434714 11.676119 2.6207024\n[139,] 9.055564 6.433180 11.677947 2.6223837\n[140,] 9.055754 6.431700 11.679807 2.6240536\n[141,] 9.055986 6.430274 11.681699 2.6257121\n[142,] 9.056261 6.428902 11.683620 2.6273593\n[143,] 9.056577 6.427582 11.685572 2.6289952\n[144,] 9.056933 6.426313 11.687553 2.6306198\n[145,] 9.057330 6.425097 11.689563 2.6322331\n[146,] 9.057766 6.423931 11.691601 2.6338351\n[147,] 9.058240 6.422815 11.693666 2.6354258\n[148,] 9.058753 6.421748 11.695759 2.6370053\n[149,] 9.059304 6.420731 11.697878 2.6385735\n[150,] 9.059892 6.419761 11.700022 2.6401304\n[151,] 9.060516 6.418840 11.702192 2.6416761\n[152,] 9.061176 6.417965 11.704386 2.6432106\n[153,] 9.061871 6.417137 11.706605 2.6447339\n[154,] 9.062601 6.416355 11.708847 2.6462460\n[155,] 9.063365 6.415618 11.711112 2.6477470\n[156,] 9.064162 6.414926 11.713399 2.6492368\n[157,] 9.064993 6.414277 11.715708 2.6507155\n[158,] 9.065856 6.413673 11.718039 2.6521830\n[159,] 9.066751 6.413111 11.720390 2.6536395\n[160,] 9.067677 6.412592 11.722762 2.6550850\n[161,] 9.068634 6.412114 11.725153 2.6565194\n[162,] 9.069621 6.411678 11.727564 2.6579427\n[163,] 9.070638 6.411282 11.729993 2.6593551\n[164,] 9.071684 6.410927 11.732440 2.6607566\n[165,] 9.072758 6.410611 11.734905 2.6621470\n[166,] 9.073861 6.410334 11.737387 2.6635266\n[167,] 9.074991 6.410096 11.739886 2.6648953\n[168,] 9.076148 6.409895 11.742401 2.6662532\n[169,] 9.077332 6.409732 11.744932 2.6676002\n[170,] 9.078542 6.409605 11.747478 2.6689365\n[171,] 9.079777 6.409515 11.750039 2.6702620\n[172,] 9.081037 6.409461 11.752614 2.6715767\n[173,] 9.082322 6.409441 11.755203 2.6728808\n[174,] 9.083631 6.409457 11.757805 2.6741742\n[175,] 9.084963 6.409506 11.760420 2.6754570\n[176,] 9.086319 6.409590 11.763048 2.6767292\n[177,] 9.087697 6.409706 11.765687 2.6779908\n[178,] 9.089097 6.409855 11.768339 2.6792419\n[179,] 9.090518 6.410036 11.771001 2.6804826\n[180,] 9.091961 6.410248 11.773674 2.6817128\n[181,] 9.093424 6.410492 11.776357 2.6829326\n[182,] 9.094908 6.410766 11.779050 2.6841420\n[183,] 9.096411 6.411070 11.781753 2.6853411\n[184,] 9.097934 6.411404 11.784464 2.6865299\n[185,] 9.099476 6.411767 11.787184 2.6877085\n[186,] 9.101036 6.412159 11.789913 2.6888769\n[187,] 9.102614 6.412578 11.792649 2.6900352\n[188,] 9.104209 6.413026 11.795392 2.6911833\n[189,] 9.105822 6.413500 11.798143 2.6923213\n[190,] 9.107451 6.414001 11.800900 2.6934494\n[191,] 9.109096 6.414529 11.803664 2.6945674\n[192,] 9.110758 6.415082 11.806433 2.6956755\n[193,] 9.112434 6.415661 11.809208 2.6967737\n[194,] 9.114126 6.416264 11.811988 2.6978621\n[195,] 9.115832 6.416892 11.814773 2.6989407\n[196,] 9.117553 6.417543 11.817562 2.7000095\n[197,] 9.119287 6.418219 11.820356 2.7010686\n[198,] 9.121035 6.418917 11.823153 2.7021181\n[199,] 9.122796 6.419638 11.825953 2.7031579\n[200,] 9.124569 6.420381 11.828757 2.7041882\n[201,] 9.126354 6.421145 11.831563 2.7052090\n[202,] 9.128152 6.421932 11.834372 2.7062203\n[203,] 9.129961 6.422739 11.837183 2.7072222\n[204,] 9.131781 6.423566 11.839996 2.7082148\n[205,] 9.133612 6.424413 11.842810 2.7091980\n[206,] 9.135453 6.425281 11.845625 2.7101720\n[207,] 9.137304 6.426167 11.848441 2.7111368\n[208,] 9.139165 6.427072 11.851257 2.7120924\n[209,] 9.141035 6.427996 11.854074 2.7130389\n[210,] 9.142914 6.428938 11.856890 2.7139763\n[211,] 9.144802 6.429897 11.859706 2.7149048\n[212,] 9.146698 6.430873 11.862522 2.7158243\n[213,] 9.148602 6.431867 11.865336 2.7167349\n[214,] 9.150513 6.432877 11.868150 2.7176367\n[215,] 9.152432 6.433902 11.870962 2.7185296\n[216,] 9.154358 6.434944 11.873772 2.7194139\n[217,] 9.156290 6.436001 11.876580 2.7202894\n[218,] 9.158229 6.437073 11.879385 2.7211563\n[219,] 9.160174 6.438159 11.882189 2.7220147\n[220,] 9.162124 6.439260 11.884989 2.7228645\n[221,] 9.164080 6.440374 11.887786 2.7237058\n[222,] 9.166041 6.441502 11.890580 2.7245388\n[223,] 9.168007 6.442644 11.893370 2.7253633\n[224,] 9.169977 6.443798 11.896157 2.7261796\n[225,] 9.171952 6.444964 11.898939 2.7269876\n[226,] 9.173930 6.446143 11.901718 2.7277875\n[227,] 9.175912 6.447333 11.904491 2.7285791\n[228,] 9.177897 6.448535 11.907260 2.7293627\n[229,] 9.179886 6.449748 11.910024 2.7301383\n[230,] 9.181877 6.450971 11.912783 2.7309059\n[231,] 9.183871 6.452205 11.915536 2.7316656\n[232,] 9.185867 6.453450 11.918284 2.7324174\n[233,] 9.187865 6.454704 11.921026 2.7331613\n[234,] 9.189865 6.455967 11.923762 2.7338976\n[235,] 9.191866 6.457240 11.926492 2.7346261\n[236,] 9.193868 6.458521 11.929215 2.7353470\n[237,] 9.195872 6.459812 11.931932 2.7360603\n[238,] 9.197876 6.461110 11.934642 2.7367661\n[239,] 9.199881 6.462416 11.937345 2.7374643\n[240,] 9.201886 6.463730 11.940041 2.7381552\n[241,] 9.203891 6.465052 11.942729 2.7388387\n[242,] 9.205895 6.466380 11.945410 2.7395148\n[243,] 9.207900 6.467716 11.948083 2.7401837\n[244,] 9.209903 6.469058 11.950749 2.7408454\n[245,] 9.211906 6.470406 11.953406 2.7415000\n[246,] 9.213907 6.471760 11.956055 2.7421475\n[247,] 9.215908 6.473120 11.958696 2.7427879\n[248,] 9.217906 6.474485 11.961328 2.7434213\n[249,] 9.219903 6.475855 11.963951 2.7440478\n[250,] 9.221898 6.477231 11.966566 2.7446674\n[251,] 9.223891 6.478611 11.969171 2.7452802\n[252,] 9.225881 6.479995 11.971768 2.7458862\n[253,] 9.227869 6.481384 11.974355 2.7464855\n[254,] 9.229854 6.482776 11.976933 2.7470782\n[255,] 9.231837 6.484172 11.979501 2.7476642\n[256,] 9.233816 6.485572 11.982059 2.7482437\n[257,] 9.235791 6.486975 11.984608 2.7488167\n[258,] 9.237763 6.488380 11.987147 2.7493833\n[259,] 9.239732 6.489788 11.989675 2.7499434\n[260,] 9.241696 6.491199 11.992194 2.7504972\n[261,] 9.243657 6.492612 11.994702 2.7510448\n[262,] 9.245613 6.494027 11.997199 2.7515861\n[263,] 9.247565 6.495444 11.999686 2.7521212\n[264,] 9.249513 6.496862 12.002163 2.7526502\n[265,] 9.251455 6.498282 12.004628 2.7531731\n[266,] 9.253393 6.499703 12.007083 2.7536899\n[267,] 9.255326 6.501125 12.009527 2.7542009\n[268,] 9.257253 6.502547 12.011959 2.7547059\n[269,] 9.259176 6.503971 12.014380 2.7552050\n[270,] 9.261092 6.505394 12.016791 2.7556983\n[271,] 9.263003 6.506817 12.019189 2.7561858\n[272,] 9.264909 6.508241 12.021576 2.7566677\n[273,] 9.266808 6.509664 12.023952 2.7571438\n[274,] 9.268701 6.511087 12.026316 2.7576144\n[275,] 9.270588 6.512509 12.028668 2.7580794\n[276,] 9.272469 6.513930 12.031008 2.7585389\n[277,] 9.274344 6.515351 12.033336 2.7589929\n[278,] 9.276211 6.516770 12.035653 2.7594415\n[279,] 9.278072 6.518188 12.037957 2.7598847\n[280,] 9.279927 6.519604 12.040249 2.7603226\n[281,] 9.281774 6.521018 12.042529 2.7607553\n[282,] 9.283614 6.522431 12.044797 2.7611828\n[283,] 9.285447 6.523842 12.047052 2.7616050\n[284,] 9.287273 6.525250 12.049295 2.7620222\n[285,] 9.289091 6.526656 12.051525 2.7624343\n[286,] 9.290901 6.528060 12.053743 2.7628414\n[287,] 9.292704 6.529461 12.055948 2.7632434\n[288,] 9.294500 6.530859 12.058140 2.7636406\n[289,] 9.296287 6.532254 12.060320 2.7640329\n[290,] 9.298067 6.533646 12.062487 2.7644204\n[291,] 9.299838 6.535035 12.064641 2.7648030\n[292,] 9.301602 6.536421 12.066783 2.7651810\n[293,] 9.303357 6.537803 12.068911 2.7655542\n[294,] 9.305104 6.539181 12.071026 2.7659228\n[295,] 9.306842 6.540555 12.073129 2.7662868\n[296,] 9.308572 6.541926 12.075218 2.7666462\n[297,] 9.310293 6.543292 12.077294 2.7670011\n[298,] 9.312006 6.544654 12.079357 2.7673515\n[299,] 9.313710 6.546012 12.081407 2.7676976\n[300,] 9.315405 6.547366 12.083444 2.7680392\n[301,] 9.317091 6.548714 12.085467 2.7683765\n[302,] 9.318768 6.550059 12.087478 2.7687095\n[303,] 9.320436 6.551398 12.089475 2.7690383\n[304,] 9.322095 6.552732 12.091458 2.7693629\n[305,] 9.323745 6.554062 12.093429 2.7696833\n[306,] 9.325386 6.555386 12.095385 2.7699996\n[307,] 9.327017 6.556705 12.097329 2.7703118\n[308,] 9.328639 6.558019 12.099259 2.7706200\n[309,] 9.330252 6.559327 12.101176 2.7709242\n[310,] 9.331855 6.560630 12.103079 2.7712245\n[311,] 9.333448 6.561927 12.104969 2.7715208\n[312,] 9.335032 6.563219 12.106845 2.7718133\n[313,] 9.336606 6.564504 12.108708 2.7721019\n[314,] 9.338170 6.565784 12.110557 2.7723868\n[315,] 9.339725 6.567057 12.112393 2.7726679\n[316,] 9.341270 6.568325 12.114215 2.7729453\n[317,] 9.342805 6.569586 12.116024 2.7732190\n[318,] 9.344330 6.570841 12.117819 2.7734892\n[319,] 9.345845 6.572090 12.119601 2.7737557\n[320,] 9.347350 6.573332 12.121369 2.7740187\n[321,] 9.348846 6.574568 12.123124 2.7742781\n[322,] 9.350331 6.575797 12.124865 2.7745341\n[323,] 9.351806 6.577019 12.126592 2.7747867\n[324,] 9.353271 6.578235 12.128306 2.7750359\n[325,] 9.354725 6.579444 12.130007 2.7752817\n[326,] 9.356170 6.580646 12.131694 2.7755242\n[327,] 9.357604 6.581841 12.133367 2.7757634\n[328,] 9.359028 6.583029 12.135027 2.7759993\n[329,] 9.360442 6.584210 12.136674 2.7762321\n[330,] 9.361845 6.585383 12.138307 2.7764616\n[331,] 9.363238 6.586550 12.139926 2.7766880\n[332,] 9.364621 6.587709 12.141532 2.7769114\n[333,] 9.365993 6.588861 12.143125 2.7771316\n[334,] 9.367355 6.590006 12.144704 2.7773488\n[335,] 9.368706 6.591143 12.146269 2.7775631\n[336,] 9.370047 6.592273 12.147822 2.7777743\n[337,] 9.371378 6.593395 12.149361 2.7779826\n[338,] 9.372698 6.594510 12.150886 2.7781881\n[339,] 9.374008 6.595617 12.152398 2.7783906\n[340,] 9.375307 6.596716 12.153897 2.7785903\n[341,] 9.376595 6.597808 12.155383 2.7787873\n[342,] 9.377874 6.598892 12.156855 2.7789814\n[343,] 9.379141 6.599968 12.158314 2.7791729\n[344,] 9.380398 6.601037 12.159760 2.7793616\n[345,] 9.381645 6.602097 12.161192 2.7795477\n[346,] 9.382881 6.603150 12.162612 2.7797311\n[347,] 9.384106 6.604194 12.164018 2.7799119\n[348,] 9.385321 6.605231 12.165411 2.7800901\n[349,] 9.386525 6.606259 12.166791 2.7802658\n[350,] 9.387719 6.607280 12.168158 2.7804390\n[351,] 9.388902 6.608293 12.169512 2.7806097\n[352,] 9.390075 6.609297 12.170853 2.7807780\n[353,] 9.391237 6.610293 12.172181 2.7809438\n[354,] 9.392389 6.611282 12.173496 2.7811072\n[355,] 9.393530 6.612262 12.174798 2.7812683\n[356,] 9.394660 6.613233 12.176087 2.7814270\n[357,] 9.395780 6.614197 12.177364 2.7815834\n[358,] 9.396890 6.615152 12.178628 2.7817376\n[359,] 9.397989 6.616100 12.179879 2.7818895\n[360,] 9.399078 6.617038 12.181117 2.7820391\n[361,] 9.400156 6.617969 12.182342 2.7821866\n[362,] 9.401223 6.618891 12.183555 2.7823319\n[363,] 9.402280 6.619805 12.184755 2.7824751\n[364,] 9.403327 6.620711 12.185943 2.7826161\n[365,] 9.404363 6.621608 12.187118 2.7827551\n\n$small_biotech\n           fcst     lower    upper        CI\n  [1,] 46.99368 42.351352 51.63601  4.642330\n  [2,] 46.83867 40.189466 53.48788  6.649208\n  [3,] 46.65904 38.519022 54.79905  8.140016\n  [4,] 46.35762 36.982004 55.73324  9.375620\n  [5,] 46.42192 35.997358 56.84648 10.424560\n  [6,] 46.27665 34.978821 57.57449 11.297832\n  [7,] 46.24293 34.168984 58.31688 12.073948\n  [8,] 46.07369 33.288815 58.85857 12.784879\n  [9,] 45.99765 32.538918 59.45638 13.458732\n [10,] 45.91318 31.751569 60.07480 14.161613\n [11,] 45.74836 30.919909 60.57681 14.828448\n [12,] 45.65694 30.196896 61.11699 15.460046\n [13,] 45.49944 29.441454 61.55743 16.057986\n [14,] 45.40528 28.785407 62.02514 16.619869\n [15,] 45.26722 28.115806 62.41864 17.151415\n [16,] 45.16427 27.509691 62.81884 17.654576\n [17,] 45.02612 26.887663 63.16459 18.138461\n [18,] 44.89481 26.297224 63.49240 18.597589\n [19,] 44.79716 25.754285 63.84003 19.042873\n [20,] 44.67367 25.202423 64.14492 19.471249\n [21,] 44.56993 24.686186 64.45367 19.883740\n [22,] 44.44154 24.160524 64.72255 20.281013\n [23,] 44.33969 23.677135 65.00225 20.662555\n [24,] 44.22772 23.197068 65.25837 21.030652\n [25,] 44.12061 22.736522 65.50471 21.384093\n [26,] 44.01410 22.288180 65.74001 21.725916\n [27,] 43.90571 21.850259 65.96115 22.055448\n [28,] 43.80936 21.434590 66.18413 22.374770\n [29,] 43.70360 21.019928 66.38727 22.683669\n [30,] 43.60818 20.625523 66.59084 22.982657\n [31,] 43.50740 20.234864 66.77993 23.272531\n [32,] 43.41377 19.860648 66.96688 23.553117\n [33,] 43.31988 19.494482 67.14528 23.825398\n [34,] 43.22748 19.138306 67.31665 24.089170\n [35,] 43.13860 18.793222 67.48398 24.345378\n [36,] 43.04849 18.454485 67.64249 24.594004\n [37,] 42.96423 18.128613 67.79985 24.835619\n [38,] 42.87830 17.807831 67.94877 25.070470\n [39,] 42.79685 17.498099 68.09560 25.298750\n [40,] 42.71533 17.194438 68.23621 25.520888\n [41,] 42.63666 16.899736 68.37357 25.736919\n [42,] 42.55993 16.612653 68.50721 25.947276\n [43,] 42.48421 16.332209 68.63621 26.152001\n [44,] 42.41142 16.059983 68.76285 26.351433\n [45,] 42.33913 15.793426 68.88483 26.545702\n [46,] 42.27000 15.534984 69.00502 26.735019\n [47,] 42.20157 15.281990 69.12116 26.919582\n [48,] 42.13559 15.036088 69.23510 27.099505\n [49,] 42.07103 14.796018 69.34605 27.275014\n [50,] 42.00824 14.562048 69.45442 27.446188\n [51,] 41.94744 14.334210 69.56067 27.613229\n [52,] 41.88790 14.111674 69.66413 27.776227\n [53,] 41.83055 13.895215 69.76589 27.935337\n [54,] 41.77440 13.683726 69.86508 28.090676\n [55,] 41.72032 13.477966 69.96267 28.242354\n [56,] 41.66760 13.277103 70.05810 28.390500\n [57,] 41.61664 13.081440 70.15184 28.535199\n [58,] 41.56728 12.890699 70.24386 28.676578\n [59,] 41.51941 12.704698 70.33413 28.814713\n [60,] 41.47328 12.523567 70.42300 28.949715\n [61,] 41.42851 12.346851 70.51018 29.081663\n [62,] 41.38546 12.174814 70.59611 29.210647\n [63,] 41.34378 12.007025 70.68053 29.336753\n [64,] 41.30370 11.843645 70.76376 29.460056\n [65,] 41.26505 11.684414 70.84569 29.580640\n [66,] 41.22787 11.529302 70.92644 29.698570\n [67,] 41.19217 11.378247 71.00610 29.813926\n [68,] 41.15784 11.231073 71.08462 29.926772\n [69,] 41.12500 11.087822 71.16217 30.037176\n [70,] 41.09348 10.948275 71.23868 30.145201\n [71,] 41.06339 10.812478 71.31430 30.250910\n [72,] 41.03462 10.680257 71.38898 30.354363\n [73,] 41.00721 10.551598 71.46283 30.455616\n [74,] 40.98113 10.426404 71.53586 30.554727\n [75,] 40.95634 10.304595 71.60809 30.651749\n [76,] 40.93287 10.186134 71.67960 30.746735\n [77,] 40.91064 10.070907 71.75038 30.839735\n [78,] 40.88970  9.958900 71.82050 30.930799\n [79,] 40.86998  9.850001 71.88995 31.019974\n [80,] 40.85149  9.744184 71.95880 31.107306\n [81,] 40.83421  9.641365 72.02705 31.192841\n [82,] 40.81812  9.541494 72.09474 31.276621\n [83,] 40.80320  9.444514 72.16189 31.358690\n [84,] 40.78944  9.350358 72.22853 31.439087\n [85,] 40.77684  9.258986 72.29469 31.517853\n [86,] 40.76535  9.170326 72.36038 31.595026\n [87,] 40.75498  9.084341 72.42563 31.670644\n [88,] 40.74571  9.000965 72.49045 31.744743\n [89,] 40.73752  8.920158 72.55488 31.817360\n [90,] 40.73039  8.841865 72.61892 31.888527\n [91,] 40.72432  8.766036 72.68260 31.958280\n [92,] 40.71928  8.692628 72.74593 32.026650\n [93,] 40.71526  8.621588 72.80893 32.093670\n [94,] 40.71225  8.552878 72.87162 32.159370\n [95,] 40.71023  8.486445 72.93401 32.223780\n [96,] 40.70918  8.422252 72.99611 32.286930\n [97,] 40.70910  8.360251 73.05795 32.348848\n [98,] 40.70996  8.300402 73.11953 32.409562\n [99,] 40.71176  8.242664 73.18086 32.469100\n[100,] 40.71448  8.186995 73.24197 32.527487\n[101,] 40.71811  8.133357 73.30285 32.584748\n[102,] 40.72262  8.081709 73.36353 32.640911\n[103,] 40.72801  8.032015 73.42401 32.695997\n[104,] 40.73427  7.984235 73.48430 32.750032\n[105,] 40.74137  7.938335 73.54441 32.803039\n[106,] 40.74932  7.894276 73.60436 32.855040\n[107,] 40.75808  7.852024 73.66414 32.906057\n[108,] 40.76766  7.811545 73.72377 32.956112\n[109,] 40.77803  7.772803 73.78325 33.005226\n[110,] 40.78918  7.735767 73.84260 33.053418\n[111,] 40.80111  7.700401 73.90182 33.100710\n[112,] 40.81380  7.666675 73.96092 33.147121\n[113,] 40.82722  7.634556 74.01989 33.192669\n[114,] 40.84139  7.604014 74.07876 33.237373\n[115,] 40.85627  7.575017 74.13752 33.281251\n[116,] 40.87186  7.547536 74.19618 33.324321\n[117,] 40.88814  7.521540 74.25474 33.366601\n[118,] 40.90511  7.497002 74.31321 33.408106\n[119,] 40.92275  7.473893 74.37160 33.448854\n[120,] 40.94104  7.452184 74.42990 33.488860\n[121,] 40.95999  7.431847 74.48813 33.528140\n[122,] 40.97957  7.412857 74.54628 33.566710\n[123,] 40.99977  7.395186 74.60435 33.604584\n[124,] 41.02058  7.378808 74.66236 33.641776\n[125,] 41.04200  7.363698 74.72030 33.678302\n[126,] 41.06400  7.349829 74.77818 33.714175\n[127,] 41.08659  7.337178 74.83600 33.749408\n[128,] 41.10974  7.325720 74.89375 33.784016\n[129,] 41.13344  7.315431 74.95145 33.818010\n[130,] 41.15769  7.306287 75.00909 33.851403\n[131,] 41.18247  7.298265 75.06668 33.884209\n[132,] 41.20778  7.291343 75.12422 33.916438\n[133,] 41.23360  7.285497 75.18170 33.948103\n[134,] 41.25992  7.280706 75.23914 33.979216\n[135,] 41.28674  7.276949 75.29652 34.009787\n[136,] 41.31403  7.274202 75.35386 34.039828\n[137,] 41.34180  7.272447 75.41114 34.069349\n[138,] 41.37002  7.271662 75.46838 34.098361\n[139,] 41.39870  7.271826 75.52557 34.126874\n[140,] 41.42782  7.272920 75.58272 34.154898\n[141,] 41.45737  7.274925 75.63981 34.182444\n[142,] 41.48734  7.277820 75.69686 34.209520\n[143,] 41.51772  7.281587 75.75386 34.236136\n[144,] 41.54851  7.286207 75.81081 34.262301\n[145,] 41.57969  7.291662 75.86771 34.288025\n[146,] 41.61125  7.297933 75.92456 34.313316\n[147,] 41.64318  7.305003 75.98137 34.338182\n[148,] 41.67549  7.312855 76.03812 34.362632\n[149,] 41.70814  7.321470 76.09482 34.386674\n[150,] 41.74115  7.330832 76.15147 34.410317\n[151,] 41.77449  7.340925 76.20806 34.433567\n[152,] 41.80817  7.351732 76.26460 34.456433\n[153,] 41.84216  7.363237 76.32108 34.478923\n[154,] 41.87647  7.375423 76.37751 34.501043\n[155,] 41.91108  7.388276 76.43388 34.522800\n[156,] 41.94598  7.401779 76.49018 34.544202\n[157,] 41.98117  7.415919 76.54643 34.565256\n[158,] 42.01665  7.430679 76.60261 34.585968\n[159,] 42.05239  7.446046 76.65873 34.606344\n[160,] 42.08840  7.462004 76.71479 34.626392\n[161,] 42.12466  7.478540 76.77078 34.646118\n[162,] 42.16117  7.495639 76.82669 34.665527\n[163,] 42.19791  7.513288 76.88254 34.684625\n[164,] 42.23489  7.531474 76.93831 34.703419\n[165,] 42.27210  7.550183 76.99401 34.721914\n[166,] 42.30952  7.569402 77.04963 34.740117\n[167,] 42.34715  7.589118 77.10518 34.758031\n[168,] 42.38498  7.609318 77.16064 34.775663\n[169,] 42.42301  7.629990 77.21603 34.793018\n[170,] 42.46122  7.651123 77.27132 34.810101\n[171,] 42.49962  7.672702 77.32654 34.826917\n[172,] 42.53819  7.694718 77.38166 34.843472\n[173,] 42.57693  7.717157 77.43670 34.859769\n[174,] 42.61582  7.740010 77.49164 34.875814\n[175,] 42.65487  7.763263 77.54648 34.891611\n[176,] 42.69407  7.786907 77.60124 34.907165\n[177,] 42.73341  7.810930 77.65589 34.922480\n[178,] 42.77288  7.835321 77.71044 34.937560\n[179,] 42.81248  7.860070 77.76489 34.952410\n[180,] 42.85220  7.885167 77.81923 34.967034\n[181,] 42.89204  7.910601 77.87347 34.981436\n[182,] 42.93198  7.936362 77.92760 34.995619\n[183,] 42.97203  7.962440 77.98162 35.009589\n[184,] 43.01217  7.988825 78.03552 35.023348\n[185,] 43.05241  8.015509 78.08931 35.036900\n[186,] 43.09273  8.042480 78.14298 35.050249\n[187,] 43.13313  8.069731 78.19653 35.063398\n[188,] 43.17360  8.097252 78.24996 35.076352\n[189,] 43.21415  8.125034 78.30326 35.089113\n[190,] 43.25475  8.153067 78.35644 35.101684\n[191,] 43.29541  8.181345 78.40948 35.114070\n[192,] 43.33613  8.209857 78.46240 35.126272\n[193,] 43.37689  8.238595 78.51519 35.138295\n[194,] 43.41769  8.267552 78.56784 35.150142\n[195,] 43.45853  8.296719 78.62035 35.161815\n[196,] 43.49941  8.326088 78.67272 35.173317\n[197,] 43.54030  8.355651 78.72495 35.184651\n[198,] 43.58122  8.385401 78.77704 35.195821\n[199,] 43.62216  8.415329 78.82899 35.206828\n[200,] 43.66311  8.445429 78.88078 35.217676\n[201,] 43.70406  8.475693 78.93243 35.228368\n[202,] 43.74502  8.506114 78.98392 35.238905\n[203,] 43.78597  8.536685 79.03526 35.249290\n[204,] 43.82692  8.567398 79.08645 35.259526\n[205,] 43.86786  8.598247 79.13748 35.269616\n[206,] 43.90879  8.629226 79.18835 35.279561\n[207,] 43.94969  8.660327 79.23906 35.289364\n[208,] 43.99057  8.691544 79.28960 35.299028\n[209,] 44.03142  8.722870 79.33998 35.308554\n[210,] 44.07225  8.754300 79.39019 35.317945\n[211,] 44.11303  8.785827 79.44023 35.327203\n[212,] 44.15378  8.817445 79.49011 35.336331\n[213,] 44.19448  8.849148 79.53981 35.345329\n[214,] 44.23513  8.880931 79.58933 35.354201\n[215,] 44.27573  8.912787 79.63868 35.362947\n[216,] 44.31628  8.944710 79.68785 35.371572\n[217,] 44.35677  8.976696 79.73685 35.380075\n[218,] 44.39720  9.008739 79.78566 35.388459\n[219,] 44.43756  9.040833 79.83429 35.396726\n[220,] 44.47785  9.072974 79.88273 35.404878\n[221,] 44.51807  9.105155 79.93099 35.412916\n[222,] 44.55822  9.137373 79.97906 35.420842\n[223,] 44.59828  9.169621 80.02694 35.428659\n[224,] 44.63826  9.201896 80.07463 35.436366\n[225,] 44.67816  9.234191 80.12213 35.443968\n[226,] 44.71797  9.266503 80.16943 35.451463\n[227,] 44.75768  9.298828 80.21654 35.458856\n[228,] 44.79731  9.331159 80.26345 35.466146\n[229,] 44.83683  9.363494 80.31017 35.473336\n[230,] 44.87625  9.395827 80.35668 35.480427\n[231,] 44.91557  9.428154 80.40299 35.487420\n[232,] 44.95479  9.460471 80.44911 35.494317\n[233,] 44.99389  9.492775 80.49501 35.501120\n[234,] 45.03289  9.525060 80.54072 35.507829\n[235,] 45.07177  9.557323 80.58622 35.514446\n[236,] 45.11053  9.589560 80.63151 35.520973\n[237,] 45.14918  9.621768 80.67659 35.527410\n[238,] 45.18770  9.653941 80.72146 35.533760\n[239,] 45.22610  9.686078 80.76612 35.540023\n[240,] 45.26437  9.718174 80.81057 35.546200\n[241,] 45.30252  9.750225 80.85481 35.552293\n[242,] 45.34053  9.782229 80.89883 35.558303\n[243,] 45.37841  9.814182 80.94264 35.564231\n[244,] 45.41616  9.846080 80.98624 35.570078\n[245,] 45.45377  9.877920 81.02961 35.575846\n[246,] 45.49124  9.909700 81.07277 35.581536\n[247,] 45.52856  9.941416 81.11571 35.587148\n[248,] 45.56575  9.973064 81.15843 35.592684\n[249,] 45.60279 10.004643 81.20093 35.598144\n[250,] 45.63968 10.036148 81.24321 35.603531\n[251,] 45.67642 10.067578 81.28527 35.608844\n[252,] 45.71301 10.098929 81.32710 35.614085\n[253,] 45.74945 10.130199 81.36871 35.619255\n[254,] 45.78574 10.161385 81.41010 35.624355\n[255,] 45.82187 10.192484 81.45126 35.629386\n[256,] 45.85784 10.223495 81.49219 35.634349\n[257,] 45.89366 10.254413 81.53290 35.639244\n[258,] 45.92931 10.285238 81.57338 35.644073\n[259,] 45.96480 10.315965 81.61364 35.648836\n[260,] 46.00013 10.346595 81.65366 35.653535\n[261,] 46.03529 10.377123 81.69346 35.658170\n[262,] 46.07029 10.407547 81.73303 35.662742\n[263,] 46.10512 10.437867 81.77237 35.667252\n[264,] 46.13978 10.468079 81.81148 35.671701\n[265,] 46.17427 10.498181 81.85036 35.676090\n[266,] 46.20859 10.528171 81.88901 35.680419\n[267,] 46.24274 10.558048 81.92743 35.684689\n[268,] 46.27671 10.587809 81.96561 35.688901\n[269,] 46.31051 10.617453 82.00356 35.693056\n[270,] 46.34413 10.646978 82.04129 35.697154\n[271,] 46.37758 10.676382 82.07877 35.701197\n[272,] 46.41085 10.705663 82.11603 35.705184\n[273,] 46.44394 10.734819 82.15305 35.709118\n[274,] 46.47685 10.763850 82.18984 35.712997\n[275,] 46.50958 10.792753 82.22640 35.716824\n[276,] 46.54213 10.821527 82.26272 35.720599\n[277,] 46.57449 10.850171 82.29881 35.724322\n[278,] 46.60668 10.878682 82.33467 35.727994\n[279,] 46.63868 10.907060 82.37029 35.731616\n[280,] 46.67049 10.935303 82.40568 35.735189\n[281,] 46.70212 10.963410 82.44084 35.738713\n[282,] 46.73357 10.991379 82.47576 35.742188\n[283,] 46.76483 11.019210 82.51044 35.745616\n[284,] 46.79590 11.046901 82.54489 35.748997\n[285,] 46.82678 11.074450 82.57911 35.752331\n[286,] 46.85748 11.101858 82.61310 35.755620\n[287,] 46.88799 11.129122 82.64685 35.758864\n[288,] 46.91830 11.156241 82.68037 35.762062\n[289,] 46.94843 11.183216 82.71365 35.765217\n[290,] 46.97837 11.210044 82.74670 35.768328\n[291,] 47.00812 11.236724 82.77952 35.771397\n[292,] 47.03768 11.263257 82.81210 35.774423\n[293,] 47.06705 11.289640 82.84445 35.777407\n[294,] 47.09622 11.315874 82.87657 35.780350\n[295,] 47.12521 11.341956 82.90846 35.783252\n[296,] 47.15400 11.367887 82.94011 35.786113\n[297,] 47.18260 11.393666 82.97154 35.788935\n[298,] 47.21101 11.419292 83.00273 35.791718\n[299,] 47.23923 11.444764 83.03369 35.794462\n[300,] 47.26725 11.470082 83.06442 35.797168\n[301,] 47.29508 11.495245 83.09492 35.799836\n[302,] 47.32272 11.520252 83.12519 35.802467\n[303,] 47.35016 11.545103 83.15523 35.805061\n[304,] 47.37742 11.569798 83.18504 35.807619\n[305,] 47.40448 11.594336 83.21462 35.810141\n[306,] 47.43134 11.618715 83.24397 35.812628\n[307,] 47.45802 11.642937 83.27310 35.815079\n[308,] 47.48450 11.667001 83.30199 35.817496\n[309,] 47.51078 11.690905 83.33066 35.819879\n[310,] 47.53688 11.714651 83.35911 35.822229\n[311,] 47.56278 11.738237 83.38733 35.824545\n[312,] 47.58849 11.761663 83.41532 35.826829\n[313,] 47.61401 11.784929 83.44309 35.829080\n[314,] 47.63933 11.808034 83.47063 35.831299\n[315,] 47.66447 11.830979 83.49795 35.833487\n[316,] 47.68941 11.853763 83.52505 35.835643\n[317,] 47.71416 11.876386 83.55192 35.837769\n[318,] 47.73871 11.898848 83.57858 35.839865\n[319,] 47.76308 11.921148 83.60501 35.841931\n[320,] 47.78725 11.943287 83.63122 35.843967\n[321,] 47.81124 11.965265 83.65721 35.845974\n[322,] 47.83503 11.987081 83.68298 35.847952\n[323,] 47.85864 12.008735 83.70854 35.849902\n[324,] 47.88205 12.030227 83.73387 35.851824\n[325,] 47.90528 12.051558 83.75899 35.853718\n[326,] 47.92831 12.072727 83.78390 35.855584\n[327,] 47.95116 12.093735 83.80858 35.857424\n[328,] 47.97382 12.114581 83.83306 35.859237\n[329,] 47.99629 12.135265 83.85731 35.861024\n[330,] 48.01857 12.155788 83.88136 35.862785\n[331,] 48.04067 12.176149 83.90519 35.864521\n[332,] 48.06258 12.196350 83.92881 35.866231\n[333,] 48.08431 12.216389 83.95222 35.867916\n[334,] 48.10584 12.236267 83.97542 35.869577\n[335,] 48.12720 12.255985 83.99841 35.871213\n[336,] 48.14837 12.275542 84.02119 35.872825\n[337,] 48.16935 12.294939 84.04377 35.874414\n[338,] 48.19016 12.314176 84.06614 35.875980\n[339,] 48.21077 12.333253 84.08830 35.877522\n[340,] 48.23121 12.352170 84.11025 35.879042\n[341,] 48.25147 12.370929 84.13201 35.880539\n[342,] 48.27154 12.389528 84.15356 35.882014\n[343,] 48.29144 12.407968 84.17490 35.883467\n[344,] 48.31115 12.426251 84.19605 35.884899\n[345,] 48.33068 12.444375 84.21699 35.886309\n[346,] 48.35004 12.462341 84.23774 35.887699\n[347,] 48.36922 12.480151 84.25829 35.889068\n[348,] 48.38822 12.497803 84.27864 35.890416\n[349,] 48.40704 12.515299 84.29879 35.891744\n[350,] 48.42569 12.532639 84.31874 35.893053\n[351,] 48.44416 12.549823 84.33851 35.894341\n[352,] 48.46246 12.566852 84.35807 35.895611\n[353,] 48.48059 12.583726 84.37745 35.896861\n[354,] 48.49854 12.600446 84.39663 35.898093\n[355,] 48.51632 12.617012 84.41562 35.899306\n[356,] 48.53393 12.633425 84.43443 35.900500\n[357,] 48.55136 12.649685 84.45304 35.901677\n[358,] 48.56863 12.665793 84.47146 35.902836\n[359,] 48.58573 12.681748 84.48970 35.903977\n[360,] 48.60265 12.697553 84.50775 35.905101\n[361,] 48.61941 12.713206 84.52562 35.906207\n[362,] 48.63601 12.728710 84.54330 35.907297\n[363,] 48.65243 12.744064 84.56080 35.908371\n[364,] 48.66870 12.759268 84.57812 35.909427\n[365,] 48.68479 12.774325 84.59526 35.910468\n\n\nCode\nfanchart(fit.pr, mar = c(2,2,2,2))"
  },
  {
    "objectID": "financial_time_series.html",
    "href": "financial_time_series.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Applying ARCH or GARCH models to the healthcare stocks like Pfizer (PFE), Vertex Pharmaceuticals (VRTX), TEVA Pharmaceutical(TEVA), and Sage Therapeutics (SAGE) can be a valuable method for us to understand and forecast the volatility of these stocks.\nThe ARCH model would allow us to model the volatility as a function of the size of previous time periods’ errors, capturing short-term fluctuations. In contrast, the GARCH model would incorporate both these shocks and the persistent volatility over time, which could provide a more comprehensive understanding of the volatility structure in these healthcare stocks.\nThe decision to use ARCH versus GARCH would depend on the characteristics of the stock’s return series—specifically, whether the volatility shocks are short-lived (suggesting ARCH) or whether they have a more enduring impact (suggesting GARCH). Before applying these models, it’s crucial for us to conduct preliminary analyses such as checking for stationarity, identifying the presence of volatility clustering, and determining the optimal lag length for the models.\n\n\nCode\n# Import Stock Price data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"PFE\", \"VRTX\", \"TEVA\", \"SAGE\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2019-01-01\",\n             to = \"2024-02-02\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(PFE$PFE.Adjusted,\n                    VRTX$VRTX.Adjusted,\n                    TEVA$TEVA.Adjusted,\n                    SAGE$SAGE.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock &lt;- stock %&gt;%\n  rownames_to_column(var = \"date\")\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n# Extract every stock price\npfizer &lt;- getSymbols(\"PFE\", from = \"2019-01-01\", to = \"2024-02-02\", src = \"yahoo\", auto.assign = FALSE)\nvertex &lt;- getSymbols(\"VRTX\", from = \"2019-01-01\", to = \"2024-02-02\", src = \"yahoo\", auto.assign = FALSE)\nteva &lt;- getSymbols(\"TEVA\", from = \"2019-01-01\", to = \"2024-02-02\", src = \"yahoo\", auto.assign = FALSE)\nsage &lt;- getSymbols(\"SAGE\", from = \"2019-01-01\", to = \"2024-02-02\", src = \"yahoo\", auto.assign = FALSE)\n\n\n\nData Visualization\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nchartSeries(pfizer, theme = chartTheme(\"white\"),\n            bar.type = \"hlc\",  \n            up.col = \"green\",  \n            dn.col = \"red\") \n\n\n\n\n\n\n\n\n\nFor Pfizer’s stock price from January 2019 to February 2024. There were fluctuations during 2020, but an overall increase trend can be observed, which is due to the COVID-19 pandemic and the development of the COVID-19 vaccine. However, for recent years, the stock price has been decreasing with some fluctuations.\n\n\n\n\nCode\nchartSeries(vertex, theme = chartTheme(\"white\"),\n            bar.type = \"hlc\",  \n            up.col = \"green\",  \n            dn.col = \"red\") \n\n\n\n\n\n\n\n\n\nFor Vertex Pharmaceuticals’ stock price from January 2019 to February 2024, the stock price has been decreasing in the beginning of 2020, but it has been increasing significantly since March 2022, with some fluctuations. The significant increase in Vertex stock price could be attributed to several factors: Successful Drug Developments, Strategic Partnerships or Acquisitions with other companies for drug development or expansion into new markets, and Breakthroughs in biotechnology or patents for new drug formulas.\n\n\n\n\nCode\nchartSeries(teva, theme = chartTheme(\"white\"),\n            bar.type = \"hlc\",  \n            up.col = \"green\",  \n            dn.col = \"red\") \n\n\n\n\n\n\n\n\n\nFor TEVA stock price from January 2019 to February 2024. The stock price has been decreasing hugely since 2019, reaching its lowest point in 2020. Since then, the stock price has been relatively stable with fluctuations.\n\n\n\n\nCode\nchartSeries(sage, theme = chartTheme(\"white\"),\n            bar.type = \"hlc\",  \n            up.col = \"green\",  \n            dn.col = \"red\") \n\n\n\n\n\n\n\n\n\nFor Sage Therapeutics’ stock price from January 2019 to February 2024, the stock price has been decreasing since the middle of 2019 too, with some fluctuations. Then it has been increasing, reaching the peak in 2021, and then decreasing a little again, then stay stable with some fluctuations.\n\n\n\n\n\nStock Returns\n\nPfizerVertexTevaSage\n\n\n\n\nCode\npfe_ts &lt;- ts(pfizer$PFE.Adjusted, start=decimal_date(as.Date(\"2019-01-01\")), frequency = 365.25)\n\nreturns_pfe = log(pfe_ts) %&gt;% \n  diff()\n\n# Plot the PFE returns\nautoplot(returns_pfe, color=\"#27aeef\") + theme_bw() +ggtitle(\"Pfizer Returns\")\n\n\n\n\n\n\n\n\n\nThe returns of Pfizer stock price shows clear volatility clustering. The returns are centered around zero, with some extreme values indicating high volatility. The periods of high volatility are around the beginning of 2020 and 2021, which could be due to the COVID-19 pandemic and the development of the COVID-19 vaccine.\n\n\n\n\nCode\nvertex_ts &lt;- ts(vertex$VRTX.Adjusted, start=decimal_date(as.Date(\"2019-01-01\")), frequency = 365.25)\n\nreturns_vertex = log(vertex_ts) %&gt;% \n  diff()\n\n# Plot the VRTX returns\nautoplot(returns_vertex, color=\"#27aeef\") + theme_bw() +ggtitle(\"Vertex Returns\")\n\n\n\n\n\n\n\n\n\nThe returns of Vertex stock price are centered around zero, with some extreme values indicating high volatility. The periods of high volatility are in the end of 2019, the middle of 2020, and the middle of 2022, which could be due to the company’s financial performance, drug developments, or COVID-19 pandemic.\n\n\n\n\nCode\nteva_ts &lt;- ts(teva$TEVA.Adjusted, start=decimal_date(as.Date(\"2019-01-01\")), frequency = 365.25)\n\nreturns_teva = log(teva_ts) %&gt;% \n  diff()\n\n# Plot the TEVA returns\nautoplot(returns_teva, color=\"#27aeef\") + theme_bw() +ggtitle(\"Teva Returns\")\n\n\n\n\n\n\n\n\n\nThe returns of TEVA stock price shows clear volatility clustering. The returns are centered around zero, with some extreme values indicating high volatility. The periods of high volatility are around the end of 2019, and there was a high spike in the middle of 2021, which could be due to the company’s financial performance, drug developments, or COVID-19 pandemic.\n\n\n\n\nCode\nsage_ts &lt;- ts(sage$SAGE.Adjusted, start=decimal_date(as.Date(\"2019-01-01\")), frequency = 365.25)\n\nreturns_sage = log(sage_ts) %&gt;% \n  diff()\n\n# Plot the SAGE returns\nautoplot(returns_sage, color=\"#27aeef\") + theme_bw() +ggtitle(\"Sage Returns\")\n\n\n\n\n\n\n\n\n\nThe Sage Therapeutics’ stock returns are fluctuating around zero, but two large negative spikes of the volatility stand out—one in late 2019 and another in early 2022. These could indicate significant stock price drops, possibly due to negative clinical trial results, regulatory setbacks, poor financial performance, or COVID-19. Between these events, returns show regular volatility, which is common in biotech stocks due to the high-impact nature of news on drug development progress.\n\n\n\n\n\nACF/PACF Plots\n\nPfizerVertexTevaSage\n\n\nAfter analyzing the ACF and PACF plots for Pfizer’s stock returns, we can see significant lags at p = 1, 9, 10, q = 1, 9, 10. This indicates that the returns are not stationary and exhibit significant autocorrelation at these lags. Besides, we can see the returns of Pfizer are most stationary without high correlation.\n\n\nCode\nplot1&lt;-ggAcf(returns_pfe, 40)+ggtitle(\"Pfizer Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_pfe, 40)+theme_bw()+ggtitle(\"Pfizer Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nHere, I will check the ACF and PACF plots for the absolute returns of Pfizer’s stock price. We can see that the PACF plot shows significant lags at p = 1, 2, 3, 4, 5, 8, and the ACF plot shows significant lags at q = 1 to 10 and so on.\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_pfe), 40)+ggtitle(\"Pfizer Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_pfe), 40)+theme_bw()+ggtitle(\"Pfizer Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nThen, let’s check the ACF and PACF plots for the squared returns of Pfizer’s stock price. We can see that the PACF plot shows significant lags at p = 1, 2, 3, 4, 5, 7, 8, and the ACF plot shows significant lags at q = 1 to 10 and so on.\n\n\nCode\nplot1&lt;-ggAcf(returns_pfe^2, 40)+ggtitle(\"Pfizer Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_pfe^2, 40)+theme_bw()+ggtitle(\"Pfizer Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots for Vertex’s stock returns, we can see significant lags at p = 1, 6, 7, 8, 9, q = 1, 6, 7, 8, 9. This indicates that the returns are not stationary and exhibit significant autocorrelation at these lags. Besides, we can see the returns of Vertex are most stationary without high correlation.\n\n\nCode\nplot1&lt;-ggAcf(returns_vertex, 40)+ggtitle(\"Vertex Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_vertex, 40)+theme_bw()+ggtitle(\"Vertex Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nHere, I will check the ACF and PACF plots for the absolute returns of Vertex’s stock price. We can see that the PACF plot shows significant lags at p = 1, 2, 3, 4, 5, 6, 7, 9, and the ACF plot shows significant lags at q = 1 to 10 and so on.\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_vertex), 40)+ggtitle(\"Vertex Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_vertex), 40)+theme_bw()+ggtitle(\"Vertex Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nThen, let’s check the ACF and PACF plots for the squared returns of Vertex’s stock price. We can see that the PACF plot shows significant lags at p = 1, and the ACF plot shows significant lags at q = 1, 7, 9.\n\n\nCode\nplot1&lt;-ggAcf(returns_vertex^2, 40)+ggtitle(\"Vertex Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_vertex^2, 40)+theme_bw()+ggtitle(\"Vertex Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots for TEVA’s stock returns, we can see significant lags at p = 14, q = 14. This indicates that the returns are not stationary and exhibit significant autocorrelation at these lags. Besides, we can see the returns of TEVA are most stationary without high correlation.\n\n\nCode\nplot1&lt;-ggAcf(returns_teva, 40)+ggtitle(\"TEVA Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_teva, 40)+theme_bw()+ggtitle(\"TEVA Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nHere, I will check the ACF and PACF plots for the absolute returns of Teva’s stock price. We can see that the PACF plot shows significant lags at p = 1, 2, 3, 4, 6, 7, and the ACF plot shows significant lags at q = 1 to 10 and so on.\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_teva), 40)+ggtitle(\"TEVA Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_teva), 40)+theme_bw()+ggtitle(\"TEVA Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nThen, let’s check the ACF and PACF plots for the squared returns of Teva’s stock price. We can see that the PACF plot shows significant lags at p = 1, 2, 5, 10 and the ACF plot shows significant lags at q = 1, 2, 3, 4, 5, 10.\n\n\nCode\nplot1&lt;-ggAcf(returns_teva^2, 40)+ggtitle(\"TEVA Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_teva^2, 40)+theme_bw()+ggtitle(\"TEVA Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots for SAGE’s stock returns, we can see significant lags at p = 4, 8, q = 4, 8. This indicates that the returns are not stationary and exhibit significant autocorrelation at these lags. Besides, we can see the returns of SAGE are most stationary without high correlation.\n\n\nCode\nplot1&lt;-ggAcf(returns_sage, 40)+ggtitle(\"SAGE Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_sage, 40)+theme_bw()+ggtitle(\"SAGE Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nHere, I will check the ACF and PACF plots for the absolute returns of Sage’s stock price. We can see that the PACF plot shows significant lags at p = 1, 4, and the ACF plot shows significant lags at q = 1, 4, 8.\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_sage), 40)+ggtitle(\"SAGE Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_sage), 40)+theme_bw()+ggtitle(\"SAGE Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nThen, let’s check the ACF and PACF plots for the squared returns of Sage’s stock price. We can see that none of the lags are significant in the PACF plot and the ACF plot.\n\n\nCode\nplot1&lt;-ggAcf(returns_sage^2, 40)+ggtitle(\"SAGE Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_sage^2, 40)+theme_bw()+ggtitle(\"SAGE Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nARCH Test\nIn this section, I’ll use the ARCH test to check for the presence of ARCH effects in the stock returns of Pfizer, Vertex, Teva, and Sage. The null hypothesis of the ARCH test is that there are no ARCH effects (the presence of heteroskedasticity or varying volatility) in the data. If the p-value is less than 0.05, then we can reject the null hypothesis and conclude that there are ARCH effects in the data. The ARCH test is often applied to financial market data where volatility clustering occurs—periods of high volatility are followed by high volatility, and periods of low volatility follow low volatility. The ARCH test evaluates whether past error terms can predict future variance, implying that the error terms have a pattern rather than being random. If the ARCH effect is present, models such as ARCH or GARCH might be suitable for capturing this characteristic in the data.\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nArchTest(returns_pfe, lags=31, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns_pfe\nChi-squared = 205.62, df = 31, p-value &lt; 2.2e-16\n\n\nAs we can see, the p-value of the ARCH test for Pfizer’s stock returns is less than 0.05, indicating that there are ARCH effects in the data. This suggests that the volatility of Pfizer’s stock returns is not constant and exhibits clustering behavior.\n\n\n\n\nCode\nArchTest(returns_vertex, lags=31, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns_vertex\nChi-squared = 22.27, df = 31, p-value = 0.8743\n\n\nAs we can see, the p-value of the ARCH test for Vertex’s stock returns is more than 0.05, indicating that there are not ARCH effects in the data. This suggests that the volatility of Vertex’s stock returns is constant and may not exhibit clustering behavior.\n\n\n\n\nCode\nArchTest(returns_teva, lags=31, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns_teva\nChi-squared = 69.941, df = 31, p-value = 7.781e-05\n\n\nAs we can see, the p-value of the ARCH test for Teva’s stock returns is less than 0.05, indicating that there are ARCH effects in the data. This suggests that the volatility of Teva’s stock returns is not constant and may exhibit clustering behavior.\n\n\n\n\nCode\nArchTest(returns_sage, lags=31, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns_sage\nChi-squared = 0.64864, df = 31, p-value = 1\n\n\nAs we can see, the p-value of the ARCH test for Sage’s stock returns is more than 0.05, indicating that there are no ARCH effects in the data. This suggests that the volatility of Sage’s stock returns is constant and may not exhibit clustering behavior.\n\n\n\n\n\nFitting the ARIMA Model\nIn this section, I’ll fit an ARIMA model on the returns firstly.\n\nPfizerVertexTevaSage\n\n\nAs we can see from the ACF and PACF plots before, the data is already mostly stationary, so it’s not necessary to difference the data.\n\n\nCode\n# p = 1:10, d = 0, q = 1:10\n\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*15),nrow=15)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:0)#\n    {\n      \n      if(p+d+q&lt;=6)\n      {\n        \n        model&lt;- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n  \n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput &lt;- ARIMA.c(1,10,1,10,data=returns_pfe)\noutput\n\n\n   p d q       AIC       BIC      AICc\n1  1 0 1 -6822.726 -6802.110 -6822.694\n2  1 0 2 -6832.033 -6806.264 -6831.986\n3  1 0 3 -6835.574 -6804.651 -6835.508\n4  1 0 4 -6833.607 -6797.530 -6833.519\n5  1 0 5 -6833.676 -6792.446 -6833.563\n6  2 0 1 -6823.261 -6797.492 -6823.214\n7  2 0 2 -6821.884 -6790.961 -6821.818\n8  2 0 3 -6833.560 -6797.483 -6833.472\n9  2 0 4 -6832.607 -6791.376 -6832.494\n10 3 0 1 -6836.299 -6805.376 -6836.233\n11 3 0 2 -6834.267 -6798.190 -6834.178\n12 3 0 3 -6839.576 -6798.345 -6839.463\n13 4 0 1 -6834.316 -6798.239 -6834.228\n14 4 0 2 -6833.431 -6792.200 -6833.317\n15 5 0 1 -6835.338 -6794.107 -6835.225\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q       AIC       BIC      AICc\n12 3 0 3 -6839.576 -6798.345 -6839.463\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n2 1 0 2 -6832.033 -6806.264 -6831.986\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q       AIC       BIC      AICc\n12 3 0 3 -6839.576 -6798.345 -6839.463\n\n\nAs we can see, the best model according to BIC is ARIMA(1,0,2). This model’s AIC is quite close to the other two models, but the BIC is the lowest among the three. Considering the principle of parsimony, we’ll choose ARIMA(1,0,2) as the best model for Pfizer’s stock returns.\n\n\nCode\nfit_pfe &lt;- Arima(returns_pfe,order=c(1,0,2))\nsummary(fit_pfe)\n\n\nSeries: returns_pfe \nARIMA(1,0,2) with non-zero mean \n\nCoefficients:\n          ar1     ma1      ma2    mean\n      -0.8619  0.8208  -0.0850  -2e-04\ns.e.   0.0494  0.0539   0.0272   4e-04\n\nsigma^2 = 0.000279:  log likelihood = 3421.02\nAIC=-6832.03   AICc=-6831.99   BIC=-6806.26\n\nTraining set error measures:\n                        ME       RMSE       MAE MPE MAPE      MASE         ACF1\nTraining set -1.440239e-06 0.01667706 0.0118512 NaN  Inf 0.6645112 -0.005143767\n\n\nAfter fitting ARIMA(1,0,2) to the data, we can see that there is one insignificant coefficient in the model on the 0.05 significance level, which is MA(1).\n\n\nAs we can see from the ACF and PACF plots before, the data is already mostly stationary, so it’s not necessary to difference the data.\n\n\nCode\n# p = 1:9, d = 0, q = 1:9\n\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*15),nrow=15)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:0)#\n    {\n      \n      if(p+d+q&lt;=6)\n      {\n        \n        model&lt;- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n  \n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput &lt;- ARIMA.c(1,9,1,9,data=returns_vertex)\noutput\n\n\n   p d q       AIC       BIC      AICc\n1  1 0 1 -6364.220 -6343.605 -6364.189\n2  1 0 2 -6362.400 -6336.631 -6362.353\n3  1 0 3 -6362.833 -6331.910 -6362.767\n4  1 0 4 -6361.191 -6325.114 -6361.103\n5  1 0 5 -6371.612 -6330.382 -6371.499\n6  2 0 1 -6361.343 -6335.574 -6361.296\n7  2 0 2 -6362.452 -6331.529 -6362.386\n8  2 0 3 -6359.692 -6323.615 -6359.604\n9  2 0 4 -6358.850 -6317.619 -6358.737\n10 3 0 1 -6361.252 -6330.329 -6361.186\n11 3 0 2 -6360.449 -6324.372 -6360.360\n12 3 0 3 -6392.453 -6351.222 -6392.339\n13 4 0 1 -6361.238 -6325.161 -6361.149\n14 4 0 2 -6359.234 -6318.003 -6359.120\n15 5 0 1 -6380.442 -6339.212 -6380.329\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q       AIC       BIC      AICc\n12 3 0 3 -6392.453 -6351.222 -6392.339\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q       AIC       BIC      AICc\n12 3 0 3 -6392.453 -6351.222 -6392.339\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q       AIC       BIC      AICc\n12 3 0 3 -6392.453 -6351.222 -6392.339\n\n\nAs we can see, the best model according to AIC, BIC, and AICc is ARIMA(3,0,3).\n\n\nCode\nfit_vertex &lt;- Arima(returns_vertex,order=c(3,0,3))\nsummary(fit_vertex)\n\n\nSeries: returns_vertex \nARIMA(3,0,3) with non-zero mean \n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3   mean\n      -0.8653  0.5634  0.8015  0.7552  -0.6125  -0.7704  7e-04\ns.e.   0.5090  0.8625  0.4467  0.4731   0.7374   0.3561  4e-04\n\nsigma^2 = 0.0003924:  log likelihood = 3204.23\nAIC=-6392.45   AICc=-6392.34   BIC=-6351.22\n\nTraining set error measures:\n                       ME       RMSE        MAE      MPE    MAPE      MASE\nTraining set 1.732898e-05 0.01975556 0.01366113 109.6771 183.909 0.6694657\n                    ACF1\nTraining set -0.01221804\n\n\nAfter fitting ARIMA(3,0,3) to the data, we can see that there is no significant coefficient in the model on the 0.05 significance level.\n\n\nAs we can see from the ACF and PACF plots before, the data is already mostly stationary, so it’s not necessary to difference the data.\n\n\nCode\n# p = 1:14, d = 0, q = 1:14\n\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*20),nrow=20)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:0)#\n    {\n      \n      if(p+d+q&lt;=6)\n      {\n        \n        model&lt;- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n  \n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput &lt;- ARIMA.c(1,14,1,14,data=returns_teva)\noutput\n\n\n    p  d  q       AIC       BIC      AICc\n1   1  0  1 -5264.767 -5244.152 -5264.736\n2   1  0  2 -5265.205 -5239.436 -5265.158\n3   1  0  3 -5263.546 -5232.623 -5263.480\n4   1  0  4 -5261.897 -5225.820 -5261.809\n5   1  0  5 -5260.198 -5218.967 -5260.084\n6   2  0  1 -5265.099 -5239.330 -5265.052\n7   2  0  2 -5263.838 -5232.915 -5263.771\n8   2  0  3 -5261.505 -5225.428 -5261.417\n9   2  0  4 -5259.905 -5218.674 -5259.791\n10  3  0  1 -5263.442 -5232.519 -5263.376\n11  3  0  2 -5261.706 -5225.629 -5261.618\n12  3  0  3 -5259.927 -5218.696 -5259.813\n13  4  0  1 -5261.784 -5225.707 -5261.695\n14  4  0  2 -5259.854 -5218.623 -5259.740\n15  5  0  1 -5261.925 -5220.695 -5261.812\n16 NA NA NA        NA        NA        NA\n17 NA NA NA        NA        NA        NA\n18 NA NA NA        NA        NA        NA\n19 NA NA NA        NA        NA        NA\n20 NA NA NA        NA        NA        NA\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n  p d q       AIC       BIC      AICc\n2 1 0 2 -5265.205 -5239.436 -5265.158\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n1 1 0 1 -5264.767 -5244.152 -5264.736\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n2 1 0 2 -5265.205 -5239.436 -5265.158\n\n\nAs we can see, the best model according to BIC is ARIMA(1,0,1). This model’s AIC is quite close to the other two models, but the BIC is the lowest among the three. Considering the principle of parsimony, we’ll choose ARIMA(1,0,1) as the best model for Teva’s stock returns.\n\n\nCode\nfit_teva &lt;- Arima(returns_teva,order=c(1,0,1))\nsummary(fit_teva)\n\n\nSeries: returns_teva \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ma1    mean\n      0.8183  -0.7890  -2e-04\ns.e.  0.0971   0.1027   1e-03\n\nsigma^2 = 0.0009509:  log likelihood = 2636.38\nAIC=-5264.77   AICc=-5264.74   BIC=-5244.15\n\nTraining set error measures:\n                        ME       RMSE        MAE MPE MAPE      MASE        ACF1\nTraining set -5.764348e-06 0.03080044 0.02156897 NaN  Inf 0.6871996 -0.03170607\n\n\nAfter fitting ARIMA(1,0,1) to the data, we can see that there is no significant coefficient in the model on the 0.05 significance level.\n\n\nAs we can see from the ACF and PACF plots before, the data is already mostly stationary, so it’s not necessary to difference the data.\n\n\nCode\n# p = 1:8, d = 0, q = 1:8\n\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*15),nrow=15)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:0)#\n    {\n      \n      if(p+d+q&lt;=6)\n      {\n        \n        model&lt;- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n  \n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput &lt;- ARIMA.c(1,8,1,8,data=returns_sage)\noutput\n\n\n   p d q       AIC       BIC      AICc\n1  1 0 1 -4072.590 -4051.974 -4072.558\n2  1 0 2 -4070.626 -4044.857 -4070.579\n3  1 0 3 -4071.760 -4040.837 -4071.694\n4  1 0 4 -4076.818 -4040.741 -4076.730\n5  1 0 5 -4075.114 -4033.883 -4075.000\n6  2 0 1 -4071.656 -4045.886 -4071.608\n7  2 0 2 -4072.263 -4041.340 -4072.197\n8  2 0 3 -4081.603 -4045.526 -4081.515\n9  2 0 4 -4081.700 -4040.470 -4081.587\n10 3 0 1 -4069.809 -4038.886 -4069.743\n11 3 0 2 -4069.712 -4033.635 -4069.624\n12 3 0 3 -4068.737 -4027.506 -4068.623\n13 4 0 1 -4077.946 -4041.869 -4077.858\n14 4 0 2 -4081.600 -4040.369 -4081.487\n15 5 0 1 -4076.424 -4035.194 -4076.311\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n  p d q     AIC      BIC      AICc\n9 2 0 4 -4081.7 -4040.47 -4081.587\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC       BIC      AICc\n1 1 0 1 -4072.59 -4051.974 -4072.558\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q     AIC      BIC      AICc\n9 2 0 4 -4081.7 -4040.47 -4081.587\n\n\nAs we can see, the best model according to BIC is ARIMA(1,0,1). This model’s AIC is quite close to the other two models, but the BIC is the lowest among the three. Considering the principle of parsimony, we’ll choose ARIMA(1,0,1) as the best model for Sage’s stock returns.\n\n\nCode\nfit_sage &lt;- Arima(returns_sage,order=c(1,0,1))\nsummary(fit_sage)\n\n\nSeries: returns_sage \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n          ar1     ma1     mean\n      -0.1448  0.1064  -0.0010\ns.e.   0.4693  0.4711   0.0013\n\nsigma^2 = 0.002415:  log likelihood = 2040.29\nAIC=-4072.59   AICc=-4072.56   BIC=-4051.97\n\nTraining set error measures:\n                        ME       RMSE        MAE MPE MAPE      MASE\nTraining set -4.692071e-06 0.04908677 0.02653977 NaN  Inf 0.6817242\n                     ACF1\nTraining set 0.0002227219\n\n\nAfter fitting ARIMA(1,0,1) to the data, we can see that there is no significant coefficient in the model on the 0.05 significance level.\n\n\n\n\n\nResiduals Analysis\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nsarima(returns_pfe, 1,0,2)\n\n\ninitial  value -4.088329 \niter   2 value -4.088438\niter   3 value -4.090035\niter   4 value -4.090037\niter   5 value -4.090037\niter   6 value -4.090042\niter   7 value -4.090046\niter   8 value -4.090051\niter   9 value -4.090055\niter  10 value -4.090060\niter  11 value -4.090064\niter  12 value -4.090071\niter  13 value -4.090078\niter  14 value -4.090081\niter  15 value -4.090082\niter  16 value -4.090083\niter  17 value -4.090084\niter  18 value -4.090088\niter  19 value -4.090098\niter  20 value -4.090110\niter  21 value -4.090110\niter  22 value -4.090111\niter  23 value -4.090120\niter  24 value -4.090127\niter  25 value -4.090136\niter  26 value -4.090144\niter  27 value -4.090161\niter  28 value -4.090203\niter  29 value -4.090244\niter  30 value -4.090258\niter  31 value -4.090281\niter  32 value -4.090300\niter  33 value -4.090307\niter  34 value -4.090308\niter  35 value -4.090309\niter  36 value -4.090312\niter  37 value -4.090321\niter  38 value -4.090328\niter  39 value -4.090329\niter  40 value -4.090330\niter  41 value -4.090331\niter  42 value -4.090336\niter  43 value -4.090347\niter  44 value -4.090390\niter  45 value -4.090458\niter  46 value -4.090527\niter  47 value -4.090571\niter  48 value -4.090575\niter  49 value -4.090594\niter  50 value -4.090640\niter  51 value -4.090714\niter  52 value -4.090804\niter  53 value -4.090816\niter  54 value -4.090844\niter  55 value -4.090880\niter  56 value -4.091043\niter  57 value -4.091167\niter  58 value -4.091292\niter  59 value -4.091342\niter  60 value -4.091395\niter  61 value -4.091403\niter  62 value -4.091463\niter  63 value -4.091507\niter  64 value -4.091541\niter  65 value -4.091542\niter  66 value -4.091542\niter  66 value -4.091542\niter  66 value -4.091542\nfinal  value -4.091542 \nconverged\ninitial  value -4.092815 \niter   2 value -4.092851\niter   3 value -4.092904\niter   4 value -4.093147\niter   5 value -4.093355\niter   6 value -4.093551\niter   7 value -4.093677\niter   8 value -4.093690\niter   9 value -4.093694\niter  10 value -4.093695\niter  11 value -4.093695\niter  12 value -4.093696\niter  13 value -4.093697\niter  13 value -4.093697\niter  13 value -4.093697\nfinal  value -4.093697 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE  t.value p.value\nar1    -0.8619 0.0494 -17.4476  0.0000\nma1     0.8208 0.0539  15.2260  0.0000\nma2    -0.0850 0.0272  -3.1296  0.0018\nxmean  -0.0002 0.0004  -0.3608  0.7183\n\nsigma^2 estimated as 0.0002781243 on 1275 degrees of freedom \n \nAIC = -5.341699  AICc = -5.341674  BIC = -5.321551 \n \n\n\n\n\n\n\n\n\n\nCode\npfe.res&lt;-fit_pfe$residuals\n\n\nAs we can see from the Standardized Residuals plot, there are still volatility clustering around the end of 2019, and 2021. The ACF plot for the residuals shows that the residuals are mostly random, implying that the model has captured the serial correlation in the data effectively. The Quantile-Quantile (Q-Q) plot suggests the residuals are approximately normally distributed. And the Ljung-Box plot shows that some of the p-values are above the 0.05 significance level, which is a good sign.\n\n\nCode\nplot1&lt;-ggAcf(pfe.res, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(pfe.res, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot3&lt;-ggAcf(pfe.res^2, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot4&lt;- ggPacf(pfe.res^2, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot3, plot4,nrow=2)\n\n\n\n\n\n\n\n\n\nAfter examining the ACF and PACF plots of the residuals and squared residuals, we can see that there are still some correlations at lag 1,2,3,4,5 and so on, which means that the model is not capturing all the information in the data. Fitting a GARCH model might help us capture other correlations.\n\n\n\n\nCode\nsarima(returns_vertex, 3,0,3)\n\n\ninitial  value -3.899871 \niter   2 value -3.908685\niter   3 value -3.912691\niter   4 value -3.913050\niter   5 value -3.913227\niter   6 value -3.913252\niter   7 value -3.913474\niter   8 value -3.913605\niter   9 value -3.913669\niter  10 value -3.913687\niter  11 value -3.913717\niter  12 value -3.913779\niter  13 value -3.913868\niter  14 value -3.913967\niter  15 value -3.914071\niter  16 value -3.914246\niter  17 value -3.914296\niter  18 value -3.914441\niter  19 value -3.914578\niter  20 value -3.914626\niter  21 value -3.914682\niter  22 value -3.914694\niter  23 value -3.914709\niter  24 value -3.914749\niter  25 value -3.914823\niter  26 value -3.914923\niter  27 value -3.915093\niter  28 value -3.916724\niter  29 value -3.916990\niter  30 value -3.917331\niter  31 value -3.917471\niter  32 value -3.918120\niter  33 value -3.918294\niter  34 value -3.918946\niter  35 value -3.919466\niter  36 value -3.921044\niter  37 value -3.921841\niter  38 value -3.922949\niter  39 value -3.923054\niter  40 value -3.923703\niter  41 value -3.923992\niter  42 value -3.924503\niter  43 value -3.924709\niter  44 value -3.924723\niter  45 value -3.924752\niter  46 value -3.924929\niter  47 value -3.924985\niter  48 value -3.925061\niter  49 value -3.925116\niter  50 value -3.925198\niter  51 value -3.925209\niter  52 value -3.925213\niter  53 value -3.925217\niter  54 value -3.925221\niter  55 value -3.925222\niter  56 value -3.925223\niter  57 value -3.925223\niter  58 value -3.925224\niter  58 value -3.925224\nfinal  value -3.925224 \nconverged\ninitial  value -3.923553 \niter   2 value -3.923572\niter   3 value -3.923621\niter   4 value -3.923700\niter   5 value -3.923788\niter   6 value -3.923838\niter   7 value -3.923910\niter   8 value -3.923997\niter   9 value -3.924084\niter  10 value -3.924140\niter  11 value -3.924167\niter  12 value -3.924182\niter  13 value -3.924194\niter  14 value -3.924197\niter  15 value -3.924198\niter  15 value -3.924198\niter  15 value -3.924198\nfinal  value -3.924198 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1    -0.8653 0.5090 -1.7001  0.0894\nar2     0.5634 0.8625  0.6533  0.5137\nar3     0.8015 0.4467  1.7942  0.0730\nma1     0.7552 0.4731  1.5962  0.1107\nma2    -0.6125 0.7374 -0.8307  0.4063\nma3    -0.7704 0.3561 -2.1636  0.0307\nxmean   0.0007 0.0004  1.7767  0.0759\n\nsigma^2 estimated as 0.0003902822 on 1272 degrees of freedom \n \nAIC = -4.998008  AICc = -4.997939  BIC = -4.965772 \n \n\n\n\n\n\n\n\n\n\nCode\nvertex.res&lt;-fit_vertex$residuals\n\n\nAs we can see from the Standardized Residuals plot, the flat, relatively constant line without patterns or structures suggests that the model has captured the time series data’s dynamics reasonably well. There’s no apparent volatility clustering or obvious trends remaining, which is a good sign. The ACF plot for the residuals shows that the residuals are mostly random, implying that the model has captured the serial correlation in the data effectively. The Quantile-Quantile (Q-Q) plot suggests the residuals are approximately normally distributed. And the Ljung-Box plot shows that many of the p-values are above the 0.05 significance level, which is a good sign.\n\n\nCode\nplot1&lt;-ggAcf(vertex.res, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\nplot2&lt;- ggPacf(vertex.res, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot3&lt;-ggAcf(vertex.res^2, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\nplot4&lt;- ggPacf(vertex.res^2, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\ngrid.arrange(plot3, plot4,nrow=2)\n\n\n\n\n\n\n\n\n\nAs we can see from the Standardized Residuals plot, and the ACF and PACF plots of the residuals and squared residuals, there’s no need to fit a GARCH model to the residuals. The residuals are approximately normally distributed, and there’s no significant autocorrelation in the residuals, which aligns with the conclusion we got from the ARCH test.\n\n\n\n\nCode\nsarima(returns_teva, 1,0,1)\n\n\ninitial  value -3.479201 \niter   2 value -3.479203\niter   3 value -3.479462\niter   4 value -3.479578\niter   5 value -3.479633\niter   6 value -3.479801\niter   7 value -3.480049\niter   8 value -3.480054\niter   9 value -3.480059\niter  10 value -3.480374\niter  11 value -3.480864\niter  12 value -3.480938\niter  13 value -3.480945\niter  14 value -3.481047\niter  15 value -3.481086\niter  16 value -3.481122\niter  17 value -3.481177\niter  18 value -3.481486\niter  19 value -3.481510\niter  20 value -3.481512\niter  21 value -3.481512\niter  22 value -3.481514\niter  23 value -3.481514\niter  23 value -3.481514\niter  23 value -3.481514\nfinal  value -3.481514 \nconverged\ninitial  value -3.480199 \niter   2 value -3.480206\niter   3 value -3.480209\niter   4 value -3.480210\niter   5 value -3.480215\niter   6 value -3.480218\niter   7 value -3.480219\niter   8 value -3.480219\niter   9 value -3.480219\niter  10 value -3.480222\niter  11 value -3.480222\niter  12 value -3.480222\niter  13 value -3.480223\niter  14 value -3.480223\niter  15 value -3.480224\niter  15 value -3.480224\nfinal  value -3.480224 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.8183 0.0971  8.4294  0.0000\nma1    -0.7890 0.1027 -7.6792  0.0000\nxmean  -0.0002 0.0010 -0.1749  0.8612\n\nsigma^2 estimated as 0.000948667 on 1276 degrees of freedom \n \nAIC = -4.116315  AICc = -4.116301  BIC = -4.100197 \n \n\n\n\n\n\n\n\n\n\nCode\nteva.res&lt;-fit_teva$residuals\n\n\nAs we can see from the Standardized Residuals plot, there are still volatility clustering or high volatility around the end of 2019, 2020, and 2022. The ACF plot for the residuals shows that the residuals are mostly random, implying that the model has captured the serial correlation in the data effectively. The Quantile-Quantile (Q-Q) plot suggests the residuals are approximately normally distributed. And the Ljung-Box plot shows that all of the p-values are above the 0.05 significance level, which is a good sign.\n\n\nCode\nplot1&lt;-ggAcf(teva.res, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\nplot2&lt;- ggPacf(teva.res, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot3&lt;-ggAcf(teva.res^2, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\nplot4&lt;- ggPacf(teva.res^2, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\ngrid.arrange(plot3, plot4,nrow=2)\n\n\n\n\n\n\n\n\n\nAfter examining the ACF and PACF plots of the residuals and squared residuals, we can see that there are still some correlations at lag 1,2,3,4,5 and 10, which means that the model is not capturing all the information in the data. Fitting a GARCH model might help us capture other correlations.\n\n\n\n\nCode\nsarima(returns_sage, 1,0,1)\n\n\ninitial  value -3.013023 \niter   2 value -3.013493\niter   3 value -3.013753\niter   4 value -3.013753\niter   5 value -3.013758\niter   6 value -3.013765\niter   7 value -3.013771\niter   8 value -3.013774\niter   9 value -3.013774\niter  10 value -3.013774\niter  11 value -3.013775\niter  12 value -3.013775\niter  13 value -3.013775\niter  13 value -3.013775\niter  13 value -3.013775\nfinal  value -3.013775 \nconverged\ninitial  value -3.014165 \niter   1 value -3.014165\nfinal  value -3.014165 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1    -0.1448 0.4693 -0.3085  0.7578\nma1     0.1064 0.4711  0.2259  0.8213\nxmean  -0.0010 0.0013 -0.7249  0.4686\n\nsigma^2 estimated as 0.002409511 on 1276 degrees of freedom \n \nAIC = -3.184198  AICc = -3.184184  BIC = -3.16808 \n \n\n\n\n\n\n\n\n\n\nCode\nsage.res&lt;-fit_sage$residuals\n\n\nAs we can see from the Standardized Residuals plot, the flat, relatively constant line without patterns or structures suggests that the model has captured the time series data’s dynamics reasonably well. There’s no apparent volatility clustering or obvious trends remaining, which is a good sign. The ACF plot for the residuals shows that the residuals are mostly random, implying that the model has captured the serial correlation in the data effectively. The Quantile-Quantile (Q-Q) plot suggests the residuals are approximately normally distributed. And the Ljung-Box plot shows that many of the p-values(especially lag 1) are above the 0.05 significance level, which is a good sign.\n\n\nCode\nplot1&lt;-ggAcf(sage.res, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\nplot2&lt;- ggPacf(sage.res, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot3&lt;-ggAcf(sage.res^2, 40) + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\nplot4&lt;- ggPacf(sage.res^2, 40)+theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA))\n\ngrid.arrange(plot3, plot4,nrow=2)\n\n\n\n\n\n\n\n\n\nAs we can see from the Standardized Residuals plot, and the ACF and PACF plots of the residuals and squared residuals, there’s no need to fit a GARCH model to the residuals. The residuals are approximately normally distributed, and there’s no significant autocorrelation in the residuals, which aligns with the conclusion we got from the ARCH test.\n\n\n\n\n\nARIMA + GARCH\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:10) {\n  for (q in 1:10) {\n  \nmodel[[cc]] &lt;- garch(pfe.res,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\nCode\nprint(\"The best model according to AIC is:\")\n\n\n[1] \"The best model according to AIC is:\"\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = pfe.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n1.501e-05  1.201e-01  8.288e-01  \n\n\nAs we can see from the result, the best model according to AIC is GARCH(1,1). Then let’s do the Model Evaluation for the GARCH(1,1), GARCH(1,2), and GARCH(2,1) models.\n\n\nCode\nsummary(garchFit(~garch(1,1), pfe.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = pfe.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x0000029e0feb8df8&gt;\n [data = pfe.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-1.4402e-05   1.5039e-05   1.2024e-01   8.2856e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     -1.440e-05   4.029e-04   -0.036    0.971    \nomega   1.504e-05   3.694e-06    4.072 4.67e-05 ***\nalpha1  1.202e-01   2.112e-02    5.694 1.24e-08 ***\nbeta1   8.286e-01   2.649e-02   31.280  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 3521.786    normalized:  2.753547 \n\nDescription:\n Thu May  2 15:12:28 2024 by user: china \n\n\nStandardised Residuals Tests:\n                                  Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  605.0248038 0.0000000000\n Shapiro-Wilk Test  R    W        0.9673121 0.0000000000\n Ljung-Box Test     R    Q(10)   12.2673855 0.2675599211\n Ljung-Box Test     R    Q(15)   15.0800261 0.4456682784\n Ljung-Box Test     R    Q(20)   21.8078718 0.3510264124\n Ljung-Box Test     R^2  Q(10)   34.6586049 0.0001427669\n Ljung-Box Test     R^2  Q(15)   41.4411007 0.0002738319\n Ljung-Box Test     R^2  Q(20)   42.1324652 0.0026577114\n LM Arch Test       R    TR^2    33.9608999 0.0006841843\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.500838 -5.484720 -5.500858 -5.494786 \n\n\nCode\nsummary(garchFit(~garch(1,2), pfe.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = pfe.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n&lt;environment: 0x0000029e0fe4dd88&gt;\n [data = pfe.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2  \n-1.4402e-05   1.5062e-05   1.2075e-01   8.2646e-01   1.7009e-03  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     -1.440e-05   4.028e-04   -0.036 0.971480    \nomega   1.506e-05   4.443e-06    3.390 0.000698 ***\nalpha1  1.207e-01   3.061e-02    3.944 8.01e-05 ***\nbeta1   8.265e-01   2.555e-01    3.235 0.001215 ** \nbeta2   1.701e-03   2.235e-01    0.008 0.993929    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 3521.922    normalized:  2.753653 \n\nDescription:\n Thu May  2 15:12:28 2024 by user: china \n\n\nStandardised Residuals Tests:\n                                  Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  602.5774283 0.0000000000\n Shapiro-Wilk Test  R    W        0.9674049 0.0000000000\n Ljung-Box Test     R    Q(10)   12.2137196 0.2710076986\n Ljung-Box Test     R    Q(15)   15.0143514 0.4503839411\n Ljung-Box Test     R    Q(20)   21.7711363 0.3530574489\n Ljung-Box Test     R^2  Q(10)   34.6278430 0.0001444985\n Ljung-Box Test     R^2  Q(15)   41.4076648 0.0002770778\n Ljung-Box Test     R^2  Q(20)   42.0990391 0.0026845467\n LM Arch Test       R    TR^2    33.9279105 0.0006924242\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.499488 -5.479340 -5.499518 -5.491922 \n\n\nCode\nsummary(garchFit(~garch(2,1), pfe.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = pfe.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n&lt;environment: 0x0000029e0f05b0a0&gt;\n [data = pfe.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1       alpha2        beta1  \n-1.4402e-05   1.5043e-05   1.2058e-01   1.0000e-08   8.2840e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     -1.440e-05   4.029e-04   -0.036 0.971481    \nomega   1.504e-05   4.181e-06    3.598 0.000321 ***\nalpha1  1.206e-01   3.494e-02    3.451 0.000559 ***\nalpha2  1.000e-08   4.076e-02    0.000 1.000000    \nbeta1   8.284e-01   3.290e-02   25.179  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 3521.922    normalized:  2.753653 \n\nDescription:\n Thu May  2 15:12:28 2024 by user: china \n\n\nStandardised Residuals Tests:\n                                  Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  602.3448862 0.0000000000\n Shapiro-Wilk Test  R    W        0.9674081 0.0000000000\n Ljung-Box Test     R    Q(10)   12.2130440 0.2710513088\n Ljung-Box Test     R    Q(15)   15.0141292 0.4503999350\n Ljung-Box Test     R    Q(20)   21.7698039 0.3531312405\n Ljung-Box Test     R^2  Q(10)   34.6271822 0.0001445360\n Ljung-Box Test     R^2  Q(15)   41.4085292 0.0002769934\n Ljung-Box Test     R^2  Q(20)   42.0996522 0.0026840521\n LM Arch Test       R    TR^2    33.9289248 0.0006921694\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.499488 -5.479340 -5.499518 -5.491922 \n\n\nAll the coefficients are significant for GARCH(1,1). For model GARCH(1,2), the coefficient for beta2 is not significant, and the coefficient for beta1 is only significant at 0.1 level. For the GARCH(2,1) model, the coefficient for alpha2 is not significant. Then let’s use the Cross-Validation to evaluate the above models.\n\n\nCode\nk &lt;- 700 # minimum data length for fitting a model 3324*.2 (20%)\nn &lt;- length(returns_pfe)\n# n-k \ni=1\nerr1 = c()\nerr2 = c()\nerr3 = c()\n\nrmse1=c()\nrmse2=c()\nrmse3=c()\n\nfor(i in 1:100)\n{\n  xtrain &lt;- returns_pfe[1:(k-1)+i] \n  xtest &lt;- returns_pfe[k+i] \n\n  # ARIMA(1,0,2) + GARCH(1,1)\n  arima.fit&lt;-Arima(xtrain,order=c(1,0,2),include.drift = TRUE)\n  arima.res &lt;- residuals(arima.fit)\n  fit1 &lt;- garchFit(~garch(1,1), arima.res,trace = F)\n  fcast1 &lt;- predict(fit1, n.ahead=1)\n  \n  # ARIMA(1,0,2) + GARCH(1,2)\n  fit2 &lt;- garchFit(~garch(1,2), arima.res,trace = F)\n  fcast2 &lt;- predict(fit2, n.ahead=1)\n  \n  # ARIMA(1,0,2) + GARCH(2,1)\n  fit3 &lt;- garchFit(~garch(2,1), arima.res,trace = F)\n  fcast3 &lt;- predict(fit3, n.ahead=1)\n  \n  err1 = c(err1, (fcast1$meanForecast-xtest)^2)\n  err2 = c(err2, (fcast2$meanForecast-xtest)^2)\n  err3 = c(err3, (fcast3$meanForecast-xtest)^2)\n\n  \n}\n\nRMSE1=sqrt(mean(err1)) \nRMSE2=sqrt(mean(err2))\nRMSE3=sqrt(mean(err3))\n\nRMSE1\n\n\n[1] 0.02327513\n\n\nCode\nRMSE2\n\n\n[1] 0.02327465\n\n\nCode\nRMSE3\n\n\n[1] 0.02327418\n\n\nWe can see that the RMSE for these three models are so similar to each other. Considering that the GARCH(1,1) model has all significant coefficients, we can conclude that the GARCH(1,1) model is the best model. In conclusion, the ARIMA(1,0,2) + GARCH(1,1) model is the best model for the Pfizer stock returns.\n\n\nNo GARCH model needed. Only ARIMA(3,0,3) or ARMA(3,3) is needed.\n\n\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:10) {\n  for (q in 1:10) {\n  \nmodel[[cc]] &lt;- garch(teva.res,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\nCode\nprint(\"The best model according to AIC is:\")\n\n\n[1] \"The best model according to AIC is:\"\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = teva.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n2.716e-05  2.924e-02  9.415e-01  \n\n\nAs we can see from the result, the best model according to AIC is GARCH(1,1). Then let’s do the Model Evaluation for the GARCH(1,1), GARCH(1,2), and GARCH(2,1) models.\n\n\nCode\nsummary(garchFit(~garch(1,1), teva.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = teva.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x0000029e17918eb8&gt;\n [data = teva.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n5.7643e-05  2.7649e-05  2.9444e-02  9.4078e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     5.764e-05   8.133e-04    0.071 0.943499    \nomega  2.765e-05   1.225e-05    2.257 0.024034 *  \nalpha1 2.944e-02   8.232e-03    3.577 0.000348 ***\nbeta1  9.408e-01   1.961e-02   47.983  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2676.301    normalized:  2.092495 \n\nDescription:\n Thu May  2 15:12:53 2024 by user: china \n\n\nStandardised Residuals Tests:\n                                   Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2348.2275307 0.0000000\n Shapiro-Wilk Test  R    W         0.9425608 0.0000000\n Ljung-Box Test     R    Q(10)     4.7898578 0.9047663\n Ljung-Box Test     R    Q(15)     9.4218297 0.8544493\n Ljung-Box Test     R    Q(20)    12.3769727 0.9024901\n Ljung-Box Test     R^2  Q(10)     2.6182459 0.9890383\n Ljung-Box Test     R^2  Q(15)     8.5082445 0.9017962\n Ljung-Box Test     R^2  Q(20)    10.1622170 0.9651342\n LM Arch Test       R    TR^2      4.4670991 0.9734595\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-4.178735 -4.162617 -4.178754 -4.172682 \n\n\nCode\nsummary(garchFit(~garch(1,2), teva.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = teva.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n&lt;environment: 0x0000029e19b985a8&gt;\n [data = teva.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n5.7643e-05  3.4129e-05  3.6740e-02  6.7730e-01  2.4924e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)  \nmu     5.764e-05   8.132e-04    0.071   0.9435  \nomega  3.413e-05   1.802e-05    1.894   0.0582 .\nalpha1 3.674e-02   1.522e-02    2.414   0.0158 *\nbeta1  6.773e-01   4.433e-01    1.528   0.1265  \nbeta2  2.492e-01   4.216e-01    0.591   0.5544  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2676.496    normalized:  2.092647 \n\nDescription:\n Thu May  2 15:12:53 2024 by user: china \n\n\nStandardised Residuals Tests:\n                                   Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2389.0642829 0.0000000\n Shapiro-Wilk Test  R    W         0.9422025 0.0000000\n Ljung-Box Test     R    Q(10)     4.8441936 0.9013399\n Ljung-Box Test     R    Q(15)     9.4864698 0.8507404\n Ljung-Box Test     R    Q(20)    12.4370273 0.9002132\n Ljung-Box Test     R^2  Q(10)     2.6875174 0.9878524\n Ljung-Box Test     R^2  Q(15)     8.6937557 0.8929779\n Ljung-Box Test     R^2  Q(20)    10.3679925 0.9609979\n LM Arch Test       R    TR^2      4.6000187 0.9700238\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-4.177476 -4.157328 -4.177506 -4.169910 \n\n\nCode\nsummary(garchFit(~garch(2,1), teva.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = teva.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n&lt;environment: 0x0000029e0d4686b8&gt;\n [data = teva.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n5.7643e-05  2.7675e-05  2.9532e-02  1.0000e-08  9.4070e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     5.764e-05   8.133e-04    0.071   0.9435    \nomega  2.767e-05   1.300e-05    2.129   0.0333 *  \nalpha1 2.953e-02   2.433e-02    1.214   0.2248    \nalpha2 1.000e-08   2.648e-02    0.000   1.0000    \nbeta1  9.407e-01   2.123e-02   44.311   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 2676.369    normalized:  2.092548 \n\nDescription:\n Thu May  2 15:12:54 2024 by user: china \n\n\nStandardised Residuals Tests:\n                                   Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2347.6689051 0.0000000\n Shapiro-Wilk Test  R    W         0.9425678 0.0000000\n Ljung-Box Test     R    Q(10)     4.7898454 0.9047671\n Ljung-Box Test     R    Q(15)     9.4216398 0.8544602\n Ljung-Box Test     R    Q(20)    12.3724205 0.9026614\n Ljung-Box Test     R^2  Q(10)     2.6203835 0.9890029\n Ljung-Box Test     R^2  Q(15)     8.4983170 0.9022563\n Ljung-Box Test     R^2  Q(20)    10.1498025 0.9653735\n LM Arch Test       R    TR^2      4.4654378 0.9735007\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-4.177277 -4.157129 -4.177307 -4.169711 \n\n\nAll the coefficients are significant for GARCH(1,1). For model GARCH(1,2), the coefficients for beta1, beta2 are not significant. For the GARCH(2,1) model, the coefficients for alpha1, alpha2 are not significant. Then let’s use the Cross-Validation to evaluate the above models.\n\n\nCode\nk &lt;- 700 # minimum data length for fitting a model 3324*.2 (20%)\nn &lt;- length(returns_teva)\n# n-k \ni=1\nerr1 = c()\nerr2 = c()\nerr3 = c()\n\nrmse1=c()\nrmse2=c()\nrmse3=c()\n\nfor(i in 1:100)\n{\n  xtrain &lt;- returns_teva[1:(k-1)+i] \n  xtest &lt;- returns_teva[k+i] \n\n  # ARIMA(1,0,1) + GARCH(1,1)\n  arima.fit&lt;-Arima(xtrain,order=c(1,0,1),include.drift = TRUE)\n  arima.res &lt;- residuals(arima.fit)\n  fit1 &lt;- garchFit(~garch(1,1), arima.res,trace = F)\n  fcast1 &lt;- predict(fit1, n.ahead=1)\n  \n  # ARIMA(1,0,1) + GARCH(1,2)\n  fit2 &lt;- garchFit(~garch(1,2), arima.res,trace = F)\n  fcast2 &lt;- predict(fit2, n.ahead=1)\n  \n  # ARIMA(1,0,1) + GARCH(2,1)\n  fit3 &lt;- garchFit(~garch(2,1), arima.res,trace = F)\n  fcast3 &lt;- predict(fit3, n.ahead=1)\n  \n  err1 = c(err1, (fcast1$meanForecast-xtest)^2)\n  err2 = c(err2, (fcast2$meanForecast-xtest)^2)\n  err3 = c(err3, (fcast3$meanForecast-xtest)^2)\n\n  \n}\n\nRMSE1=sqrt(mean(err1)) \nRMSE2=sqrt(mean(err2))\nRMSE3=sqrt(mean(err3))\n\nRMSE1\n\n\n[1] 0.02604004\n\n\nCode\nRMSE2\n\n\n[1] 0.02603885\n\n\nCode\nRMSE3\n\n\n[1] 0.02603878\n\n\nThe RMSE for these three models are so similar to each other, with only 0.000001 difference. Considering that the GARCH(1,1) model has all significant coefficients, we can conclude that the GARCH(1,1) model is the best model. In conclusion, the ARIMA(1,0,1) + GARCH(1,1) model is the best model for the Teva stock returns.\n\n\nNo GARCH model needed. Only ARIMA(1,0,1) or ARMA(1,1) is needed.\n\n\n\n\n\nFinal Model Fitting\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nsummary(arima.fit_pfe &lt;- Arima(returns_pfe,order=c(1,0,2),include.drift = TRUE))\n\n\nSeries: returns_pfe \nARIMA(1,0,2) with drift \n\nCoefficients:\n          ar1     ma1      ma2  intercept  drift\n      -0.8627  0.8207  -0.0856      6e-04      0\ns.e.   0.0494  0.0539   0.0272      9e-04      0\n\nsigma^2 = 0.000279:  log likelihood = 3421.53\nAIC=-6831.06   AICc=-6830.99   BIC=-6800.14\n\nTraining set error measures:\n                        ME       RMSE        MAE MPE MAPE      MASE\nTraining set -3.937903e-06 0.01667036 0.01184094 NaN  Inf 0.6639357\n                     ACF1\nTraining set -0.005191263\n\n\nCode\nsummary(final.fit_pfe &lt;- garch(order=c(1,1), pfe.res,trace = F))\n\n\n\nCall:\ngarch(x = pfe.res, order = c(1, 1), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-5.627016 -0.595526 -0.006863  0.556180  5.949619 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 1.500e-05   2.377e-06    6.314 2.72e-10 ***\na1 1.201e-01   1.255e-02    9.569  &lt; 2e-16 ***\nb1 8.288e-01   1.610e-02   51.477  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 611.41, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.001706, df = 1, p-value = 0.9671\n\n\nAs we can see from the results, for the ARIMA(1,0,2) model, most of the parameters are significant. Also, we can see that the error measures are quite low, which indicates that the model fits the data well. For the GARCH(1,1) model we used to account for volatility clustering, all the coefficients are significant, and the Jarque-Bera test on the residuals shows that the residuals are not normally distributed, which is expected for financial data. The Box-Ljung test shows that there is no autocorrelation left. This indicates that the GARCH model does a good job of capturing the autocorrelations in the volatility of the returns.\n\n\n\n\nCode\nsummary(fit_vertex)\n\n\nSeries: returns_vertex \nARIMA(3,0,3) with non-zero mean \n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3   mean\n      -0.8653  0.5634  0.8015  0.7552  -0.6125  -0.7704  7e-04\ns.e.   0.5090  0.8625  0.4467  0.4731   0.7374   0.3561  4e-04\n\nsigma^2 = 0.0003924:  log likelihood = 3204.23\nAIC=-6392.45   AICc=-6392.34   BIC=-6351.22\n\nTraining set error measures:\n                       ME       RMSE        MAE      MPE    MAPE      MASE\nTraining set 1.732898e-05 0.01975556 0.01366113 109.6771 183.909 0.6694657\n                    ACF1\nTraining set -0.01221804\n\n\nCode\n# Plot the Box Ljung test\nBox.test(residuals(fit_vertex), lag = 20, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  residuals(fit_vertex)\nX-squared = 18.322, df = 20, p-value = 0.5662\n\n\nAs we can see from the results, for the ARIMA(1,0,1) model, the error measures are quite low(eg. ME, RMSE), which indicates that the model fits the data well. The Box-Ljung test shows that there is no autocorrelation left in the residuals. In other words, our model appears to be adequately capturing the autocorrelation, and there is no significant evidence of leftover patterns in the residuals that the model has not accounted for.\n\n\n\n\nCode\nsummary(arima.fit_teva &lt;- Arima(returns_teva,order=c(1,0,1),include.drift = TRUE))\n\n\nSeries: returns_teva \nARIMA(1,0,1) with drift \n\nCoefficients:\n         ar1      ma1  intercept  drift\n      0.8127  -0.7847    -0.0019      0\ns.e.  0.1015   0.1070     0.0020      0\n\nsigma^2 = 0.0009509:  log likelihood = 2636.86\nAIC=-5263.72   AICc=-5263.68   BIC=-5237.96\n\nTraining set error measures:\n                        ME       RMSE        MAE MPE MAPE      MASE        ACF1\nTraining set -1.249074e-05 0.03078892 0.02157769 NaN  Inf 0.6874772 -0.03119052\n\n\nCode\nsummary(final.fit_teva &lt;- garch(order=c(1,1), teva.res,trace = F))\n\n\n\nCall:\ngarch(x = teva.res, order = c(1, 1), trace = F)\n\nModel:\nGARCH(1,1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-6.41403 -0.51151 -0.02348  0.50800  8.33417 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(&gt;|t|)    \na0 2.716e-05   7.954e-06    3.415 0.000638 ***\na1 2.924e-02   5.844e-03    5.003 5.66e-07 ***\nb1 9.415e-01   1.341e-02   70.196  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 2358.8, df = 2, p-value &lt; 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.0043149, df = 1, p-value = 0.9476\n\n\nAs we can see from the results, for the ARIMA(1,0,1) model, the error measures are quite low(eg. ME, RMSE), which indicates that the model fits the data well. For the GARCH(1,1) model we used to account for volatility clustering, all the coefficients are significant, and the Jarque-Bera test on the residuals shows that the residuals are not normally distributed, which is expected for financial data. The Box-Ljung test shows that there is no autocorrelation left. This indicates that the GARCH model does a good job of capturing the autocorrelations in the volatility of the returns.\n\n\n\n\nCode\nsummary(fit_sage)\n\n\nSeries: returns_sage \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n          ar1     ma1     mean\n      -0.1448  0.1064  -0.0010\ns.e.   0.4693  0.4711   0.0013\n\nsigma^2 = 0.002415:  log likelihood = 2040.29\nAIC=-4072.59   AICc=-4072.56   BIC=-4051.97\n\nTraining set error measures:\n                        ME       RMSE        MAE MPE MAPE      MASE\nTraining set -4.692071e-06 0.04908677 0.02653977 NaN  Inf 0.6817242\n                     ACF1\nTraining set 0.0002227219\n\n\nCode\n# Plot the Box Ljung test\nBox.test(residuals(fit_sage), lag = 20, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  residuals(fit_sage)\nX-squared = 22.323, df = 20, p-value = 0.3232\n\n\nAs we can see from the results, for the ARIMA(1,0,1) model, the error measures are quite low(eg. ME, RMSE), which indicates that the model fits the data well. The Box-Ljung test shows that there is no autocorrelation left in the residuals. In other words, our model appears to be adequately capturing the autocorrelation, and there is no significant evidence of leftover patterns in the residuals that the model has not accounted for.\n\n\n\n\n\nModel Equations\n\nPfizerVertexTevaSage\n\n\nThe best model for the Pfizer stock returns is ARIMA(1,0,2) + GARCH(1,1). The model equations are as follows:\n\nARIMA(1,0,2)\n\n\\[\n(1 - \\phi_1 B)Y_t = (1 + \\theta_1 B + \\theta_2 B^2)\\varepsilon_t\n\\]\n\nGARCH(1,1)\n\n\\[\n\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\n\\]\n\n\nThe best model for the Vertex stock returns is ARIMA(3,0,3). The model equations are as follows:\n\nARIMA(3,0,3)\n\n\\[\n(1 - \\phi_1 B - \\phi_2 B^2 - \\phi_3 B^3)Y_t = (1 + \\theta_1 B + \\theta_2 B^2 + \\theta_3 B^3)\\varepsilon_t\n\\]\n\n\nThe best model for the Teva stock returns is ARIMA(1,0,1) + GARCH(1,1). The model equations are as follows:\n\nARIMA(1,0,1)\n\n\\[\n(1 - \\phi_1 B)Y_t = (1 + \\theta_1 B)\\varepsilon_t\n\\]\n\nGARCH(1,1)\n\n\\[\n\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\n\\]\n\n\nThe best model for the Sage stock returns is ARIMA(1,0,1). The model equations are as follows:\n\nARIMA(1,0,1)\n\n\\[\n(1 - \\phi_1 B)Y_t = (1 + \\theta_1 B)\\varepsilon_t\n\\]\n\n\n\n\n\nVolatility Plot\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nfinal.fit_pfe &lt;- garchFit(~garch(1,1), pfe.res,trace = F)\nht &lt;- final.fit_pfe@h.t \npfe_all=data.frame(pfizer)\npfe_all &lt;- data.frame(pfizer,rownames(pfe_all))\ncolnames(pfe_all)[7] = \"date\"\npfe_all$date&lt;-as.Date(pfe_all$date,\"%Y-%m-%d\")\ndata_pfe = data.frame(ht,pfe_all$date[-1])\n\nggplot(data_pfe, aes(y = ht, x = pfe_all.date..1.)) + \n  geom_line(col = \"#27aeef\") + \n  ylab('Conditional Variance') + \n  xlab('Date')+\n  ggtitle(\"Volatality plot for Pfizer\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresiduals_vertex &lt;- residuals(fit_vertex)\nsquared_residuals &lt;- residuals_vertex^2\nvertex_all &lt;- data.frame(vertex)\nvertex_all &lt;- data.frame(vertex_all, rownames(vertex_all))\ncolnames(vertex_all)[7] &lt;- \"date\"\nvertex_all$date &lt;- as.Date(vertex_all$date, \"%Y-%m-%d\")\ndata_vertex &lt;- data.frame(ht = squared_residuals, date = vertex_all$date[-1])\n\nggplot(data_vertex, aes(x = date, y = ht)) +\n  geom_line(col = \"#27aeef\") +\n  ylab('Conditional Variance') +\n  xlab('Date') +\n  ggtitle(\"Volatility Plot for Vertex\")+ theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfinal.fit_teva &lt;- garchFit(~garch(1,1), teva.res,trace = F)\nht &lt;- final.fit_teva@h.t\nteva_all=data.frame(teva)\nteva_all &lt;- data.frame(teva,rownames(teva_all))\ncolnames(teva_all)[7] = \"date\"\nteva_all$date&lt;-as.Date(teva_all$date,\"%Y-%m-%d\")\ndata_teva = data.frame(ht,teva_all$date[-1])\n\nggplot(data_teva, aes(y = ht, x = teva_all.date..1.)) + \n  geom_line(col = \"#27aeef\") + \n  ylab('Conditional Variance') + \n  xlab('Date')+\n  ggtitle(\"Volatality plot for Teva\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresiduals_sage &lt;- residuals(fit_sage)\nsquared_residuals &lt;- residuals_sage^2\nsage_all &lt;- data.frame(sage)\nsage_all &lt;- data.frame(sage_all, rownames(sage_all))\ncolnames(sage_all)[7] &lt;- \"date\"\nsage_all$date &lt;- as.Date(sage_all$date, \"%Y-%m-%d\")\ndata_sage &lt;- data.frame(ht = squared_residuals, date = sage_all$date[-1])\n\nggplot(data_sage, aes(x = date, y = ht)) +\n  geom_line(col = \"#27aeef\") +\n  ylab('Conditional Variance') +\n  xlab('Date') +\n  ggtitle(\"Volatility Plot for SAGE\")+ theme_bw()"
  },
  {
    "objectID": "financial_time_series.html#acfpacf-plots",
    "href": "financial_time_series.html#acfpacf-plots",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF/PACF Plots",
    "text": "ACF/PACF Plots\n\nPfizerVertexTevaSage\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_pfe, 40)+ggtitle(\"Pfizer Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_pfe, 40)+theme_bw()+ggtitle(\"Pfizer Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_pfe), 40)+ggtitle(\"Pfizer Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_pfe), 40)+theme_bw()+ggtitle(\"Pfizer Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_pfe^2, 40)+ggtitle(\"Pfizer Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_pfe^2, 40)+theme_bw()+ggtitle(\"Pfizer Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_vertex, 40)+ggtitle(\"Vertex Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_vertex, 40)+theme_bw()+ggtitle(\"Vertex Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_vertex), 40)+ggtitle(\"Vertex Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_vertex), 40)+theme_bw()+ggtitle(\"Vertex Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_vertex^2, 40)+ggtitle(\"Vertex Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_vertex^2, 40)+theme_bw()+ggtitle(\"Vertex Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_teva, 40)+ggtitle(\"TEVA Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_teva, 40)+theme_bw()+ggtitle(\"TEVA Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_teva), 40)+ggtitle(\"TEVA Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_teva), 40)+theme_bw()+ggtitle(\"TEVA Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_teva^2, 40)+ggtitle(\"TEVA Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_teva^2, 40)+theme_bw()+ggtitle(\"TEVA Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_sage, 40)+ggtitle(\"SAGE Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_sage, 40)+theme_bw()+ggtitle(\"SAGE Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(abs(returns_sage), 40)+ggtitle(\"SAGE Absolute Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(abs(returns_sage), 40)+theme_bw()+ggtitle(\"SAGE Absolute Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nCode\nplot1&lt;-ggAcf(returns_sage^2, 40)+ggtitle(\"SAGE Squared Returns ACF\") + theme_bw()+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \nplot2&lt;- ggPacf(returns_sage^2, 40)+theme_bw()+ggtitle(\"SAGE Squared Returns PACF\")+\n      theme(plot.background = element_rect(color = NA),\n            panel.background = element_rect(color = NA)) \n\ngrid.arrange(plot1, plot2,nrow=2)"
  },
  {
    "objectID": "dl_time_series.html",
    "href": "dl_time_series.html",
    "title": "DL for Time Series",
    "section": "",
    "text": "In this comprehensive analysis, I aim to navigate the intricate realm of forecasting time series data using cutting-edge deep learning techniques. My focus will be on delving into the functionalities and applications of Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), and Long Short-Term Memory networks (LSTMs). These sophisticated models will undergo rigorous scrutiny, comparing their predictive capabilities and practicality against established statistical methodologies like ARMA, ARIMA, and SARIMA models. Through evaluation, I seek to shed light on the inherent strengths and potential trade-offs between the innovative deep learning approaches and traditional statistical methods in the realm of time series analysis.\nTo augment my analysis, I’ll integrate pertinent real-world data. Firstly, tracking the Pfizer and TEVA stock prices will provide invaluable insights into the pharmaceutical industry’s performance. Additionally, considering average life expectancy trends can enrich our understanding of broader demographic shifts and potential economic impacts. Furthermore, observing smoking prevalence rates can serve as a proxy for public health dynamics, potentially influencing consumer behavior and market trends. By intertwining these diverse datasets, I aim to craft a holistic analysis that not only elucidates the efficacy of our forecasting methodologies but also captures the intricate interplay between economic, demographic, and health factors in shaping healthcare industry.\nCode\nlibrary(reticulate)\nlibrary(readxl)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\nCode\n# !pip3 install scikit-learn\n# !pip3 install tensorflow \n# !pip3 install yfinance\n# !pip3 install plotly\n# !pip3 install statsmodels\n# !pip3 install IPython\n# !pip3 install matplotlib\n# !pip3 install seaborn\n# !pip3 install jupyter\n# !pip3 install keras\nCode\nimport tensorflow\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport yfinance as yf\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom IPython.display import IFrame\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom tensorflow import keras\nfrom keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nCode\n# Pfizer\npfe = yf.download(\"PFE\", '2019-01-01','2024-04-01')\n\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nCode\n# Teva\nteva = yf.download(\"TEVA\", '2019-01-01','2024-04-01')\n\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nCode\n\n# Clean PFE\npfe = pfe.reset_index()\npfe = pfe.rename(columns={'Date':'t', 'Adj Close':'y'})\npfe = pfe[['t', 'y']]\nt_pfe = np.array([*range(0,pfe.shape[0])])\nx_pfe = np.array(pfe['y']).reshape(t_pfe.shape[0],1)\nfeature_columns_pfe = [0]\ntarget_columns_pfe = [0]\n\n# Clean TEVA\nteva = teva.reset_index()\nteva = teva.rename(columns={'Date':'t', 'Adj Close':'y'})\nteva = teva[['t', 'y']]\nt_teva = np.array([*range(0,teva.shape[0])])\nx_teva = np.array(teva['y']).reshape(t_teva.shape[0],1)\nfeature_columns_teva = [0]\ntarget_columns_teva = [0]\n\n# Average Life Expectancy\n\nale = pd.read_excel('data/life_expectancy_7countries_2000_2022.xlsx')\nale = ale[['Year', 'United States']]\nale = ale.rename(columns={'Year':'t', 'United States':'y'})\nale = ale[['t', 'y']]\nt_ale = np.array([*range(0,ale.shape[0])])\nx_ale = np.array(ale['y']).reshape(t_ale.shape[0],1)\nfeature_columns_ale = [0]\ntarget_columns_ale = [0]\n\n# Number of Smokers\n\nsmoke = pd.read_excel('data/number_of_adult_smokers_us_1965_2021.xlsx')\nsmoke = smoke.rename(columns={'Year':'t', 'Number':'y'})\nsmoke = smoke[['t', 'y']]\nt_smoke = np.array([*range(0,smoke.shape[0])])\nx_smoke = np.array(smoke['y']).reshape(t_smoke.shape[0],1)\nfeature_columns_smoke = [0]\ntarget_columns_smoke = [0]"
  },
  {
    "objectID": "dl_time_series.html#data-normalization",
    "href": "dl_time_series.html#data-normalization",
    "title": "Deep Learning for TS",
    "section": "Data Normalization",
    "text": "Data Normalization\n\nLarge-Scale PharmaceuticalsGeneric and Specialty Pharmaceuticals\n\n\n\n\nCode\n#|code-fold: true\n\n# Normalize the Pfizer data\nprint(np.mean(x_pfe,axis=0).shape,np.std(x_pfe,axis=0).shape)\n\n\n(1,) (1,)\n\n\nCode\nx_pfe=(x_pfe-np.mean(x_pfe,axis=0))/np.std(x_pfe,axis=0)\nprint(x_pfe.shape)\n\n\n(1323, 1)\n\n\nCode\n# Plot the normalized Pfizer data\nfig, ax = plt.subplots()\nfor i in range(0,x_pfe.shape[1]):\n    ax.plot(t_pfe, x_pfe[:,i],'o')\n    ax.plot(t_pfe, x_pfe[:,i],\"-\")\nax.plot(t_pfe, 0*x_pfe[:,0],\"-\") \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#|code-fold: true\n\n# Normalize the Teva data\nprint(np.mean(x_teva,axis=0).shape,np.std(x_teva,axis=0).shape)\n\n\n(1,) (1,)\n\n\nCode\nx_teva=(x_teva-np.mean(x_teva,axis=0))/np.std(x_teva,axis=0)\n\n# Plot the normalized Teva data\nfig, ax = plt.subplots()\nfor i in range(0,x_teva.shape[1]):\n    ax.plot(t_teva, x_teva[:,i],'o')\n    ax.plot(t_teva, x_teva[:,i],\"-\")\nax.plot(t_teva, 0*x_teva[:,0],\"-\")\nplt.show()"
  },
  {
    "objectID": "dl_time_series.html#data-splitting",
    "href": "dl_time_series.html#data-splitting",
    "title": "Deep Learning for TS",
    "section": "Data Splitting",
    "text": "Data Splitting\n\nLarge-Scale PharmaceuticalsGeneric and Specialty Pharmaceuticals\n\n\n\n\nCode\n#|code-fold: true\n\n# Split the Pfizer data\nsplit_fr = 0.8\ncut=int(split_fr*x_pfe.shape[0])\ntt_pfe=t_pfe[0:cut]; xt_pfe=x_pfe[0:cut]\ntv_pfe=t_pfe[cut:]; xv_pfe=x_pfe[cut:]\n\n# Plot the split Pfizer data\nfig, ax = plt.subplots()\nfor i in range(0,x_pfe.shape[1]):\n    ax.plot(tt_pfe, xt_pfe[:,i],'ro',alpha=0.25)\n    ax.plot(tt_pfe, xt_pfe[:,i],\"g-\")\nfor i in range(0,x_pfe.shape[1]):\n    ax.plot(tv_pfe, xv_pfe[:,i],'bo',alpha=0.25)\n    ax.plot(tv_pfe, xv_pfe[:,i],\"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#|code-fold: true\n\n# Split the Teva data\nsplit_fr = 0.8\ncut=int(split_fr*x_teva.shape[0])\ntt_teva=t_teva[0:cut]; xt_teva=x_teva[0:cut]\ntv_teva=t_teva[cut:]; xv_teva=x_teva[cut:]\n\n# Plot the split Teva data\nfig, ax = plt.subplots()\nfor i in range(0,x_teva.shape[1]):\n    ax.plot(tt_teva, xt_teva[:,i],'ro',alpha=0.25)\n    ax.plot(tt_teva, xt_teva[:,i],\"g-\")\nfor i in range(0,x_teva.shape[1]):\n    ax.plot(tv_teva, xv_teva[:,i],'bo',alpha=0.25)\n    ax.plot(tv_teva, xv_teva[:,i],\"g-\")\nplt.show()"
  },
  {
    "objectID": "dl_time_series.html#mini-batching",
    "href": "dl_time_series.html#mini-batching",
    "title": "Deep Learning for TS",
    "section": "Mini-Batching",
    "text": "Mini-Batching"
  },
  {
    "objectID": "dl_time_series.html#data-preprossing",
    "href": "dl_time_series.html#data-preprossing",
    "title": "DL for Time Series",
    "section": "2.1 Data Preprossing",
    "text": "2.1 Data Preprossing\nIn this following section, given the diverse scales of variables within the original data, I’ll first apply normalization to standardize the regression values, aiming to optimize model performance by ensuring data uniformity, thus boosting the accuracy and efficacy of predictive models. Then, I’ll separate the datasets into training and testing subset. In order to further bolster the training process, lastly I’ll integrate mini-batching, a strategy involving more frequent gradient updates within each epoch.\n\nData Normalization & Splitting\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\n\n\nCode\n# Pfizer\nprint(np.mean(x_pfe,axis=0).shape,np.std(x_pfe,axis=0).shape)\n\n\n(1,) (1,)\n\n\nCode\nx_pfe=(x_pfe-np.mean(x_pfe,axis=0))/np.std(x_pfe,axis=0)\nprint(x_pfe.shape)\n\n\n(1319, 1)\n\n\nCode\nfig, ax = plt.subplots()\nfor i in range(0,x_pfe.shape[1]):\n    ax.plot(t_pfe, x_pfe[:,i],'o')\n    ax.plot(t_pfe, x_pfe[:,i],\"-\")\nax.plot(t_pfe, 0*x_pfe[:,0],\"-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# TEVA\nprint(np.mean(x_teva,axis=0).shape,np.std(x_teva,axis=0).shape)\n\n\n(1,) (1,)\n\n\nCode\nx_teva=(x_teva-np.mean(x_teva,axis=0))/np.std(x_teva,axis=0)\nprint(x_teva.shape)\n\n\n(1319, 1)\n\n\nCode\nfig, ax = plt.subplots()\nfor i in range(0,x_teva.shape[1]):\n    ax.plot(t_teva, x_teva[:,i],'o')\n    ax.plot(t_teva, x_teva[:,i],\"-\")\nax.plot(t_teva, 0*x_teva[:,0],\"-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(np.mean(x_ale,axis=0).shape,np.std(x_ale,axis=0).shape)\n\n\n(1,) (1,)\n\n\nCode\nx_ale=(x_ale-np.mean(x_ale,axis=0))/np.std(x_ale,axis=0)\nprint(x_ale.shape)\n\n\n(23, 1)\n\n\nCode\nfig, ax = plt.subplots()\nfor i in range(0,x_ale.shape[1]):\n    ax.plot(t_ale, x_ale[:,i],'o')\n    ax.plot(t_ale, x_ale[:,i],\"-\")\nax.plot(t_ale, 0*x_ale[:,0],\"-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(np.mean(x_smoke,axis=0).shape,np.std(x_smoke,axis=0).shape)\n\n\n(1,) (1,)\n\n\nCode\nx_smoke=(x_smoke-np.mean(x_smoke,axis=0))/np.std(x_smoke,axis=0)\nprint(x_smoke.shape)\n\n\n(31, 1)\n\n\nCode\nfig, ax = plt.subplots()\nfor i in range(0,x_smoke.shape[1]):\n    ax.plot(t_smoke, x_smoke[:,i],'o')\n    ax.plot(t_smoke, x_smoke[:,i],\"-\")\nax.plot(t_smoke, 0*x_smoke[:,0],\"-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Split\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\n\n\nCode\n## PFE\nsplit_fr = 0.8\ncut=int(split_fr*x_pfe.shape[0])\ntt_pfe=t_pfe[0:cut]; xt_pfe=x_pfe[0:cut]\ntv_pfe=t_pfe[cut:]; xv_pfe=x_pfe[cut:]\n\nfig, ax = plt.subplots()\nfor i in range(0,x_pfe.shape[1]):\n    ax.plot(tt_pfe, xt_pfe[:,i],'ro',alpha=0.25)\n    ax.plot(tt_pfe, xt_pfe[:,i],\"g-\")\nfor i in range(0,x_pfe.shape[1]):\n    ax.plot(tv_pfe, xv_pfe[:,i],'bo',alpha=0.25)\n    ax.plot(tv_pfe, xv_pfe[:,i],\"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## TEVA\nsplit_fr = 0.8\ncut=int(split_fr*x_teva.shape[0])\ntt_teva=t_teva[0:cut]; xt_teva=x_teva[0:cut]\ntv_teva=t_teva[cut:]; xv_teva=x_teva[cut:]\n\nfig, ax = plt.subplots()\nfor i in range(0,x_teva.shape[1]):\n    ax.plot(tt_teva, xt_teva[:,i],'ro',alpha=0.25)\n    ax.plot(tt_teva, xt_teva[:,i],\"g-\")\nfor i in range(0,x_teva.shape[1]):\n    ax.plot(tv_teva, xv_teva[:,i],'bo',alpha=0.25)\n    ax.plot(tv_teva, xv_teva[:,i],\"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Average Life Expectancy\nsplit_fr = 0.8\ncut=int(split_fr*x_ale.shape[0])\ntt_ale=t_ale[0:cut]; xt_ale=x_ale[0:cut]\ntv_ale=t_ale[cut:]; xv_ale=x_ale[cut:]\n\nfig, ax = plt.subplots()\nfor i in range(0,x_ale.shape[1]):\n    ax.plot(tt_ale, xt_ale[:,i],'ro',alpha=0.25)\n    ax.plot(tt_ale, xt_ale[:,i],\"g-\")\nfor i in range(0,x_ale.shape[1]):\n    ax.plot(tv_ale, xv_ale[:,i],'bo',alpha=0.25)\n    ax.plot(tv_ale, xv_ale[:,i],\"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Smoking Prevalence\nsplit_fr = 0.8\ncut=int(split_fr*x_smoke.shape[0])\ntt_smoke=t_smoke[0:cut]; xt_smoke=x_smoke[0:cut]\ntv_smoke=t_smoke[cut:]; xv_smoke=x_smoke[cut:]\n\nfig, ax = plt.subplots()\nfor i in range(0,x_smoke.shape[1]):\n    ax.plot(tt_smoke, xt_smoke[:,i],'ro',alpha=0.25)\n    ax.plot(tt_smoke, xt_smoke[:,i],\"g-\")\nfor i in range(0,x_smoke.shape[1]):\n    ax.plot(tv_smoke, xv_smoke[:,i],'bo',alpha=0.25)\n    ax.plot(tv_smoke, xv_smoke[:,i],\"g-\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Batching\n\n\nCode\n\ndef from_arrays(x, lookback=3, delay=1, step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n  # initialize \n  i_start = 0\n  count = 0\n  x_out = []\n  y_out = []\n\n  # sequentially build mini-batch samples\n  while i_start + lookback + delay &lt; x.shape[0]:\n    i_stop = i_start + lookback\n    i_pred = i_stop + delay\n\n    # report if desired \n    if verbose and count &lt; 2: \n      print(\"indice range:\", i_start, i_stop, \"--&gt;\", i_pred)\n\n    # define arrays: \n    indices_to_keep = []\n    j = i_stop\n    while j &gt;= i_start:\n      indices_to_keep.append(j)\n      j -= step\n\n    # create mini-batch sample\n    xtmp = x[indices_to_keep, :]    # isolate relevant indices\n    xtmp = xtmp[:, feature_columns] # isolate desired features\n    ytmp = x[i_pred, target_columns]\n    x_out.append(xtmp)\n    y_out.append(ytmp)\n\n    # report if desired \n    #if verbose and count &lt; 2: \n      #print(xtmp, \"--&gt;\", ytmp)\n      #print(\"shape:\", xtmp.shape, \"--&gt;\", ytmp.shape)\n\n    # Plot for debugging    \n    if verbose and count &lt; 2:\n      fig, ax = plt.subplots()\n      ax.plot(x, 'b-')\n      ax.plot(x, 'bx')\n      ax.plot(indices_to_keep, xtmp, 'go')\n      ax.plot(i_pred * np.ones(len(target_columns)), ytmp, 'ro')\n      plt.show()\n\n     # Update start point \n    if unique:\n      i_start += lookback\n    else:\n      i_start += 1\n    count += 1\n        \n  return np.array(x_out), np.array(y_out)\n\n\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\n\n\nCode\n## PFE\nL=25; S=1; D=1\n\nXt_pfe, Yt_pfe = from_arrays(xt_pfe, lookback=L, delay=D, step=S, feature_columns=feature_columns_pfe, target_columns=target_columns_pfe, unique=False,verbose=True)\n\n\nindice range: 0 25 --&gt; 26\nindice range: 1 26 --&gt; 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nXv_pfe, Yv_pfe = from_arrays(xv_pfe, lookback=L, delay=D, step=S, feature_columns=feature_columns_pfe, target_columns=target_columns_pfe, unique=False,verbose=True)\n\n\nindice range: 0 25 --&gt; 26\nindice range: 1 26 --&gt; 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Training:', Xt_pfe.shape, Yt_pfe.shape)\n\n\nTraining: (1029, 26, 1) (1029, 1)\n\n\nCode\nprint('Validation:', Xv_pfe.shape, Yv_pfe.shape)\n\n\nValidation: (238, 26, 1) (238, 1)\n\n\n\n\n\n\nCode\n## TEVA\nL=25; S=1; D=1\n\nXt_teva, Yt_teva = from_arrays(xt_teva, lookback=L, delay=D, step=S, feature_columns=feature_columns_teva, target_columns=target_columns_teva, unique=False,verbose=True)\n\n\nindice range: 0 25 --&gt; 26\nindice range: 1 26 --&gt; 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nXv_teva, Yv_teva = from_arrays(xv_teva, lookback=L, delay=D, step=S, feature_columns=feature_columns_teva, target_columns=target_columns_teva, unique=False,verbose=True)\n\n\nindice range: 0 25 --&gt; 26\nindice range: 1 26 --&gt; 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Training:', Xt_teva.shape, Yt_teva.shape)\n\n\nTraining: (1029, 26, 1) (1029, 1)\n\n\nCode\nprint('Validation:', Xv_teva.shape, Yv_teva.shape)\n\n\nValidation: (238, 26, 1) (238, 1)\n\n\n\n\n\n\nCode\n## ALE\nL=2; S=1; D=1\n\nXt_ale, Yt_ale = from_arrays(xt_ale, lookback=L, delay=D, step=S, feature_columns=feature_columns_ale, target_columns=target_columns_ale, unique=False,verbose=True)\n\n\nindice range: 0 2 --&gt; 3\nindice range: 1 3 --&gt; 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nXv_ale, Yv_ale = from_arrays(xv_ale, lookback=L, delay=D, step=S, feature_columns=feature_columns_ale, target_columns=target_columns_ale, unique=False,verbose=True)\n\n\nindice range: 0 2 --&gt; 3\nindice range: 1 3 --&gt; 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Training:', Xt_ale.shape, Yt_ale.shape)\n\n\nTraining: (15, 3, 1) (15, 1)\n\n\nCode\nprint('Validation:', Xv_ale.shape, Yv_ale.shape)\n\n\nValidation: (2, 3, 1) (2, 1)\n\n\n\n\n\n\nCode\n## Smoking\nL=4; S=1; D=1\n\nXt_smoke, Yt_smoke = from_arrays(xt_smoke, lookback=L, delay=D, step=S, feature_columns=feature_columns_smoke, target_columns=target_columns_smoke, unique=False,verbose=True)\n\n\nindice range: 0 4 --&gt; 5\nindice range: 1 5 --&gt; 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nXv_smoke, Yv_smoke = from_arrays(xv_smoke, lookback=L, delay=D, step=S, feature_columns=feature_columns_smoke, target_columns=target_columns_smoke, unique=False,verbose=True)\n\n\nindice range: 0 4 --&gt; 5\nindice range: 1 5 --&gt; 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Training:', Xt_smoke.shape, Yt_smoke.shape)\n\n\nTraining: (19, 5, 1) (19, 1)\n\n\nCode\nprint('Validation:', Xv_smoke.shape, Yv_smoke.shape)\n\n\nValidation: (2, 5, 1) (2, 1)"
  },
  {
    "objectID": "dl_time_series.html#rnn",
    "href": "dl_time_series.html#rnn",
    "title": "DL for Time Series",
    "section": "2.2 RNN",
    "text": "2.2 RNN\nIn this section, I’ll apply RNNs to the four datasets I chose. Recurrent Neural Networks (RNNs) stand as a foundational pillar in the landscape of deep learning, offering a powerful framework for processing sequential data. At their core, RNNs are designed to capture temporal dependencies within sequences, making them exceptionally well-suited for tasks such as time series forecasting, and natural language processing. Unlike traditional feedforward neural networks, RNNs possess an inherent memory mechanism that enables them to retain information about past inputs while processing current ones. This unique capability allows RNNs to exhibit dynamic behavior, effectively transforming input sequences into meaningful output representations. However, traditional RNNs are susceptible to the vanishing gradient problem, hindering their ability to effectively capture long-range dependencies. Despite this limitation, simple RNNs serve as a fundamental building block upon which more advanced architectures, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, have been developed to address these challenges and further extend the capabilities of sequential data processing.\nThe following code will be used to generate the models, which include regularization.\n\n\nCode\n## Regression Report Function\n\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()\n\n\n\n\nCode\n## History Plot\ndef history_plot(history):\n  FS=18 #fontsize\n\n  history_dict = history.history\n  loss_values = history_dict['loss']\n  val_loss_values = history_dict['val_loss']\n  epochs = range(1, len(loss_values) + 1)\n  plt.plot(epochs, loss_values, 'bo', label='Training Loss')\n  plt.plot(epochs, val_loss_values, 'b', label='Validation Loss')\n  plt.title('Training and Validation Loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.legend()\n  plt.show()\n\n\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\n\n\nCode\n## PFE\nXt1_pfe = Xt_pfe.reshape(Xt_pfe.shape[0],Xt_pfe.shape[1]*Xt_pfe.shape[2])\n# New Sizes \nprint(Xt_pfe.shape,\"--&gt;\",Yt_pfe.shape)\n\n\n(1029, 26, 1) --&gt; (1029, 1)\n\n\nCode\nprint(Xv_pfe.shape,\"--&gt;\",Yv_pfe.shape)\n\n\n(238, 26, 1) --&gt; (238, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-4\ninput_shape=(Xt_pfe.shape[1],Xt_pfe.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_pfe)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=128\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(l2=L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn (SimpleRNN)          │ (None, 128)            │        16,640 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 16,769 (65.50 KB)\n Trainable params: 16,769 (65.50 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_pfe = model.fit(Xt_pfe,\n                    Yt_pfe,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_pfe, Yv_pfe))\n# History plot\nhistory_plot(history_pfe)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_pfe=model.predict(Xt_pfe)\n\n\n\n\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 79ms/step\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\n\nCode\nYvp_pfe=model.predict(Xv_pfe) \n\n\n\n\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n\n\nCode\n# REPORT\nregression_report(Yt_pfe,Ytp_pfe,Yv_pfe,Yvp_pfe)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.2014103880853053\n MAE: 0.31524035619832336\nVALIDATION:\n MSE: 0.04661374148582871\n MAE: 0.16709192121499344\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## TEVA\nXt1_teva = Xt_teva.reshape(Xt_teva.shape[0],Xt_teva.shape[1]*Xt_teva.shape[2])\n# New Sizes \nprint(Xt_teva.shape,\"--&gt;\",Yt_teva.shape)\n\n\n(1029, 26, 1) --&gt; (1029, 1)\n\n\nCode\nprint(Xv_teva.shape,\"--&gt;\",Yv_teva.shape)\n\n\n(238, 26, 1) --&gt; (238, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-4\ninput_shape=(Xt_teva.shape[1],Xt_teva.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_teva)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=128\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_1\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (SimpleRNN)        │ (None, 128)            │        16,640 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 16,769 (65.50 KB)\n Trainable params: 16,769 (65.50 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_teva = model.fit(Xt_teva,\n                    Yt_teva,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_teva, Yv_teva))\n# History plot\nhistory_plot(history_teva)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_teva=model.predict(Xt_teva)\n\n\n\n\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 75ms/step\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\n\nCode\nYvp_teva=model.predict(Xv_teva) \n\n\n\n\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n\n\nCode\n# REPORT\nregression_report(Yt_teva,Ytp_teva,Yv_teva,Yvp_teva)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.07727331619009817\n MAE: 0.20032958951748622\nVALIDATION:\n MSE: 0.10713771899932957\n MAE: 0.22996969651209992\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## ALE\nXt1_ale = Xt_ale.reshape(Xt_ale.shape[0],Xt_ale.shape[1]*Xt_ale.shape[2])\n# New Sizes \nprint(Xt_ale.shape,\"--&gt;\",Yt_ale.shape)\n\n\n(15, 3, 1) --&gt; (15, 1)\n\n\nCode\nprint(Xv_ale.shape,\"--&gt;\",Yv_ale.shape)\n\n\n(2, 3, 1) --&gt; (2, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=20 #100\nL1=0\nL2=1e-3\ninput_shape=(Xt_ale.shape[1],Xt_ale.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_ale)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_2\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_2 (SimpleRNN)        │ (None, 32)             │         1,088 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 1,121 (4.38 KB)\n Trainable params: 1,121 (4.38 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_ale = model.fit(Xt_ale,\n                    Yt_ale,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_ale, Yv_ale))\n# History plot\nhistory_plot(history_ale)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_ale=model.predict(Xt_ale)\n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n\n\nCode\nYvp_ale=model.predict(Xv_ale) \n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n\n\nCode\n# REPORT\nregression_report(Yt_ale,Ytp_ale,Yv_ale,Yvp_ale)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.061257357350496\n MAE: 0.1992127179874392\nVALIDATION:\n MSE: 1.6203133695751508\n MAE: 0.9282160375243631\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Smoking\nXt1_smoke = Xt_smoke.reshape(Xt_smoke.shape[0],Xt_smoke.shape[1]*Xt_smoke.shape[2])\n# New Sizes \nprint(Xt_smoke.shape,\"--&gt;\",Yt_smoke.shape)\n\n\n(19, 5, 1) --&gt; (19, 1)\n\n\nCode\nprint(Xv_smoke.shape,\"--&gt;\",Yv_smoke.shape)\n\n\n(2, 5, 1) --&gt; (2, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1= 0 # 1e-3\nL2=1e-4\n#L2=0 #1e-4\n\ninput_shape=(Xt_smoke.shape[1],Xt_smoke.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_smoke)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=64\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L1L2(l1=L1,l2=L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_3\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_3 (SimpleRNN)        │ (None, 64)             │         4,224 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 1)              │            65 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,289 (16.75 KB)\n Trainable params: 4,289 (16.75 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_smoke = model.fit(Xt_smoke,\n                    Yt_smoke,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_smoke, Yv_smoke))\n# History plot\nhistory_plot(history_smoke)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_smoke=model.predict(Xt_smoke)\n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n\n\nCode\nYvp_smoke=model.predict(Xv_smoke) \n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n\n\nCode\n# REPORT\nregression_report(Yt_smoke,Ytp_smoke,Yv_smoke,Yvp_smoke)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0019346812711495643\n MAE: 0.03704895138562833\nVALIDATION:\n MSE: 4.988991881483324\n MAE: 2.2261524960690116"
  },
  {
    "objectID": "dl_time_series.html#gru",
    "href": "dl_time_series.html#gru",
    "title": "DL for Time Series",
    "section": "2.3 GRU",
    "text": "2.3 GRU\nIn this section, I’ll apply GRU to the four datasets I chose. Gated Recurrent Units (GRUs) represent a pivotal advancement in the realm of recurrent neural networks (RNNs), offering a streamlined yet powerful architecture for sequential data processing. Built upon the foundation of traditional RNNs, GRUs introduce a sophisticated gating mechanism that enables them to effectively capture long-range dependencies while mitigating the vanishing gradient problem. Unlike standard RNNs, GRUs feature gated units that regulate the flow of information throughout the network, allowing for more efficient information retention and utilization. This innovative design grants GRUs the ability to adaptively learn and update their internal states, making them particularly well-suited for tasks requiring memory and context preservation. Furthermore, GRUs exhibit computational efficiency compared to more complex architectures like Long Short-Term Memory (LSTM) networks, making them a popular choice for various applications where resource constraints are a consideration. With their blend of simplicity and effectiveness, GRUs stand as a versatile tool in the arsenal of deep learning practitioners, offering a robust solution for modeling sequential data with ease and efficiency.\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\n\n\nCode\n# GRU\n## PFE\nXt1_pfe = Xt_pfe.reshape(Xt_pfe.shape[0],Xt_pfe.shape[1]*Xt_pfe.shape[2])\n# NEW SIZES \nprint(Xt_pfe.shape,\"--&gt;\",Yt_pfe.shape)\n\n\n(1029, 26, 1) --&gt; (1029, 1)\n\n\nCode\nprint(Xv_pfe.shape,\"--&gt;\",Yv_pfe.shape)\n\n\n(238, 26, 1) --&gt; (238, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-4\ninput_shape=(Xt_pfe.shape[1],Xt_pfe.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_pfe)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=128\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(l2=L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_4\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru (GRU)                       │ (None, 128)            │        50,304 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 50,433 (197.00 KB)\n Trainable params: 50,433 (197.00 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_pfe = model.fit(Xt_pfe,\n                    Yt_pfe,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_pfe, Yv_pfe))\n# History plot\nhistory_plot(history_pfe)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_pfe=model.predict(Xt_pfe)\n\n\n\n\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 96ms/step\n\u001b[1m27/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\n\nCode\nYvp_pfe=model.predict(Xv_pfe) \n\n\n\n\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\n\nCode\n# REPORT\nregression_report(Yt_pfe,Ytp_pfe,Yv_pfe,Yvp_pfe)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.0782360009383643\n MAE: 0.21989806642245702\nVALIDATION:\n MSE: 0.04910458904246377\n MAE: 0.16816512772002734\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## TEVA\nXt1_teva = Xt_teva.reshape(Xt_teva.shape[0],Xt_teva.shape[1]*Xt_teva.shape[2])\n# NEW SIZES \nprint(Xt_teva.shape,\"--&gt;\",Yt_teva.shape)\n\n\n(1029, 26, 1) --&gt; (1029, 1)\n\n\nCode\nprint(Xv_teva.shape,\"--&gt;\",Yv_teva.shape)\n\n\n(238, 26, 1) --&gt; (238, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-4\ninput_shape=(Xt_teva.shape[1],Xt_teva.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_teva)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=128\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_5\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_1 (GRU)                     │ (None, 128)            │        50,304 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 50,433 (197.00 KB)\n Trainable params: 50,433 (197.00 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_teva = model.fit(Xt_teva,\n                    Yt_teva,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_teva, Yv_teva))\n# History plot\nhistory_plot(history_teva)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_teva=model.predict(Xt_teva)\n\n\n\n\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 99ms/step\n\u001b[1m26/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\n\nCode\nYvp_teva=model.predict(Xv_teva) \n\n\n\n\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\n\nCode\n# REPORT\nregression_report(Yt_teva,Ytp_teva,Yv_teva,Yvp_teva)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.11950324296367987\n MAE: 0.26466869066473886\nVALIDATION:\n MSE: 0.14237522915057174\n MAE: 0.28788996781911025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# GRU\n## ALE\nXt1_ale = Xt_ale.reshape(Xt_ale.shape[0],Xt_ale.shape[1]*Xt_ale.shape[2])\n# NEW SIZES \nprint(Xt_ale.shape,\"--&gt;\",Yt_ale.shape)\n\n\n(15, 3, 1) --&gt; (15, 1)\n\n\nCode\nprint(Xv_ale.shape,\"--&gt;\",Yv_ale.shape)\n\n\n(2, 3, 1) --&gt; (2, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-3\ninput_shape=(Xt_ale.shape[1],Xt_ale.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_ale)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=64\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_6\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_2 (GRU)                     │ (None, 64)             │        12,864 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 1)              │            65 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 12,929 (50.50 KB)\n Trainable params: 12,929 (50.50 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_ale = model.fit(Xt_ale,\n                    Yt_ale,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_ale, Yv_ale))\n# History plot\nhistory_plot(history_ale)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_ale=model.predict(Xt_ale)\n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n\n\nCode\nYvp_ale=model.predict(Xv_ale) \n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n\n\nCode\n# REPORT\nregression_report(Yt_ale,Ytp_ale,Yv_ale,Yvp_ale)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.008897596515946165\n MAE: 0.07350854721939334\nVALIDATION:\n MSE: 1.9252026951777848\n MAE: 1.0740769093994864\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# GRU\n## Smoking\nXt1_smoke = Xt_smoke.reshape(Xt_smoke.shape[0],Xt_smoke.shape[1]*Xt_smoke.shape[2])\n# NEW SIZES \nprint(Xt_smoke.shape,\"--&gt;\",Yt_smoke.shape)\n\n\n(19, 5, 1) --&gt; (19, 1)\n\n\nCode\nprint(Xv_smoke.shape,\"--&gt;\",Yv_smoke.shape)\n\n\n(2, 5, 1) --&gt; (2, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\n\nL1= 0 # 1e-3\nL2=1e-4\n\n#L2=0 #1e-4\ninput_shape=(Xt_smoke.shape[1],Xt_smoke.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_smoke)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=64\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L1L2(l1=L1,l2=L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_7\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_3 (GRU)                     │ (None, 64)             │        12,864 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 1)              │            65 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 12,929 (50.50 KB)\n Trainable params: 12,929 (50.50 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_smoke = model.fit(Xt_smoke,\n                    Yt_smoke,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_smoke, Yv_smoke))\n# History plot\nhistory_plot(history_smoke)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_smoke=model.predict(Xt_smoke)\n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n\n\nCode\nYvp_smoke=model.predict(Xv_smoke) \n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n\n\nCode\n# REPORT\nregression_report(Yt_smoke,Ytp_smoke,Yv_smoke,Yvp_smoke)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.031581859333375595\n MAE: 0.13240771391349465\nVALIDATION:\n MSE: 0.2974897709321268\n MAE: 0.5450980454974748"
  },
  {
    "objectID": "dl_time_series.html#lstm",
    "href": "dl_time_series.html#lstm",
    "title": "DL for Time Series",
    "section": "2.4 LSTM",
    "text": "2.4 LSTM\nIn this section, I’ll apply LSTM to the four datasets I chose. Long Short-Term Memory (LSTM) networks represent a significant breakthrough in the realm of recurrent neural networks (RNNs), addressing the inherent challenges of capturing long-range dependencies and mitigating the vanishing gradient problem. Characterized by a sophisticated memory cell and gating mechanisms, LSTMs excel at learning and retaining information over extended sequences. Unlike traditional RNNs, LSTMs feature a more intricate architecture that allows them to selectively update and forget information based on the context of the input sequence, enabling them to effectively model both short-term patterns and long-term dependencies. The key components of LSTMs, including input, forget, and output gates, work in tandem to regulate the flow of information throughout the network, facilitating robust learning and memory retention. Despite their complexity, LSTMs have become a cornerstone in the field of deep learning, demonstrating unparalleled performance in a wide range of sequential data analysis tasks and serving as a foundation for further advancements in recurrent neural network architectures.\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\n\n\nCode\n## PFE\n\nXt1_pfe = Xt_pfe.reshape(Xt_pfe.shape[0],Xt_pfe.shape[1]*Xt_pfe.shape[2])\n# NEW SIZES \nprint(Xt_pfe.shape,\"--&gt;\",Yt_pfe.shape)\n\n\n(1029, 26, 1) --&gt; (1029, 1)\n\n\nCode\nprint(Xv_pfe.shape,\"--&gt;\",Yv_pfe.shape)\n\n\n(238, 26, 1) --&gt; (238, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-4\ninput_shape=(Xt_pfe.shape[1],Xt_pfe.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_pfe)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=128\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(l2=L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_8\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (LSTM)                     │ (None, 128)            │        66,560 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 66,689 (260.50 KB)\n Trainable params: 66,689 (260.50 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_pfe = model.fit(Xt_pfe,\n                    Yt_pfe,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_pfe, Yv_pfe))\n# History plot\nhistory_plot(history_pfe)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_pfe=model.predict(Xt_pfe)\n\n\n\n\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 89ms/step\n\u001b[1m21/33\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\n\nCode\nYvp_pfe=model.predict(Xv_pfe) \n\n\n\n\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\n\nCode\n# REPORT\nregression_report(Yt_pfe,Ytp_pfe,Yv_pfe,Yvp_pfe)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.06104657445382188\n MAE: 0.19151022925966107\nVALIDATION:\n MSE: 0.08688013112783247\n MAE: 0.25481909652191176\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## TEVA\n\nXt1_teva = Xt_teva.reshape(Xt_teva.shape[0],Xt_teva.shape[1]*Xt_teva.shape[2])\n# NEW SIZES \nprint(Xt_teva.shape,\"--&gt;\",Yt_teva.shape)\n\n\n(1029, 26, 1) --&gt; (1029, 1)\n\n\nCode\nprint(Xv_teva.shape,\"--&gt;\",Yv_teva.shape)\n\n\n(238, 26, 1) --&gt; (238, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1=0\nL2=1e-4\ninput_shape=(Xt_teva.shape[1],Xt_teva.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_teva)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=128\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, kernel_regularizer=regularizers.L1L2(l1=L1, l2=L2), activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_9\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (LSTM)                   │ (None, 128)            │        66,560 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 66,689 (260.50 KB)\n Trainable params: 66,689 (260.50 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_teva = model.fit(Xt_teva,\n                    Yt_teva,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_teva, Yv_teva))\n# History plot\nhistory_plot(history_teva)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_teva=model.predict(Xt_teva)\n\n\n\n\u001b[1m 1/33\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 87ms/step\n\u001b[1m21/33\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n\n\nCode\nYvp_teva=model.predict(Xv_teva) \n\n\n\n\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n\n\nCode\n# REPORT\nregression_report(Yt_teva,Ytp_teva,Yv_teva,Yvp_teva)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.05799469819133684\n MAE: 0.18376515889546072\nVALIDATION:\n MSE: 0.043200332167507474\n MAE: 0.175543566474347\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## ALE\n\nXt1_ale = Xt_ale.reshape(Xt_ale.shape[0],Xt_ale.shape[1]*Xt_ale.shape[2])\n# NEW SIZES \nprint(Xt_ale.shape,\"--&gt;\",Yt_ale.shape)\n\n\n(15, 3, 1) --&gt; (15, 1)\n\n\nCode\nprint(Xv_ale.shape,\"--&gt;\",Yv_ale.shape)\n\n\n(2, 3, 1) --&gt; (2, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=20 #100\nL1=0\nL2=1e-3\ninput_shape=(Xt_ale.shape[1],Xt_ale.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_ale)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L2(L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_10\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_2 (LSTM)                   │ (None, 32)             │         4,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 1)              │            33 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 4,385 (17.13 KB)\n Trainable params: 4,385 (17.13 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_ale = model.fit(Xt_ale,\n                    Yt_ale,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_ale, Yv_ale))\n# History plot\nhistory_plot(history_ale)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_ale=model.predict(Xt_ale)\n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n\n\nCode\nYvp_ale=model.predict(Xv_ale) \n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n\n\nCode\n# REPORT\nregression_report(Yt_ale,Ytp_ale,Yv_ale,Yvp_ale)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.43524122210757626\n MAE: 0.6015217329531511\nVALIDATION:\n MSE: 0.88950610102399\n MAE: 0.7093434391519633\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Smoking\n\nXt1_smoke = Xt_smoke.reshape(Xt_smoke.shape[0],Xt_smoke.shape[1]*Xt_smoke.shape[2])\n# NEW SIZES \nprint(Xt_smoke.shape,\"--&gt;\",Yt_smoke.shape)\n\n\n(19, 5, 1) --&gt; (19, 1)\n\n\nCode\nprint(Xv_smoke.shape,\"--&gt;\",Yv_smoke.shape)\n\n\n(2, 5, 1) --&gt; (2, 1)\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL1= 0 # 1e-3\nL2=1e-4\n\n#L2=0 #1e-4\ninput_shape=(Xt_smoke.shape[1],Xt_smoke.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1_smoke)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=64\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nrecurrent_regularizer=regularizers.L1L2(l1=L1,l2=L2),\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n\n\nModel: \"sequential_11\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_3 (LSTM)                   │ (None, 64)             │        16,896 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (Dense)                │ (None, 1)              │            65 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 16,961 (66.25 KB)\n Trainable params: 16,961 (66.25 KB)\n Non-trainable params: 0 (0.00 B)\nNone\n\n\nCode\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory_smoke = model.fit(Xt_smoke,\n                    Yt_smoke,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv_smoke, Yv_smoke))\n# History plot\nhistory_plot(history_smoke)\n\n\n\n\n\n\n\n\n\nCode\n# Predictions \nYtp_smoke=model.predict(Xt_smoke)\n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n\n\nCode\nYvp_smoke=model.predict(Xv_smoke) \n\n\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n\n\nCode\n# REPORT\nregression_report(Yt_smoke,Ytp_smoke,Yv_smoke,Yvp_smoke)\n\n\n---------- Regression report ----------\nTRAINING:\n MSE: 0.031932517183397305\n MAE: 0.13679693340144322\nVALIDATION:\n MSE: 0.138780259060625\n MAE: 0.3702348262363053"
  },
  {
    "objectID": "dl_time_series.html#how-do-the-results-from-the-3-different-ann-models-compare-with-each-other-in-terms-of-accuracy-and-predictive-power",
    "href": "dl_time_series.html#how-do-the-results-from-the-3-different-ann-models-compare-with-each-other-in-terms-of-accuracy-and-predictive-power",
    "title": "DL for Time Series",
    "section": "3.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?",
    "text": "3.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\nSIMPLE RNN\n———- Regression report ———-\nTRAINING:\nMSE: 0.09003306405774096\nMAE: 0.22935533520666676\nVALIDATION:\nMSE: 0.07152575556546274\nMAE: 0.20140568269113573\nGRU\n———- Regression report ———-\nTRAINING:\nMSE: 0.07582283564521113\nMAE: 0.21662457798905047\nVALIDATION:\nMSE: 0.047982706225703546\nMAE: 0.16800236956128273\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.06083309764499571\nMAE: 0.19734143610035615\nVALIDATION:\nMSE: 0.051430222021359814\nMAE: 0.1647251482979423\nConclusion:\nFor the Pfizer Stock data, we can see from the evaluation metrics of three models we fit that GRU is the best performing model.\n\n\nSIMPLE RNN\n———- Regression report ———-\nTRAINING:\nMSE: 0.14224490639450332\nMAE: 0.2766673451356279\nVALIDATION:\nMSE: 0.18162044250956272\nMAE: 0.31885585749557244\nGRU\n———- Regression report ———-\nTRAINING:\nMSE: 0.13898719031178638\nMAE: 0.29045156732048505\nVALIDATION:\nMSE: 0.09160100968046733\nMAE: 0.2522322086363801\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.0570398157523587\nMAE: 0.17491911305353966\nVALIDATION:\nMSE: 0.0437083955363541\nMAE: 0.1610215293517061\nConclusion:\nFor the TEVA Stock data, we can see from the evaluation metrics of three models we fit that LSTM is the best performing model.\n\n\nSIMPLE RNN\n———- Regression report ———-\nTRAINING:\nMSE: 0.07496999048951764\nMAE: 0.23799187433914348\nVALIDATION:\nMSE: 1.6271619522321523\nMAE: 1.202708950450143\nGRU\n———- Regression report ———-\nTRAINING:\nMSE: 0.009263268811444294\nMAE: 0.07671659784789651\nVALIDATION:\nMSE: 1.974347099495756\nMAE: 1.099753070450283\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.38868042745021164\nMAE: 0.5600229344557486\nVALIDATION:\nMSE: 1.1643005406464868\nMAE: 0.8072635045413741\nConclusion:\nFor the Average Life Expectancy data, we can see from the evaluation metrics of three models we fit that LSTM is the best performing model.\n\n\nSIMPLE RNN\n———- Regression report ———-\nTRAINING:\nMSE: 0.0017923777777443042\nMAE: 0.03193420489724911\nVALIDATION:\nMSE: 3.896097217779866\nMAE: 1.9687103122764207\nGRU\n———- Regression report ———-\nTRAINING:\nMSE: 0.033673515118184075\nMAE: 0.13948307019576878\nVALIDATION:\nMSE: 1.462594036994753\nMAE: 1.203614750533638\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.03252545345599816\nMAE: 0.13955811391405032\nVALIDATION:\nMSE: 0.20626923253046567\nMAE: 0.453361609607277\nConclusion:\nFor the Smoking Prevalence data, we can see from the evaluation metrics of three models we fit that LSTM is the best performing model.\n\n\n\nTo conclude, among all the models we considered, the LSTM model emerged as the standout performer, demonstrating superiority across a spectrum of evaluation metrics. Its effectiveness, efficiency in navigating complexity, and adept handling of performance trade-offs underscored its dominance. Notably, the LSTM model exhibited exceptional accuracy and predictive prowess, particularly on unseen data. This remarkable performance can be attributed to the intricacies of its architecture, which facilitates the nuanced capture of intricate relationships and patterns within the dataset. The LSTM’s more sophisticated design empowers it to discern subtle dependencies and temporal dynamics, thereby unlocking deeper insights and driving enhanced predictive performance."
  },
  {
    "objectID": "dl_time_series.html#what-effect-does-including-regularization-have-on-the-results",
    "href": "dl_time_series.html#what-effect-does-including-regularization-have-on-the-results",
    "title": "DL for Time Series",
    "section": "3.2 What effect does including regularization have on the results?",
    "text": "3.2 What effect does including regularization have on the results?\nIncluding regularization into machine learning models is very significant, because it can improve generalization and combat overfitting by imposing a penalty on the model’s complexity. By introducing regularization techniques, such as adjusting the loss function to penalize complex models or implementing dropout layers, our aim is to promote generalization and prevent models from becoming overly specialized to the training data. Regularization helps prevent the reliance on specific features or patterns in the training data, fostering greater adaptability and resilience in model predictions. After I applied regularization to the four models I fit, I found that it improved the models’ accuracy a lot, avoiding overfitting."
  },
  {
    "objectID": "dl_time_series.html#how-far-into-the-future-can-the-deep-learning-model-accurately-predict-the-future",
    "href": "dl_time_series.html#how-far-into-the-future-can-the-deep-learning-model-accurately-predict-the-future",
    "title": "DL for Time Series",
    "section": "3.3 How far into the future can the deep learning model accurately predict the future?",
    "text": "3.3 How far into the future can the deep learning model accurately predict the future?\nThe accuracy of deep learning models such as RNNs, LSTMs, and GRUs in forecasting future events is contingent upon various factors, with predictive accuracy typically diminishing as the forecast horizon extends. While these models excel in short-term predictions, accurately forecasting the next few steps, their performance tends to degrade as the forecasting timeframe lengthens. Although architectures like LSTMs and GRUs may offer improved performance over longer horizons compared to basic RNNs due to their ability to address issues like the vanishing gradient problem, the precise range of accurate forecasting remains highly dependent on the specific application and model conditions. Considering this, I think we should focus on shorter and more reliable forecast horizons, and regularly update models with new data, which will help us yield superior results.\nMeanwhile, the predictive capability of deep learning models also relies heavily on the data complexity during training. As supervised learners, these models generate predictions based on the patterns within their training datasets. Consequently, their predictive power extends only as far as recognizing patterns similar to those in their training data. When predictions extend further into the future, their accuracy becomes increasingly dependent on the quality and representativeness of historical data, limiting accuracy when facing novel scenarios or trends not encountered during training.\nLast but not least, the accuracy of the models in predicting future depends on the model architectures as well."
  },
  {
    "objectID": "dl_time_series.html#how-does-your-deep-learning-modeling-compare-to-the-traditional-single-variable-time-series-armaarima-models",
    "href": "dl_time_series.html#how-does-your-deep-learning-modeling-compare-to-the-traditional-single-variable-time-series-armaarima-models",
    "title": "DL for Time Series",
    "section": "3.4 How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models?",
    "text": "3.4 How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models?\nIn this section, I’ll compare the ARIMA models we got before and the best performing deep learning models.\n\nPfizer StockTEVA StockAverage Life ExpectancySmoking Prevalence\n\n\nARMA(2,8)\nTraining set error measures: ME RMSE MAE MPE MAPE MASE ACF1\nTraining set -1.244409e-05 0.5955813 0.4091127 NaN Inf 0.6382266 -0.00247564\nGRU\n———- Regression report ———-\nTRAINING:\nMSE: 0.07582283564521113\nMAE: 0.21662457798905047\nVALIDATION:\nMSE: 0.047982706225703546\nMAE: 0.16800236956128273\nConclusion:\nFor the Pfizer Stock data, we can compare the MAE and the RMSE values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the deep learning model has lower RMSE and MAE values than ARIMA, suggesting that deep learning model may better handle the Pfizer Stock data and produce more accurate predictions.\n\n\nARMA(1,5)\nTraining set error measures: ME RMSE MAE MPE MAPE MASE ACF1\nTraining set -0.000172408 0.3340522 0.2310923 NaN Inf 0.7093577 -0.001059798\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.0570398157523587\nMAE: 0.17491911305353966\nVALIDATION:\nMSE: 0.0437083955363541\nMAE: 0.1610215293517061\nConclusion:\nFor the TEVA Stock data, we can compare the MAE and the RMSE values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the deep learning model has lower RMSE and MAE values than ARIMA, suggesting that deep learning model may better handle the TEVA Stock data and produce more accurate predictions.\n\n\nARMA(0,0,2)\nTraining set error measures: ME RMSE MAE MPE MAPE MASE ACF1\nTraining set -0.01784985 0.2933763 0.1901972 -Inf Inf 0.5501572 -0.1987566\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.38868042745021164\nMAE: 0.5600229344557486\nVALIDATION:\nMSE: 1.1643005406464868\nMAE: 0.8072635045413741\nConclusion:\nFor the Average Life Expectancy data, we can compare the MAE and the RMSE values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the ARIMA model surprisingly has lower RMSE and MAE values than ARIMA, suggesting that ARIMA model may better handle the Average Life Expectancy data and produce more accurate predictions.\n\n\nARMA(1,0,1)\nTraining set error measures: ME RMSE MAE MPE MAPE MASE ACF1\nTraining set -0.0724224 3.206556 1.824766 NaN Inf 0.6521255 -0.03006084\nLSTM\n———- Regression report ———-\nTRAINING:\nMSE: 0.03252545345599816\nMAE: 0.13955811391405032\nVALIDATION:\nMSE: 0.20626923253046567\nMAE: 0.453361609607277\nConclusion:\nFor the Smoking Prevalence data, we can compare the MAE and the RMSE values between the ARIMA (Part: Univariate TS Models) and the deep learning models. And we can see from the results that the deep learning model has lower RMSE and MAE values than ARIMA, suggesting that the deep learning model may better handle the Smoking Prevalence data and produce more accurate predictions."
  },
  {
    "objectID": "dl_time_series.html#deep-learning-prediction-vs.-traditional-ts-models",
    "href": "dl_time_series.html#deep-learning-prediction-vs.-traditional-ts-models",
    "title": "DL for Time Series",
    "section": "DEEP LEARNING PREDICTION Vs. Traditional TS Models",
    "text": "DEEP LEARNING PREDICTION Vs. Traditional TS Models\nIn summary, the analysis I did in this page highlights the effectiveness of deep learning models in time series forecasting tasks. While direct comparisons between deep learning and traditional models(eg. ARIMA, SARIMA, ARIMAX, SARIMAX, VAR) is challenging due to differences in training/validation splits, the RMSE values consistently favor deep learning models just except the Average Life Expectancy data.\nActually, for most of the traditional time series models we fit, they have higher RMSE, especially evident in the Smoking Prevalence data(RMSE for ARMA: 3.207), indicating their limitations in capturing data variance. Conversely, deep learning models consistently demonstrate lower RMSE values, most of them are below 1.\nAdditionally, when we compare the training/validation plots of our deep learning models with the forecasting plots of the previous traditional time series models, we can clearly see that the deep learning models more closely align with the real data trend. This disparity shows that traditional time series models such as ARIMA struggle in capturing data variance, while deep learning models adeptly memorize it. From Part 3.1, we can see that the hierarchy of deep learning models’ performance were impressive, and LSTM model outperforms than the other two models(GRU, simple RNN) in most of our data.\nTraditional time series models often require data to be stationary, often necessitating differencing to achieve forecasting accuracy. However, even with differencing, satisfactory results may not be attainable. Deep learning models, conversely, act as universal curve-fitters, capable of accommodating various data variances. This adaptability renders deep learning models more suitable for forecasting datasets characterized by fluctuations and seasonal patterns.\nConventional time series models frequently demand stationary data, prompting the need for differencing to enhance forecasting accuracy. Despite such preprocessing efforts, satisfactory results remain elusive in certain cases. In contrast, deep learning models serve as versatile curve-fitters, possessing the inherent capability to adapt to diverse data variances. This inherent adaptability positions deep learning models as ideal candidates for forecasting datasets characterized by pronounced fluctuations and intricate seasonal patterns. By virtue of their flexibility, deep learning models offer a compelling alternative to traditional approaches, seamlessly accommodating the inherent complexities of real-world data and yielding more robust and accurate forecasts."
  },
  {
    "objectID": "others_analysis.html",
    "href": "others_analysis.html",
    "title": "Other: Interrupted TS/ARFIMA/ Spectral Analysis",
    "section": "",
    "text": "Still on the way ~"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "As we can see from the Big Picture, to understand the overall health status of the United States, we need to look at various areas. Therefore, in this project, I studied five aspects of Health in the United States: Public Health Outcomes, Healthcare Access and Utilization, Healthcare Costs, Health Behaviors and Risk Factors, and Healthcare Financial Market. For Public Health Outcomes, I examined the average life expectancy, infant mortality rate, and three chronic diseases (heart disease, cancer, and diabetes). For Healthcare Access and Utilization, I looked at the prevalence of health insurance and how often Americans have visited emergency rooms in the past few years. For Healthcare Costs, I looked at the healthcare industry’s contribution to overall GDP, the cost of prescription drugs, and the cost of drug development. For Healthcare Behaviors and Risk Factors, I mainly examined Smoking and Alcohol Consumption. Finally, for the Healthcare Financial Market, I mainly selected four stocks: Pfizer, Vertex, TEVA, and SAGE, which represent the different sectors of the healthcare industry (Large-scale Pharmaceutical, Biotechnology, Generic and Specialty Drug, and Small and Medium-sized Biotech).\nAfter the above comprehensive analysis and examination, we will have an extensive and in-depth understanding of Health in the United States."
  },
  {
    "objectID": "conclusions.html#revisit-our-big-picture",
    "href": "conclusions.html#revisit-our-big-picture",
    "title": "Conclusions",
    "section": "",
    "text": "As we can see from the Big Picture, to understand the overall health status of the United States, we need to look at various areas. Therefore, in this project, I studied five aspects of Health in the United States: Public Health Outcomes, Healthcare Access and Utilization, Healthcare Costs, Health Behaviors and Risk Factors, and Healthcare Financial Market. For Public Health Outcomes, I examined the average life expectancy, infant mortality rate, and three chronic diseases (heart disease, cancer, and diabetes). For Healthcare Access and Utilization, I looked at the prevalence of health insurance and how often Americans have visited emergency rooms in the past few years. For Healthcare Costs, I looked at the healthcare industry’s contribution to overall GDP, the cost of prescription drugs, and the cost of drug development. For Healthcare Behaviors and Risk Factors, I mainly examined Smoking and Alcohol Consumption. Finally, for the Healthcare Financial Market, I mainly selected four stocks: Pfizer, Vertex, TEVA, and SAGE, which represent the different sectors of the healthcare industry (Large-scale Pharmaceutical, Biotechnology, Generic and Specialty Drug, and Small and Medium-sized Biotech).\nAfter the above comprehensive analysis and examination, we will have an extensive and in-depth understanding of Health in the United States."
  },
  {
    "objectID": "conclusions.html#part-1-public-health-outcomes",
    "href": "conclusions.html#part-1-public-health-outcomes",
    "title": "Conclusions",
    "section": "PART 1: Public Health Outcomes",
    "text": "PART 1: Public Health Outcomes\n\n1. How has the average life expectancy in the US changed?\nFor the Average Life Expectancy, we first used Plotly and Tableau to do a data visualization of the average life expectancy and total deaths of COVID-19. We can see that in the past two decades, The average life expectancy of Americans has risen every year, reaching 79.1 years in 2019. However, the arrival of COVID-19 has caused a significant drop in the average life expectancy of Americans, with an average life expectancy of only 77 years in 2021. However, as the epidemic eases, average life expectancy rebounds, with an average of 78 years in 2022. We then used an ARIMA model and three deep learning models to predict future life expectancy, and we found that the model predicted that the average life expectancy of Americans in the future would rebound and then remain about the same.\n2. What are the trends in the mortality rates of major diseases (e.g., heart disease, cancer, diabetes) over the last few decades?\nFor the mortality rates of major diseases (e.g., heart disease, cancer, diabetes) over the last few decades, I also used Plotly to show how they changed. We can clearly see that for the higher mortality rates of heart disease and cancer, there has been a dramatic decline in these two diseases over the last few decades. In the case of diabetes, the death rate was essentially flat until 2000 and has declined slightly in recent years. This is due to an improvement in the overall quality of life of Americans, as well as advances in medical care. I then used the ARIMA model to project future mortality rates for these three major diseases. I found that future mortality rates for these three diseases will remain flat for some time, implying that advances in medical technology over the past few decades have brought about dramatic reductions in mortality rates for these diseases but that we still do not have a more effective way to cure these diseases further at this time.。\n3. How have infant mortality rates varied over time?\nWe can see from the data visualization that infant mortality rates have decreased from 0.94% in 1990 to 0.54% in 2021. I then used the ARIMA model to predict future infant mortality rates, and the results showed that there would be a small increase in infant mortality rates. I believe that the issue of infant mortality still deserves a great deal of attention because of the vast inequalities that exist due to race and economic income. Washington, in particular, has one of the highest infant mortality rates in the entire United States, and a black mother has more than three times the mortality rate compared to a white mother.\nWhat’s more, I also used the ARIMAX model to find out how the average life expectancy of the US be influenced by the mortality rates of Cancer, Heart Diseases, Diabetes, and infant mortality rates (Adu, Appiahene, Afrifa, 2023). The best performing model I found is ARIMA(2,0,1). The forecasting results indicate that average life expectancy in the US is expected to continue increasing, but with a degree of uncertainty, as represented by the widening confidence intervals. This suggests optimism about future health outcomes. As we can see, the death rates due to cancer, heart disease, and diabetes are decreasing, and the infant mortality rate is also decreasing, which could be reasons for the increase in life expectancy. Meanwhile, we can see a decrease of average life expectancy around 2020, which could be due to the COVID-19 pandemic."
  },
  {
    "objectID": "conclusions.html#part-2-healthcare-access-and-utilization",
    "href": "conclusions.html#part-2-healthcare-access-and-utilization",
    "title": "Conclusions",
    "section": "PART 2: Healthcare Access and Utilization",
    "text": "PART 2: Healthcare Access and Utilization\n\n6. How has the rate of health insurance coverage changed before and after the implementation of the Affordable Care Act (ACA)?\nFirst, I visualized the Health Insurance Uncoverage in the US, we can see that the rate of health insurance uncoverage in the United States has been on a decline since 2010, which is the year of the implementation of Barack Obama’s health care reforms. However, starting from 2017, the health insurance uncoverage rate began to rise slightly, which may be attributed to the policies of the Trump administration. It is noteworthy that the 18-64 age group has consistently had the highest rate of medical insurance non-coverage among all age groups.\n7. What are the patterns in emergency room visits over the years?\nBased on the numbers of emergency room visits, we can see that those aged 65 and above have always been the group with the highest rate of emergency room visits, indicating a greater prevalence of health issues or medical emergencies within this age group. And the 45-64 age group has the lowest rate of emergency room visits, suggesting a notable disparity in emergency healthcare utilization across different age groups, with those in the older demographic requiring more frequent and intensive medical attention. Understanding these trends is crucial for healthcare resource allocation, as it highlights the need for targeted interventions and support systems to address the unique healthcare needs of diverse age groups within the population."
  },
  {
    "objectID": "conclusions.html#part-3-healthcare-costs",
    "href": "conclusions.html#part-3-healthcare-costs",
    "title": "Conclusions",
    "section": "PART 3: Healthcare Costs",
    "text": "PART 3: Healthcare Costs\n\n4. How did the Covid-19 influence the people’s health in the US? And how did the pandemic influence the GDP contribution of Health Industry?\nFrom the fact that the average life expectancy of Americans has dived in 2020, we know that COVID-19 has had a very negative impact on the health of Americans, especially the elderly or those with underlying diseases. Then, I applied the SARIMA Model to the GDP Contribution of the Healthcare Industry and found that the GDP Contribution of the Healthcare Industry will continue to rise in the future, and the upward trend will be more drastic after COVID-19. The healthcare industry in the US is expected to keep growing in the future. Healthcare is becoming more and more important in the US economy after the COVID-19 pandemic.\n5. Are there seasonal patterns or cyclical fluctuations in the health industry’s GDP contribution, and do they correspond to specific events or seasons?\nAfter applying the SARIMA model to the GDP Contribution of the Healthcare Industry, I used the AIC, BIC, and AICc values to find the best model. As a result, I found the best model is SARIMA(0,1,1)(0,1,1)[4], suggesting that the health industry’s GDP contribution has seasonal patterns. This could arise from fiscal policy cycles that impact healthcare spending or seasonal variations in healthcare service utilization—for instance, higher hospital admissions during winter months due to flu season. Also, implementing healthcare policies, which often follow a cycle, might lead to discernible seasonal effects on the economic indicators within the healthcare sector.\n8. How have the costs of prescription drugs evolved over the past 20 years? And How have the costs of drug development changed over time? Is there a relationship between the costs of prescription drugs and the public health outcomes in the US?\nFrom the data visualization of prescription drug expenditures in the US, we can see that only 2.7 billion dollars were spent on prescription drugs in 1960. However, since 2000, the Expenditure has grown drastically from 122 billion dollars to 405.9 billion dollars. Interestingly, we can also see that Expenditures increased even more hugely after COVID-19. The costs of drug development increased greatly from the 1970s to the 2010s as well.\nNext, I employed the VAR model to explore the potential relationship between the costs of prescription drugs and the average life expectancy. The most robust model I identified is VAR(3), and I conducted forecasting for Life Expectancy and Prescription Drug Expenditure based on this model. The results indicate a fluctuating pattern. The Average Life Expectancy is expected to decrease for some years, then rebound. In contrast, the Prescription Drug Expenditure is anticipated to continue its upward trajectory for the next ten years. This suggests a potential correlation between these two variables. We can speculate that the decrease in life expectancy could be attributed to the COVID-19 pandemic, which has had a profound impact on public health. Consequently, prescription drug expenditure is expected to rise, potentially influencing the average life expectancy."
  },
  {
    "objectID": "conclusions.html#part-4-health-behaviors-and-risk-factors",
    "href": "conclusions.html#part-4-health-behaviors-and-risk-factors",
    "title": "Conclusions",
    "section": "PART 4: Health Behaviors and Risk Factors",
    "text": "PART 4: Health Behaviors and Risk Factors\n\n9. How have smoking and alcohol consumption rates changed over time, and what impact have they had on public health?\nBefore 2009, the number of smokers in the US showed a slight declining trend with some fluctuations. Starting from 2010, there was a sharp decline in the number of smokers. This could be attributed to the passage of The Family Smoking Prevention and Tobacco Control Act by the U.S. government in 2010. The act implemented regulatory measures on tobacco products, including prohibiting advertising and promotional activities for tobacco products, and requiring health warnings on tobacco product packaging. Additionally, the U.S. government imposed high taxes on tobacco products. It can also be observed that alcohol consumption in the US was on the rise from 1935 to 1980, peaking in 1980. During the period from 1920 to 1933, the US implemented Alcohol Prohibition, we could speculate a sharp decrease in alcohol consumption. After the repeal of Alcohol Prohibition, alcohol consumption showed an upward trend. It is also noted that after reaching a low point in alcohol consumption in 1998, the trend has been increasing in recent years.\nNext, I employed the VAR model to explore the potential relationship between Smoking Prevalence, Alcohol Consumption, and the Mortality rates of major diseases (Cancer, Heart Disease, Diabetes). The most robust model I identified is VAR(6), and I conducted forecasting for Smoking Prevalence, Alcohol Consumption, and the Mortality rates of major diseases (Cancer, Heart Disease, Diabetes) based on this model. The forecasting results show that the total number of major diseases first increases and then decreases. The number of adult smokers shows an increasing trend, followed by a decrease. The Alcohol Consumption shows a slight increase and then a considerable reduction. We can find that the trend of Total Major Diseases Deaths is consistent with the Number of Adult Smokers and the Alcohol Consumption, both of which are increasing and then decreasing. This can indicate that unhealthy behaviors can significantly affect people’s health outcomes."
  },
  {
    "objectID": "conclusions.html#part-5-healthcare-financial-market",
    "href": "conclusions.html#part-5-healthcare-financial-market",
    "title": "Conclusions",
    "section": "PART 5: Healthcare Financial Market",
    "text": "PART 5: Healthcare Financial Market\n10. How to use time series analysis and forecasting models to predict the healthcare stock prices? And what’s the relationship between them?\nFirst, I used ShinyApp to show changes in stock prices in the healthcare sector over time. Then, I predicted four stocks (Pfizer, Vertex, Teva and Sage) using an ARIMA model. We can see that Pfizer and Vertex’s stock prices will increase while TEVA and SAGE’s stock prices will decrease. I also used the VAR model to test the relationship between them, and the results of VAR(1) show that the predicted stock prices of Pfizer and Vertex are positively correlated. In contrast, the expected stock prices of Teva and SAGE are negatively correlated.\nFurthermore, I harnessed the power of three deep learning models to forecast Pfizer and Teva’s stock prices. Upon comparing the evaluation metrics, it was evident that the deep learning models outperformed traditional methods, underscoring their superior prediction performance.\nMeanwhile, I found that Pfizer and TEVA stock returns have a volatility clustering phenomenon, so I applied the GARCH model on top of the ARIMA model and showed their volatility plots."
  },
  {
    "objectID": "conclusions.html#final-remarks",
    "href": "conclusions.html#final-remarks",
    "title": "Conclusions",
    "section": "Final Remarks",
    "text": "Final Remarks\nOverall,"
  }
]